% Lines starting with a percent sign (%) are comments. LaTeX will
% not process those lines. Similarly, everything after a percent
% sign in a line is considered a comment. To produce a percent sign
% in the output, write \% (backslash followed by the percent sign).
% ==================================================================
% Usage instructions:
% ------------------------------------------------------------------
% The file is heavily commented so that you know what the various
% commands do. Feel free to remove any comments you don't need from
% your own copy. When redistributing the example thesis file, please
% retain all the comments for the benefit of other thesis writers!
% ==================================================================
% Compilation instructions:
% ------------------------------------------------------------------
% Use pdflatex to compile! Input images are expected as PDF files.
% Example compilation:
% ------------------------------------------------------------------
% > pdflatex thesis-example.tex
% > bibtex thesis-example
% > pdflatex thesis-example.tex
% > pdflatex thesis-example.tex
% ------------------------------------------------------------------
% You need to run pdflatex multiple times so that all the cross-references
% are fixed. pdflatex will tell you if you need to re-run it (a warning
% will be issued)
% ------------------------------------------------------------------
% Compilation has been tested to work in ukk.cs.hut.fi and kosh.hut.fi
% - if you have problems of missing .sty -files, then the local LaTeX
% environment does not have all the required packages installed.
% For example, when compiling in vipunen.hut.fi, you get an error that
% tikz.sty is missing - in this case you must either compile somewhere
% else, or you cannot use TikZ graphics in your thesis and must therefore
% remove or comment out the tikz package and all the tikz definitions.
% ------------------------------------------------------------------

% General information
% ==================================================================
% Package documentation:
%
% The comments often refer to package documentation. (Almost) all LaTeX
% packages have documentation accompanying them, so you can read the
% package documentation for further information. When a package 'xxx' is
% installed to your local LaTeX environment (the document compiles
% when you have \usepackage{xxx} and LaTeX does not complain), you can
% find the documentation somewhere in the local LaTeX texmf directory
% hierarchy. In ukk.cs.hut.fi, this is /usr/texlive/2008/texmf-dist,
% and the documentation for the titlesec package (for example) can be
% found at /usr/texlive/2008/texmf-dist/doc/latex/titlesec/titlesec.pdf.
% Most often the documentation is located as a PDF file in
% /usr/texlive/2008/texmf-dist/doc/latex/xxx, where xxx is the package name;
% however, documentation for TikZ is in
% /usr/texlive/2008/texmf-dist/doc/latex/generic/pgf/pgfmanual.pdf
% (this is because TikZ is a front-end for PGF, which is meant to be a
% generic portable graphics format for LaTeX).
% You can try to look for the package manual using the ``find'' shell
% command in Linux machines; the find databases are up-to-date at least
% in ukk.cs.hut.fi. Just type ``find xxx'', where xxx is the package
% name, and you should find a documentation file.
% Note that in some packages, the documentation is in the DVI file
% format. In this case, you can copy the DVI file to your home directory,
% and convert it to PDF with the dvipdfm command (or you can read the
% DVI file directly with a DVI viewer).
%
% If you can't find the documentation for a package, just try Googling
% for ``latex packagename''; most often you can get a direct link to the
% package manual in PDF format.
% ------------------------------------------------------------------


% Document class for the thesis is report
% ------------------------------------------------------------------
% You can change this but do so at your own risk - it may break other things.
% Note that the option pdftext is used for pdflatex; there is no
% pdflatex option.
% ------------------------------------------------------------------
\documentclass[12pt,a4paper,oneside,pdftex]{report}

% The input files (tex files) are encoded with the latin-1 encoding
% (ISO-8859-1 works). Change the latin1-option if you use UTF8
% (at some point LaTeX did not work with UTF8, but I'm not sure
% what the current situation is)
\usepackage[utf8]{inputenc}
% OT1 font encoding seems to work better than T1. Check the rendered
% PDF file to see if the fonts are encoded properly as vectors (instead
% of rendered bitmaps). You can do this by zooming very close to any letter
% - if the letter is shown pixelated, you should change this setting
% (try commenting out the entire line, for example).
\usepackage[OT1]{fontenc} %fontenc first, then inputenc if need be
% The babel package provides hyphenating instructions for LaTeX. Give
% the languages you wish to use in your thesis as options to the babel
% package (as shown below). You can remove any language you are not
% going to use.
% Examples of valid language codes: english (or USenglish), british,
% finnish, swedish; and so on.
\usepackage[british]{babel}


% Font selection
% ------------------------------------------------------------------
% The default LaTeX font is a very good font for rendering your
% thesis. It is a very professional font, which will always be
% accepted.
% If you, however, wish to spicen up your thesis, you can try out
% these font variants by uncommenting one of the following lines
% (or by finding another font package). The fonts shown here are
% all fonts that you could use in your thesis (not too silly).
% Changing the font causes the layouts to shift a bit; you many
% need to manually adjust some layouts. Check the warning messages
% LaTeX gives you.
% ------------------------------------------------------------------
% To find another font, check out the font catalogue from
% http://www.tug.dk/FontCatalogue/mathfonts.html
% This link points to the list of fonts that support maths, but
% that's a fairly important point for master's theses.
% ------------------------------------------------------------------
% <rant>
% Remember, there is no excuse to use Comic Sans, ever, in any
% situation! (Well, maybe in speech bubbles in comics, but there
% are better options for those too)
% </rant>

% \usepackage{palatino}
% \usepackage{tgpagella}



% Optional packages
% ------------------------------------------------------------------
% Select those packages that you need for your thesis. You may delete
% or comment the rest.

% Natbib allows you to select the format of the bibliography references.
% The first example uses numbered citations:
\usepackage[square,sort&compress,numbers]{natbib}
% The second example uses author-year citations.
% If you use author-year citations, change the bibliography style (below);
% acm style does not work with author-year citations.
% Also, you should use \citet (cite in text) when you wish to refer
% to the author directly (\citet{blaablaa} said blaa blaa), and
% \citep when you wish to refer similarly than with numbered citations
% (It has been said that blaa blaa~\citep{blaablaa}).
% \usepackage[square]{natbib}

% The alltt package provides an all-teletype environment that acts
% like verbatim but you can use LaTeX commands in it. Uncomment if
% you want to use this environment.
% \usepackage{alltt}

% The eurosym package provides a euro symbol. Use with \euro{}
\usepackage{eurosym}

% Verbatim provides a standard teletype environment that renderes
% the text exactly as written in the tex file. Useful for code
% snippets (although you can also use the listings package to get
% automatic code formatting).
\usepackage{verbatim}

% The listing package provides automatic code formatting utilities
% so that you can copy-paste code examples and have them rendered
% nicely. See the package documentation for details.
% \usepackage{listings}

% The fancuvrb package provides fancier verbatim environments
% (you can, for example, put borders around the verbatim text area
% and so on). See package for details.
% \usepackage{fancyvrb}

% Supertabular provides a tabular environment that can span multiple
% pages.
%\usepackage{supertabular}
% Longtable provides a tabular environment that can span multiple
% pages. This is used in the example acronyms file.
\usepackage{longtable}

% The fancyhdr package allows you to set your the page headers
% manually, and allows you to add separator lines and so on.
% Check the package documentation.
% \usepackage{fancyhdr}

% Subfigure package allows you to use subfigures (i.e. many subfigures
% within one figure environment). These can have different labels and
% they are numbered automatically. Check the package documentation.
\usepackage{subfigure}

% The titlesec package can be used to alter the look of the titles
% of sections, chapters, and so on. This example uses the ``medium''
% package option which sets the titles to a medium size, making them
% a bit smaller than what is the default. You can fine-tune the
% title fonts and sizes by using the package options. See the package
% documentation.
\usepackage[medium]{titlesec}

% The TikZ package allows you to create professional technical figures.
% The learning curve is quite steep, but it is definitely worth it if
% you wish to have really good-looking technical figures.
\usepackage{tikz}
% You also need to specify which TikZ libraries you use
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.pathmorphing,decorations.markings}
\usetikzlibrary{shapes}
\usetikzlibrary{patterns}


% The aalto-thesis package provides typesetting instructions for the
% standard master's thesis parts (abstracts, front page, and so on)
% Load this package second-to-last, just before the hyperref package.
% Options that you can use:
%   mydraft - renders the thesis in draft mode.
%             Do not use for the final version.
%   doublenumbering - [optional] number the first pages of the thesis
%                     with roman numerals (i, ii, iii, ...); and start
%                     arabic numbering (1, 2, 3, ...) only on the
%                     first page of the first chapter
%   twoinstructors  - changes the title of instructors to plural form
%   twosupervisors  - changes the title of supervisors to plural form
%\usepackage[mydraft,twosupervisors]{aalto-thesis}
%usepackage[mydraft,doublenumbering]{aalto-thesis}
\usepackage{aalto-thesis}
% Added packages 11.5.2014
\usepackage{amsmath,empheq}
\usepackage{textcomp}
% Pretty tables
\usepackage{booktabs}
% Algorithm writing
\usepackage[]{algorithm2e}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
% Subfigure environment
\usepackage{caption}
\usepackage{subcaption}

% Hyperref
% ------------------------------------------------------------------
% Hyperref creates links from URLs, for references, and creates a
% TOC in the PDF file.
% This package must be the last one you include, because it has
% compatibility issues with many other packages and it fixes
% those issues when it is loaded.
\RequirePackage[pdftex]{hyperref}
% Setup hyperref so that links are clickable but do not look
% different
\hypersetup{colorlinks=false,raiselinks=false,breaklinks=true}
\hypersetup{pdfborder={0 0 0}}
\hypersetup{bookmarksnumbered=true}
% The following line suggests the PDF reader that it should show the
% first level of bookmarks opened in the hierarchical bookmark view.
\hypersetup{bookmarksopen=true,bookmarksopenlevel=1}
% Hyperref can also set up the PDF metadata fields. These are
% set a bit later on, after the thesis setup.


% Thesis setup
% ==================================================================
% Change these to fit your own thesis.
% \COMMAND always refers to the English version;
% \FCOMMAND refers to the Finnish version; and
% \SCOMMAND refers to the Swedish version.
% You may comment/remove those language variants that you do not use
% (but then you must not include the abstracts for that language)
% ------------------------------------------------------------------
% If you do not find the command for a text that is shown in the cover page or
% in the abstract texts, check the aalto-thesis.sty file and locate the text
% from there.
% All the texts are configured in language-specific blocks (lots of commands
% that look like this: \renewcommand{\ATCITY}{Espoo}.
% You can just fix the texts there. Just remember to check all the language
% variants you use (they are all there in the same place).
% ------------------------------------------------------------------
\newcommand{\TITLE}{Crane Load Object Geometry Measurement using Machine Vision}
\newcommand{\FTITLE}{Nosturin taakan geometrian mittaus konenäköavusteisesti}
\newcommand{\DATE}{May 27th, 2014}
\newcommand{\FDATE}{27. toukokuuta 2014}
\newcommand{\SDATE}{Den 27 Maj 2014}

% Supervisors and instructors
% ------------------------------------------------------------------
% If you have two supervisors, write both names here, separate them with a
% double-backslash (see below for an example)
% Also remember to add the package option ``twosupervisors'' or
% ``twoinstructors'' to the aalto-thesis package so that the titles are in
% plural.
% Example of one supervisor:
\newcommand{\SUPERVISOR}{Professor Ville Kyrki}
\newcommand{\FSUPERVISOR}{Professori Ville Kyrki}
\newcommand{\SSUPERVISOR}{Professor Ville Kyrki}
% Example of twosupervisors:
%\newcommand{\SUPERVISOR}{Professor Ville Kyrki\\
%  D.Sc. Sami Terho}
%\newcommand{\FSUPERVISOR}{Professori Ville Kyrki\\
%  TkT Sami Terho}
%\newcommand{\SSUPERVISOR}{Professor Ville Kyrki\\
%  Dr. Sami Terho}

% If you have only one instructor, just write one name here
\newcommand{\INSTRUCTOR}{D.Sc. (Tech.) Sami Terho}
\newcommand{\FINSTRUCTOR}{TkT Sami Terho}
\newcommand{\SINSTRUCTOR}{Teknologie doktor Sami Terho}
% If you have two instructors, separate them with \\ to create linefeeds
% \newcommand{\INSTRUCTOR}{Olli Ohjaaja M.Sc. (Tech.)\\
%  Elli Opas M.Sc. (Tech)}
%\newcommand{\FINSTRUCTOR}{Diplomi-insinööri Olli Ohjaaja\\
%  Diplomi-insinööri Elli Opas}
%\newcommand{\SINSTRUCTOR}{Diplomingenjör Olli Ohjaaja\\
%  Diplomingenjör Elli Opas}

% If you have two supervisors, it is common to write the schools
% of the supervisors in the cover page. If the following command is defined,
% then the supervisor names shown here are printed in the cover page. Otherwise,
% the supervisor names defined above are used.
\newcommand{\COVERSUPERVISOR}{Professor Ville Kyrki, Aalto University, School of Electrical Engineering}
% The same option is for the instructors, if you have multiple instructors.
% \newcommand{\COVERINSTRUCTOR}{Olli Ohjaaja M.Sc. (Tech.), Aalto University\\
%  Elli Opas M.Sc. (Tech), Aalto SCI}

% Other stuff
% ------------------------------------------------------------------
\newcommand{\PROFESSORSHIP}{Intelligent robotics}
\newcommand{\FPROFESSORSHIP}{Älykäs robotiikka}
\newcommand{\SPROFESSORSHIP}{Intelligent robotteknik}
% Professorship code is the same in all languages
\newcommand{\PROFCODE}{T-110}
\newcommand{\KEYWORDS}{stereo vision, crane load object, environment mapping}
\newcommand{\FKEYWORDS}{stereonäkö, nosturin taakka, ympäristön kartoitus}
\newcommand{\SKEYWORDS}{stereoseende, kran last, miljö kartläggning}
\newcommand{\LANGUAGE}{English}
\newcommand{\FLANGUAGE}{Englanti}
\newcommand{\SLANGUAGE}{Engelska}
% Author is the same for all languages
\newcommand{\AUTHOR}{Erkka Mutanen}

% Currently the English versions are used for the PDF file metadata
% Set the PDF title
\hypersetup{pdftitle={\TITLE\ \AUTHOR}}
% Set the PDF author
\hypersetup{pdfauthor={\AUTHOR}}
% Set the PDF keywords
\hypersetup{pdfkeywords={\KEYWORDS}}
% Set the PDF subject
\hypersetup{pdfsubject={Master's Thesis}}

% Layout settings
% ------------------------------------------------------------------

% When you write in English, you should use the standard LaTeX
% paragraph formatting: paragraphs are indented, and there is no
% space between paragraphs.
% When writing in Finnish, we often use no indentation in the
% beginning of the paragraph, and there is some space between the
% paragraphs.

% If you write your thesis Finnish, uncomment these lines; if
% you write in English, leave these lines commented!
% \setlength{\parindent}{0pt}
% \setlength{\parskip}{1ex}

% Use this to control how much space there is between each line of text.
% 1 is normal (no extra space), 1.3 is about one-half more space, and
% 1.6 is about double line spacing.
\linespread{1.1} % This is the default
% \linespread{1.3}

% Bibliography style
% acm style gives you a basic reference style. It works only with numbered
% references.
\bibliographystyle{acm}
% Plainnat is a plain style that works with both numbered and name citations.
% \bibliographystyle{ieeetr}



% Extra hyphenation settings
% ------------------------------------------------------------------
% You can list here all the files that are not hyphenated correctly.
% You can provide many \hyphenation commands and/or separate each word
% with a space inside a single command. Put hyphens in the places where
% a word can be hyphenated.
% Note that (by default) LaTeX will not hyphenate words that already
% have a hyphen in them (for example, if you write ``structure-modification
% operation'', the word structure-modification will never be hyphenated).
% You need a special package to hyphenate those words.
\hyphenation{di-gi-taa-li-sta yksi-suun-tai-sta}

% The preamble ends here, and the document begins.
% Place all formatting commands and such before this line.
% ------------------------------------------------------------------
\begin{document}
% This command adds a PDF bookmark to the cover page. You may leave
% it out if you don't like it...
\pdfbookmark[0]{Cover page}{bookmark.0.cover}
% This command is defined in aalto-thesis.sty. It controls the page
% numbering based on whether the doublenumbering option is specified
\startcoverpage

% Cover page
% ------------------------------------------------------------------
% Options: finnish, english, and swedish
% These control in which language the cover-page information is shown
\coverpage{english}
% Abstracts
% ------------------------------------------------------------------
% Include an abstract in the language that the thesis is written in,
% and if your native language is Finnish or Swedish, one in that language.

% Abstract in English
% ------------------------------------------------------------------
\thesisabstract{english}{

In manufacturing industry, the electric overhead crane is used in lifting and moving operations of industrial goods, such as beams, engines, and large coils. Mostly, these cranes are manually operated in locations where people work, such as a factory floor. The cranes do not operate automatically and they do not have safety features that would prevent collisions of the lifted load object and other products or workers.

Geometry measurement of the load object is required to allow for new smarter automated features to the overhead crane environment, such as route planning, collision detection, and collision avoidance. The incentives for developing such features are increased goods handling safety, and the need to improve easiness of operation for the crane operator. 

A 3d machine vision process was selected to produce a load object measurement using a passive stereographic triangulation technique. A perception platform with two high dynamic range cameras was used to acquire data from a real overhead crane environment. Captured images were processed into 3d point clouds representing the environment. Finally, the point cloud was processed into a metric axis-aligned bounding volume measurement of the load object using Point Cloud Library (PCL) processing. The measurement software was implemented using C++ libraries and robot operating system (ROS). 

This thesis studies two different applications for machine vision -based load object geometry measurement. The first case introduces a technique for measuring axis-aligned bounding volume of the load object using simple tracking of the load object in indoor overhead crane environment. The second case introduces a technique for measuring cylindrical load objects using parameter fitting technique and 2d image tracking in outdoor rotary telescopic crane environment.  
}

%BADThis thesis will primarily study a machine vision approach that uses stereo vision for the work environment
%epth mapping. Other optional sensors, such as a laser scanner, are considered.

%BADExisting machine vision approaches will be applied to the crane load problem. 
%The location measurement of the crane load could enable additional improvements in obstable avoidance while
%operating the crane.}

%\fixme{Abstract text goes here (and this is an example how to use fixme).}
%Fixme is a command that helps you identify parts of your thesis that still
%require some work. When compiled in the custom \texttt{mydraft} mode, text
%parts tagged with fixmes are shown in bold and with fixme tags around them. When
%compiled in normal mode, the fixme-tagged text is shown normally (without
%special formatting). The draft mode also causes the ``Draft'' text to appear on
%the front page, alongside with the document compilation date. The custom
%\texttt{mydraft} mode is selected by the \texttt{mydraft} option given for the
%package \texttt{aalto-thesis}, near the top of the \texttt{thesis-example.tex}
%file.

%The thesis example file (\texttt{thesis-example.tex}), all the chapter content
%files (\texttt{1introduction.tex} and so on), and the Aalto style file
%(\texttt{aalto-thesis.sty}) are commented with explanations on how the Aalto
%thesis works. The files also contain some examples on how to customize various
%details of the thesis layout, and of course the example text works as an
%example in itself. Please read the comments and the example text; that should
%get you well on your way!}

% Abstract in Finnish
% ------------------------------------------------------------------
\thesisabstract{finnish}{
Valmistusteollisuudessa käytetään sähkökäyttöisiä prosessihallinostureita tuotteiden nosto- ja siirtotehtäviin. Tuotteet ovat usein yksittäiseriä, kuten moottoreita, palkkeja, tai isoja käämejä. Yleensä nostureita ohjataan manuaalisesti tiloissa joissa ihmiset työskentelevät, esimerkiksi tehdaslattiatiloissa. Nostureita ei edellä mainituissa tilanteissa yleensä voi ohjata tietokoneella, eikä niissä ole turvaominaisuuksia jotka estävät nostettavan kappaleen törmäykset tehdasympäristössä olevien muiden tuotteiden tai työntekijöiden kanssa.

Nosturin taakan geometrian mittaus vaaditaan uusien automaattisten toimintojen mahdollistamiseksi tehdasympäristössä. Uusia toimintoja ovat esimerkiksi reitinsuunnittelu-, törmäystarkastelu-, ja törmäyksenestotoiminnot. Edellä mainittujen toimintojen kehittäminen mahdollistaa turvallisemman taakan siirtämisen tilassa, ja helpottaa nosturin operoimista.

Taakan mittauksen tuottamiseksi valittiin 3d-konenäkömenetelmä, joka käyttää passiivista stereokolmiointimenetelmää. Työn toteutuksessa käytettiin kahta high dynamic range -kameraa, jotka tallensivat kuvadataa oikeasta nosturiympäristöstä. Kuvadata prosessoitiin 3d-pistepilviksi, joista edelleen tuotettiin metrinen koordinaattiakselien myötäinen bounding volume -mittaus Point Cloud Library -kirjaston (PCL) avulla. Mittausohjelmisto toteutettiin C++ -kirjastoilla ja Robot Operating System:llä (ROS).

Tämä diplomityö käsittelee kahta eri sovellusta konenäköavusteiselle nosturin taakan geometrian mittaukselle. Ensimmäinen sovellus esittelee menetelmän koordinaattiakselien suuntaiseen bounding volume -mittaukseen yksinkertaisella nosturin taakan seuraamisella sisätiloissa. Toinen sovellus esittelee menetelmän sylinterimäisten taakkojen mittaukseen parametrisovitusmenetelmän ja 2d-kuvaseurannan avulla ulkotiloissa teleskooppinosturilla.
}
% Abstract in Swedish
% ------------------------------------------------------------------
%\thesisabstract{swedish}{
%illa Vargens universum är det tredje fiktiva universumet inom huvudfäran av de
%ecknade disneyserierna - de övriga två är Kalle Ankas och Musse Piggs
%niversum. Figurerna runt Lilla Vargen kommer huvudsakligen frän tre källor ---
%els persongalleriet i kortfilmen Tre smä grisar frän 1933 och dess uppföljare,
%dels längfilmen Sängen om Södern frän 1946, och dels frän episoden ``Bongo'' i
%ängfilmen Pank och fägelfri frän 1947. Framför allt de två första har
%sedermera även kommit att leva vidare, utvidgas och införlivas i varandra genom
%tecknade serier, främst sädana producerade av Western Publishing för
%amerikanska Disneytidningar under ären 1945--1984.

%Världen runt Lilla Vargen är, i jämförelse med den runt Kalle Anka eller Musse
%Pigg, inte helt enhetlig, vilket bland annat märks i Bror Björns skiftande
%personlighet. Den har även varit betydligt mer öppen för influenser frän andra
%Disneyvärldar, inte minst de tecknade längfilmerna. Ytterligare en skillnad är
%tt varelserna i vargserierna förefaller stä närmare sina förebilder inom den
%verkliga djurvärlden. Att vargen Zeke vill äta upp grisen Bror Duktig är till
%exempel ett ständigt äterkommande tema, men om katten Svarte Petter skulle fä
%för sig att äta upp musen Musse Pigg skulle detta antagligen höja ett och annat
%ögonbryn bland läsarna.}


% Acknowledgements
% ------------------------------------------------------------------
% Select the language you use in your acknowledgements
\selectlanguage{english}

% Uncomment this line if you wish acknoledgements to appear in the
% table of contents
%\addcontentsline{toc}{chapter}{Acknowledgements}

% The star means that the chapter isn't numbered and does not
% show up in the TOC
\chapter*{Acknowledgements}

I wish to thank all students who use \LaTeX\ for formatting their theses,
because theses formatted with \LaTeX\ are just so nice.

Thank you, and keep up the good work!
\vskip 10mm

\noindent Espoo, \DATE
\vskip 5mm
\noindent\AUTHOR

% Table of contents
% ------------------------------------------------------------------
\cleardoublepage
% This command adds a PDF bookmark that links to the contents.
% You can use \addcontentsline{} as well, but that also adds contents
% entry to the table of contents, which is kind of redundant.
% The text ``Contents'' is shown in the PDF bookmark.
\pdfbookmark[0]{Contents}{bookmark.0.contents}
\tableofcontents

% Abbreviations & Acronyms
% ------------------------------------------------------------------
% Use \cleardoublepage so that IF two-sided printing is used
% (which is not often for masters theses), then the pages will still
% start correctly on the right-hand side.
\cleardoublepage
% Example acronyms are placed in a separate file, acronyms.tex
% \input{acronyms}

\addcontentsline{toc}{chapter}{Abbreviations}
\chapter*{Abbreviations}

% The longtable environment should break the table properly to multiple pages,
% if needed

\noindent
\begin{longtable}{@{}p{0.25\textwidth}p{0.7\textwidth}@{}}
AABB & Axis-aligned bounding-box \\
AC & Alternating Current \\
AI & Artificial Intelligence \\
API & Application Programming Interface \\
BM & Block Matching \\
BVH & Bounding Volume Hierarchies \\
CCIR & Comité Consultatif International pour la Radio \\
CMOS & Complementary Metal Oxide Semiconductor \\
CSM & Complete Surface Model \\
CUDA & Compute Unified Device Architecture \\
DC & Direct Current \\
DOP & Discrete Oriented Polytope \\
EOT & Electric Overhead Traveling Crane \\
FLIR & Forward Looking Infra Red \\
FOV & Field Of View \\
GIM & Generic Intelligent Machines research group \\
GPS & Global Positioning System \\
GPU & Graphics Processing Unit \\
HOG & Histogram Of Oriented Gradients \\
HDR & High Dynamic Range \\
I/O & Input Output\\
IMU & Inertial Measurement Unit \\
LAN & Local Area Network \\
LDP & Location Determination Problem \\
LIDAR & Light Detecting And Ranging \\
MLESAC & Maximum Likelihood Estimator Sample Consensus \\
MSAC & M-estimator Sample And Consensus \\
MSER & Maximally Stable Extremal Regions \\ 
OBB & Oriented Bounding Box \\
PC & Personal Computer \\
PCL & Point Cloud Library \\
PLC & Programmable Logic Controller \\
PROSAC & Progressive Sample Consensus \\
RADAR & Radio Detecting And Ranging \\
RANSAC & Random Sample And Consensus \\
ROI & Region Of Interest \\
ROS & Robot Operating System \\
SAD & Sum of Absolute Differences \\
SFM & Structure From Motion \\
SGBM & Semi-Global Block Matching \\
SIFT & Scale-invariant Feature Transformation \\
SIMD & Single Instruction, Multiple Data \\
SOM & Self-organizing Map \\
SONAR & Sound Navigation And Ranging \\
SSD & Sum of Squared Differences \\
SSE & Streaming SIMD Extensions \\
SURF & Speeded Up Robust Features \\
TLD & Tracking, Learning, and Detection \\
TOF & Time-Of-Flight \\
UDP & User Datagram Protocol \\
VSM & Visible Surface Model \\
XML & Extensive Markup Language \\
XMLRPC & XML Remote Procedure Call \\ 
\end{longtable}

% List of tables
% ------------------------------------------------------------------
% You only need a list of tables for your thesis if you have very
% many tables. If you do, uncomment the following two lines.
% \cleardoublepage
% \listoftables

% Table of figures
% ------------------------------------------------------------------
% You only need a list of figures for your thesis if you have very
% many figures. If you do, uncomment the following two lines.
% \cleardoublepage
% \listoffigures

% The following label is used for counting the prelude pages
\label{pages-prelude}
\cleardoublepage

%%%%%%%%%%%%%%%%% The main content starts here %%%%%%%%%%%%%%%%%%%%%
% ------------------------------------------------------------------
% This command is defined in aalto-thesis.sty. It controls the page
% numbering based on whether the doublenumbering option is specified
\startfirstchapter

% Add headings to pages (the chapter title is shown)
\pagestyle{headings}

% The contents of the thesis are separated to their own files.
% Edit the content in these files, rename them as necessary.
% ------------------------------------------------------------------

% \input{1introduction.tex}


%===========================================================================================================
\chapter*{Symbols}

%\footnote{http://owl.english.purdue.edu/owl/resource/574/02/} of
%Purdue University or Strunk's Elements of
%Style\footnote{http://www.bartleby.com/141/}. Remember that footnotes

%\emph{cite}.

% An example of a traditional LaTeX table
% ------------------------------------------------------------------
% A note on underfull/overfull table cells and tables:
% ------------------------------------------------------------------
% In professional typography, the width of the text in a page is always a lot
% less than the width of the page. If you are accustomed to the (too wide) text
% areas used in Microsoft Word's standard documents, the width of the text in
% this thesis layout may suprise you. However, text in a book needs wide
% margins. Narrow text is easier to read and looks nicer. Longer lines are
% hard to read, because the start of the next line is harder to locate when
% moving from line to the next.
% However, tables that are in the middle of the text often would require a wider
% area. By default, LaTeX will complain if you create too wide tables with
% ``overfull'' error messages, and the table will not be positioned properly
% (not centered). If at all possible, try to make the table narrow enough so
% that it fits to the same space as the text (total width = \textwidth).
% If you do need more space, you can either
% 1) ignore the LaTeX warnings
% 2) use the textpos-package to manually position the table (read the package
%    documentation)
% 3) if you have the table as a PDF document (of correct size, A4), you can use
%    the pdfpages package to include the page. This overrides the margin
%    settings for this page and LaTeX will not complain.
% ------------------------------------------------------------------
% Another note:
% ------------------------------------------------------------------
% If your table fits to \textwidth, but the cells are so narrow that the text
% in p{..}-formatted cells does not flow nicely (you get underfull warnings
% because LaTeX tries to justify the text in the cells) you can manually set
% the text to unjustified by using the \raggedright command for each cell
% that you do not want to be justified (see the example below). \raggedleft
% is also possible, of course...
% ------------------------------------------------------------------
% If you need to have linefeeds (\\) inside a cell, you must create a new
% paragraph-formatting environment inside the cell. Most common ones are
% the minipage-environment and the \parbox command (see LaTeX documentation
% for details; or just google for ``LaTeX minipage'' and ``LaTeX parbox'').
% Alignment of sells: l=left, c=center, r=right.
% If you want wrapping lines, use p{width} exact cell widths.
% If you want vertical lines between columns, write | above between the letters
% Horizontal lines are generated with the \hline command:

% Place a & between the columns
% In the end of the line, use two backslashes \\ to break the line,
% then place a \hline to make a horizontal line below the row

%\begin{table}
%\begin{tabular}{|p{2cm}|p{3.8cm}|p{4.5cm}|p{1.1cm}|}
%\hline % The line on top of the table
%\textbf{Code} & \textbf{Name} & \textbf{Methods} & \textbf{Area} \\
%\hline
%T-110.6130 & Systems Engineering for Data Communications
%    Software & \raggedright Computer simulations, mathematical modeling,
%  experimental research, data analysis, and network service business
%  research methods, (agile method) & T-110 \\
%\hline
%\multicolumn{2}{|p{6.25cm}|}{Mat-2.3170 Simulation (here is an example of
% multicolumn for tables)}& Details of how to build simulations & T-110 \\
%\hline
%S-38.3184 & Network Traffic Measurements and Analysis
%& \raggedright How to measure and analyse network
%  traffic & T-110 \\ \hline
% \end{tabular} % for really simple tables, you can just use tabular
% You can place the caption either below (like here) or above the table
%\caption{Research methodology courses}
% Place the label just after the caption to make the link work
%\label{table:courses}
%\end{table} % table makes a floating object with a title
%The multicolumn command takes the following 3 arguments:
% the number of cells to merge, the cell formatting for the new cell, and the
% contents of the cell

\chapter{Introduction}
\label{chapter:introduction}
%Ongelman esittely, johdanto. Pituus noin 5 sivua.
%A machine can tirelessly inspect items on a product assembly line that would end up being dangerous, monotonous, or otherwise problematic for a human worker. Additionally, we may utilize the full electromagnetic spectral range in addition to visible light spectrum for quality control purposes.
%business

In the modern world, products are being manufactured more than ever before, and at the same time the factory workflows should be optimised to guarantee profitable continuity for a business. These new requirements challenge all business owners, including industrial product makers who use overhead cranes in their manufacturing processes. Overhead cranes are found in most warehouses and factories, and they are often used in storage solutions, or as a part of the manufacturing floor, and in the initial installation of heavy machinery in the factory.

%development
Since industrial revolution many different types of cranes were designed to help in loading and unloading of ship cargo in container crate terminals. Before the ideation of a modern container cargo handling process, the lifting of barrels, pallets, and sacks was inefficient, and the ships were stalling longer times at docks than at sea, which reduced profitability. Today, the modern sea cargo handling process is highly automated, but the development of the overhead crane never reached the same level of automation. Even though steam engine cranes have been replaced by the modern electric overhead traveling crane (EOT), the operation of EOTs still remains mostly manual at factory sites. Thus, it seems that lifting operations with an EOT crane could become more efficient with added automation systems. Depending on the amount of use, the profitability of the crane use can possibly be improved. \citep{Zrni04}

%productivity
The business of lifting solutions gets more automated as new crane systems are being developed to meet the increasing needs for productivity. Productivity may be measured using selected productivity indicators, for example, energy consumption, service intervals, speed of operations, or safety features are easily quantifiable measures that can add up to a productivity rating. All of these indicators affect performance of lifting and moving goods using overhead cranes, and with ever lowering computing prices a lot of research is going on to find ways how added programming and sensors could have a positive effect on any productivity aspects.

%In the modern world, goods are being moved more than ever before, which brings challenges to keep the goods handling easy and safe. Products are mostly transported from one location to another using a modern automatic container handling process, but for example loading and unloading events remain laborous when single industrial items, such as engines or other machinery, are moved to a new location. The electric overhead crane (EOT) is the de facto standard for heavy lifting work in factory and warehouse environments.

%The business of lifting solutions gets more automated as new crane systems are being developed to meet the ever increasing needs for performance. Speed and safety affect the productivity of the event of lifting and moving goods with cranes, and with ever lowering computing prices a lot of research is going on to find ways how added programming and sensors could benefit the crane operator for increased performance. While speed is not important in all cargo  operations, safety is important in all of them. Then again, same automation technologies could be potentially used to speed up operations where speed is important, such as log picking operation in forestry solutions.

%If an overhead traveling crane is used to move products inside a warehouse e.g. from one manufacturing station to another, then a measurement of the lifted load object can potentially provide useful information for automation systems that use the size of the load object as a parameter in smart applications. Currently, overhead cranes do not measure any geometry of the load objects that they lift. Thus, all the decision-making whether the load object is physically in correct spatial coordinates is left for the operator to decide.

%Ideas of the modern container cargo handling process only started after the second half of the 20th century, and before that the only known way of transport was sea cargo packed in barrels, sacks and pallets. It was cumbersome to load and unload a ship filled with such small units, and it was not unusual that the ships spent more time in docks than at sea because of inefficient handling of small cargo.\ref{Zrni04}\par

Manual EOT crane movement may suffer from non-optimized route selection, operator fatigue, damage to goods in collisions, safety issues for other workers, and other problems that are caused by an operator. 


Some industries are not even profitable unless a certain speed of moving goods safely is reached, which applies for e.g. forestry log picking and tree felling work. To improve things, a real-time description of the load object and its environment could be available, so it would be possible to prevent unwanted events from happening using perception sensors and safety software. \citep{Kappi13}

%measuring goods
Measuring products while they are being moved and processed at the industrial site is the very basis for all quality control in other manufacturing industries: for example, radio frequency identification tags are used to identify and control a batch of products in bulk manufacturing, video systems measure process quality in a paper factory, and a spectral analyzer measures the concentration of foreign substances in a soda bottle washing process. In an overhead crane environment, there is information available only on how large a mass is being lifted and where it is being lifted, but no information on what is being lifted or what volume is the load object. A machine vision perception system can add quality control in the crane environment by providing a feedback link with information on lifted object size, shape, orientation, color, movement, past locations, and visuals.

\section{Interactive Control In A Crane Environment}
\label{section:collision_control_in_a_crane_environment}

% käyttökokemus
Industrial goods e.g. beams, engines, large coils, drums of dangerous goods, and pallets of stacked items are expensive, often unique one-time orders that are being lifted and transferred inside a warehouse every day. If in collision, a lifted product may get damaged or break other products, which can be prevented by using interactive control. Interactive control suggests duality in control of the overhead crane: while the operator designs the initial hoisting and moving action, a computer interactively smooths and recalculates the trajectory with an optimal control scheme. Moreover, the computer controller can notify the operator of the current state of the crane operation so that the operator may adjust his or her working style with informative feedback.

% operointi
A human operator is better able to move load objects through narrow passages and execute difficult spatial tracks than a computer, but he or she cannot minimise the used energy to perform a certain movement or minimise daily wear of moving parts. A computer control can positively improve the crane life cycle and minimise maintenance breaks. 
Some automatic overhead cranes have been built, but usually they move bulk items, such as sand, steel, or other process-like material that is moved in environments where people are not allowed to be working simultaneously.

% törmäyksenesto
An overhead crane that uses collision avoidance technology or other interactive control technology helps the crane operator move products in a warehouse in a semi-automatic manner while providing added information about products and their properties. Ideally a collision avoidance technique simply prevents collision based damage to the working area housings and storaged products by engaging safety measures if needed. The system should be able to detect people who walk on the factory floor, and prevent injuries with high priority. Without designing the details of the human computer interaction here, the operator interacts with the computer safety systems so that productive and safe lifting operations can be carried out. The interaction with the overhead crane controller may be implemented using traditional control pendants, wireless controllers, or some new approach, for example, a hand motion controller suggested by Peng and Singhose in \ref{Peng12}.

% mitä tarvitaan
To realise a working collision avoidance system, an understanding of the crane environment and its qualities including all the moving parts must be formed. In general, all the static and moving parts of the crane environment should continuosly provide information about their location, dimensions, and moving speeds to begin with. Using such an extensive information model it would be possible to introduce collision avoidance techniques in overhead crane environments. Fortunately, the surrounding environment model is generated as a side product in the load object 3d reconstruction step, on which the thesis work mainly focuses on. A stereo camera pair seems like an excellent solution regarding the load object dimension measurement problem since it has the capability of modeling the imminent surrounding environment.

% technical difficulties
From a technical point of view, a discrete-time spatial tracker (4d tracker) needed in interactive collision avoidance system is difficult to realise. Still, estimated states of objects should provide enough information so that decision making processes can operate reliably even in the event of partial data loss. Trackers that would be suitable for this purpose are object state vector management systems, and Kalman filter based trackers, which are oo large topics to be covered in this thesis in more detail. If the load object is carefully tracked and its geometry is known, a collision computation with a known environment model can be used to prevent collisions before they happen. The added value is potentially great, which is why the measurement of the load object is worth researching. \citep{Leibe07, Corke11, Miller12}

%New overhead crane systems do not currently have any collision detection features, but optimized route planning could become a standard feature in the future.

\section{Applications}
% According to Yong-Seok03 vision systems in container cranes are difficult to maintenance and have high costs
%They also reported machine vision reliability issues because of limited onboard camera processing abilities, and changing lighting in the environment. An upgrade to operation in the infrared spectrum of light was suggested to counteract changing visible light conditions in their applications. REF = Peng09
%The use of machine vision brings the speed of the computer to many errands that are impossible or hazardous for a human to do.
%+ 3d data of the environment is important in factory design, facility management and urban regional planning. \ref{Surmann03}
%+ Environment sensing is needed for robotization of spaces
% application: city-scale 3d reconstruction from community-sourced online photos | ref Agarval09

Industrial applications for machine vision have become more ubiquitous as the price for machine vision systems have decreased. Different applications range from space exploration to agriculture, and from mobile robotics to quality control at a manufacturing line. Typically, applications of machine vision solve problems such as ranging, quality control, visual odometry, object detection, and tracking.

Strictly overhead crane machine vision applications include for example visual servoing\citep{Lun-Hui14}, real-time trajectory planning\citep{Kaneshige05}, wand controller following\citep{Peng09}, intelligent fault detection\citep{Rahmani10}, obstacle avoidance\citep{Nagai11}, and load object tracking and control applications\citep{Yoshida06}. 

Other machine vision applications in crane environments include remote localisation of the end effector \citep{Hainsworth94} and stereoscopic teleoperation of forestry cranes\citep{Westerberg08}. Also some new applications can be thought out that use the stereo camera platform that was used in making of the load object measurement (see chapter \ref{chapter:implementation}). For example warehouse surveillance, recording of lifting operations, operations tracking, and quality control could be possible with the perception platform installed in the crane trolley. For example, training videos can easily be recorded for new operators, and records of events may be used in case of accidents. Furthermore, the load object measurement statistics could be used to keep track of the crane usage and possibly lead to improvements in warehouse layout, or help a factory manager decide upon investing in a larger manufacturing floor if the capacity is at a limit.

Finally, a load object measurement should be used to add functionality to safety applications. A traditional safety application halts the crane process if people are detected in a dangerous work zone \cite{Raula11}. With the help of a collision control system described in section \ref{section:collision_control_in_a_crane_environment}, an adaptive safety scheme can be used to restrict the operation range of the crane, but not fully stop it.

\section{Overhead Cranes}
\label{section:overhead_cranes}
% big picture
A glimpse in the not so distant history reveals a development of the overhead crane, a hoisting system more commonly known as the bridge crane or the process crane. First mass manufactured overhead crane systems were commissioned in 1876 in England, and they were powered by steam engines in the beginning of the industrial revolution. Today, the overhead crane is electrically powered and widely used for service duties and efficient process lifting in heavy industries and in industrial custom manufacturing.

% tällä hetkellä overhead cranes
An overhead crane is typically manually operated by a technical worker who is making the decision whether the lifted product can be moved in any direction. Usually, a control pendant, or a wireless hand-held controller is used to move the load object to a desired location in the 3d crane working space. Supposedly, other workers are going about their daily work in the same factory floor where the overhead crane operates. With current generation manual overhead cranes it is possible to collide a load object with other objects that lie on the factory floor or on the shelves of a warehouse. 

Overhead cranes have automation systems that help manual operation, such as load anti-sway systems, single motor hoisting mechanisms, and wireless remote controllers. While they add value for the crane operator and improve safety and speed of the overhead crane operation, the manual systems cannot warn the operator automatically of any hazards. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{single_girder_eot.pdf}
    \caption{A single girder electric overhead traveling crane. Image adapted from \citep{Abus14}.}
    \label{fig:single_girder_eot}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{double_girder_eot.pdf}
    \caption{A double girder electric overhead traveling crane. Image adapted from \citep{Abus14}.}
    \label{fig:double_girder_eot}
  \end{center}
\end{figure}

%structure of a single overhead crane
Overhead cranes have a beam-like bridge installed between two rails as seen in figure \ref{fig:single_girder_eot}. The bridge spans the gap between the rails and travels from one end of the rail to the other. Effectively, a cuboid shaped crane working area (portal of the crane) is formed underneath the rails encompassing most space between a factory floor and the height of the installed crane.

% medium picture, applications
Different types of overhead crane installations exist depending on the manufacturer and on the design of the crane and its supposed application. Figure \ref{fig:double_girder_eot} shows a heavier design with double girder installation used for medium to heavy lifting in industrial manufacturing and process industries. For example, in steel mills, heavy lifting is needed when cylinders of molten steel or other metal products are carried to the next process station. In heavy industry, the capacity of the crane and standard compliance is important, and the crane must operate reliably in hazardous environments. Efficient and safe usage patterns depend on the application at hand. In all applications, most important factors usually are time-wise efficient movement of the crane, safe operation of the crane, and easiness of operation.

In heavy process industry, it is not often meaningful to measure load objects or containers whose size is standard and known prior to the lifting operation. For practical reasons, all testing on overhead cranes have been done using a company test site installation that was capable of lifting medium to heavy objects up to 15 tonnes. All the objects lifted with the overhead crane were smaller than 2 meters for all dimensions, which is quite a normal size for a pallet.

\section{Forestry Machine Application}
\label{section:forestry_machine_application}

A forestry machine load object measurement is a secondary objective that is researched in this thesis. Forestry machines that include a rotary telescopic crane are included as research targets, mainly log picker trucks. In tree felling work, little automation is used currently, and to increase the level of automation, a load object measurement system has been suggested. A lot of research has been done in the area of stereographic 3d teleoperation of log picker trucks, but without a better spatial understanding of the truck environment and computer aided end effector tool control it has been deemed not feasible with current teleoperation technologies. A machine vision system can provide a geometry measurement of a log picker load object, which in turn could be further used to provide a better spatial description of the forestry crane working area for the teleoperator. 

Log picking work differs from lifting work in a warehouse environment mostly due its productivity constraints. A certain speed of picking logs must be maintained to keep the operation of the log picker truck profitable in business sense. While teleoperating a rotary log picking crane is possible, the speed of a teleoperated crane is much lower than a human operated crane. Teleoperation techniques do enable a single driver operate multiple log picker trucks simultaneously, but currently the work is only productive with drivers on site. 

Research in automatic forestry machines is an ongoing effort that tries to increase productivity in the field with help of teleoperation and automatic machinery. A fully automatic log picker truck that is as fast as a human operated truck would provide the biggest productivity increase, but currently the research is focused more on aiding a single driver increase his or her productivity with help of computers. For example, new augmented reality displays are being introduced in the forestry machine cockpits to help the driver get information that helps with the work. 

\section{Goals And Objectives}
\label{section:goals_and_objectives}

The goal of this work is to measure the dimensions of industrial goods being lifted and moved in an industrial crane setting with the help of a stereo camera based machine vision system. The work is carried out and analysed for two environments: an indoor overhead crane, and an outdoor rotary boom crane on a forestry machine. The tested scenarios are detailed more in Section \ref{subsection:use_cases} \emph{Use Cases}.

The overhead crane load object measurement is the primary objective of this work. The forestry crane load object measurement is a secondary objective that investigates the applicability of the same machine vision system in a log picker truck setting.

The project affiliated with this master's thesis was the GIM research group's FAMOUS project whose deliverable the actual software client for the load object measurement was. As such, another primary objective was to implement the load object measurement software that is easy to integrate into other software solutions and supports Robot Operating System (ROS) on a network of PCs.

A tertiary objective regarding hardware was to keep the costs of the load object measurement system significantly lower than the price range of a LIDAR based solution. A tertiary objective regarding software was to make the load object measurement client provide measurement data nearly in real-time, or at least 5 measurements per second.

\chapter{Perceiving Environments In 3d}
\label{chapter:perceiving_environments_in_3d}
%+ 3 camera-setup can understand all available information in an orthographic projection => 4th camera does not add any more information
%page 473 Sonka07

The problem of the load object geometry measurement inherently requires a sensor that can recover 3d geometry of the environment. Extensive research has been done in the field of 3d geometry recovery, and by utilising these vast sources of research many different sensors can be selected that can produce a good 3d geometry model of the environment.

Besides machine vision, also other perception sensors exist that can be used to recover 3d geometry, such as laser scanners and radars. These are not considered strictly machine vision sensors, but for example the laser scanner technology is accurate enough to be used to verify a 3d geometry that any of the camera based solutions have recovered\citep{You11}. Thus, a Velodyne laser scanner was used in capturing an accurate model of the 3d environment in this work.

\section{Sensors}

Perception sensors used in machine vision system include sensors such as digital cameras, time-of-flight cameras, structured light cameras, and other equipment that may recover information about the surrounding environment by capturing emitted electromagnetic radiation. Machine vision cameras may use the visible spectrum of the light, or sometimes other spectrum of light e.g. ultraviolet light spectrum. A lot of different sensor options are commercially available for machine vision systems. The design of the machine vision system is highly affected by the selection of the sensor and the digital format of the information that is output from the sensor.

\subsection{Stereo Cameras}
\label{subsection:stereo_cameras}

Camera based vision sensing can be active or passive depending on the technology used. An active sensor emits energy into the environment and measures the energy radiated back to a sensor array. A passive sensor does not emit energy on its own, but registers and measures energy radiated to the sensor array from the environment. In general, the energy that is registered and digitally sampled by a camera is visible light, which will be the technology used in this work.



+ active sensor: a sensor that controls and uses its own images. An active sensor controls its image's illumination with an electromagentic energy source, such as laser or white light.
+ time-of-flight cameras (TOF)
According to other research, the Kinect sensor cannot observe items cast in direct sunlight, preventing extensive outdoor usage besides its range limitations \cite{tikkanen13}.  

\section{Effects Of Camera Optics In Modeling}
\label{section:effects_of_camera_optics_in_modeling}

% todo add teledyne white paper to optics
%\ref{teledyne_whitepaper}

Intensities perceived by a visual system are a function of the target geometry, reflectance of the target surface, illumination, and the camera viewpoint.
% http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/GOMES1/marr.html

The machine vision cameras include optical systems that control the amount of light entering the photosensitive CMOS sensor array in the camera. A basic understanding of parameters that affect the physical behaviour of light entering the sensor is needed to realise a working design for the application at hand. The most important parameters that must be set in the optical system include focal length, field of view (FOV), depth of view, and camera shutter speed. Other parameters that may affect the performance of the camera system are the selection of materials, and how light enters through the interface of two different materials. Different materials may be used inside the lenses and on the lens coatings. 

The aperture of the camera lens system controls the depth of field (DOF) perceived in a digital or analog image of a scene. The depth of field describes the range of depth that can be viewed in sharp detail through the lens system. If the DOF is large, then only a small depth range is in sharp detail, and the rest of the image is blurry for near-field objects and faraway objects. If the DOF is small, then near-field objects and faraway objects are sharply in focus both alike. For the design of a stereo camera depth measurement system a small DOF, or extended DOF, is used to fully enable a deep range of a scene in sharp detail. Thus, the load object will stay in sharp detail before and after the lifting nearer to the cameras. Since a small DOF is achieved by using a larger aperture value, a tradeoff is done between feature sharpness and the amount of noise present in the image. Aperture sizes are limited by diffraction, thus, small DOF images may suffer from high variance noise effects and low light conditions.

The aperture of the camera is described by a unitless number that is presented as f over a number

\begin{equation}
\label{eq:lens_aperture}
f/# = N = \frac{f}{D}
\end{equation}

where f is the focal lenght used and D is the diameter of the pupil of the lens system. Higher f numbers have a large depth of field since the camera lens approaches the asymptotic limit of the pinhole camera.


% angle of view of the lens???

% resolution of the sensor array,
%camera dynamic range
%electromagnetic spectrum range 
%Radiance is a measure of power of light radiated from a unit surface area of an object to some spatial angle. 
%Irradiance is a measure of power of light cast on a unit surface are of an object.
%The relation between an object's surface that radiates onto the camera sensor array can be formulated as

%\begin{equation}
%\label{equation:radiance}
%E = L \frac{\pi}{4} (\frac{d}{f})^2 \cos^4{\alpha}
%\end{equation}

%where L is object's radiance of unit surface area, and $\frac{d}{f}$ is the f-number of the lens in use. The $cos^4{\alpha}$ is the vignetting term. Not all radiating energy from the object is captured by the CMOS sensor array, but only the fraction that enters the camera lens and falls on the effective sensor array surface.

%weather conditions
    
\section{Digital 3d Environment Models}
\label{section:digital_3d_environment_models}
%+ possible algorithms: clustering, water-filling, and convex hull computations
%+different approaches: top-down (model-based) or bottom-up (reconstruction)

%Many research efforts report using laser range finders for 3d environment modeling \ref{Hähnel03}. This is natural since laser range finders and LADAR scanners output a high resolution point cloud that is accurate up to sub-centimeter lengths in range of hundreds of meters. In this thesis, a stereo camera based environment modeling approach is used since measured lengths are limited to maximum of 15 meters depending on the application. While the accuracy of the stereo camera depth map output is suitable for the bounding volume computation of a load object, the price of the system is much smaller when compared to a laser based solution. 

%If we wanted to fully model the load object then additional camera viewpoints would be needed that cover all the occluded and non-visible load object surfaces, such as the backside. With a full stereo camera envelop we are able to reconstruct the structure using any available reconstruction algorithm. 
%An interesting idea would be to use an adaptive reconstruction scheme that statistically models the object surface with a self-organizing map as described in de Medeiro's study\cite{Medeiros07}. For such a statistical modeling approach a normalized load object measurement for each discrete timestep would be needed as an input for the SOM model.
%It would be possible to do surface reconstruction if we can access additional viewpoints and envelop the load object fully 
%using adaptive iteration techniques in order to model the object in greater detail as we receive more information from other viewpoints, including its backside.
%statistically estimate and model the object in time using self-organizing maps and adaptive geometric meshing .
%The greatest challenge in computing the load object measurements and position is the low quality of the point cloud data received from the stereo camera rig. Due to limitations in e.g. block matching algorithm, filtering kernels, and changing environmental variables (such as lighting conditions) we may expect white noise to be present in the data for all discrete timesteps.
%Countering the varying geometric signal is difficult and a matter of discussion in itself: do we wish to add filtering that may possibly lower spatial resolution of the depicted scene in order to receive a filtered signal? 
%Since we are interested in the load object AABB measurements we are not required to normalize coordinates for 

Environment models are different types of digital data representations that describe the environments acquired by a perception sensor. The work implemented in the Chapter \ref{chapter:implementation} uses point cloud representation, which will be presented.



In the virtual reality research the modeling of real-world objects has been an issue for many years. It is not easy to recreate a real-world item in a digital system, but many approaches have been suggested.

One option is to record video from multiple cameras that look at the object from different points of view. In order to obtain a digital model we may utilise a multi-camera stereo process and compute range images for further processing. Obviously, this approach is very computationally difficult and results vary depending on the object convexity, lighting environment and the used algorithms.  

A study by the Carnegie Mellon university robotics institute call the model acquired from the intensity image and the corresponding range imagery a visible surface model (VSM) \cite{Rander97}. Basically a VSM contains a subset of real-world surface features and texture in the digital format, and the VSM can be further placed into a more comprehensive representation of the world, namely a complete surface model (CSM). 

\subsection{Point Cloud Representation}
\label{subsection:point_cloud_representation}

A point cloud is a set of data entries that typically describes a 3d surface geometry of an object in a coordinate frame. Technically, a point cloud contains simple 3d coordinate data, and it can have a few implementation dependent attributes. Point clouds are widely used in stereo 3d reconstruction, metrology applications, robotics applications, multiple viewpoint data registration, 3d data processing, and 3d visualisation purposes. Point clouds can be easily created using 3d scanners, or they can be computed generated from parametric models.

One of the most significant properties of the point cloud is whether the data contained is organised or unorganised. Organised point cloud data is structured in an array format, which has the benefit of enabling single point cloud value polling, and making it possible to e.g. set a colour value from a 2d image into a corresponding 3d coordinate. Organised data is used in many robotics applications and machine vision applications. Unordered point cloud data can only be iterated and the order of its data points is not guaranteed to be preserved.

Point cloud geometry can be easily rendered and inspected, but the data is often not usable in 3d applications without processing. For examepl, point cloud data may be required to be converted into a polygon representation, or a mesh model\ref{Rusinkiewicz00}.

Point cloud representations enable storing of organised datasets, fast binary data saving, and flexible storing of different primitive data types natively. Some challenges with point clouds include limited suitability for modeling dynamice environments, and high memory consumption when a high precision equipment is used to generate a large point cloud\citep{PCDFormat14, Hornung10}.

%Challenges
%(Hornung et al.) a point cloud uses up a large amount of memory and are only suitable for modeling static environments with high precision sensory equipment.

\subsection{Range Image Representation}
\label{subsection:range_image_representation}

Range image is a type of 3d data point representation that uses a 2d image array whose pixels correspond to a known location in the imaged scene. A range image is conceptually equivalent to an intensity image where image intensities are replaced with range measurements acquired from a sensor.

Range images have limitations in the ability to reproduce a detailed original point cloud. The term detailed point cloud here is a reference to an object model that loosely describes the object shape and details using a convex hull of points in space, including points that may be not visible from a single viewpoint. Then, if the detailed original point cloud includes multiple points on a line that projects to a single pixel in the range image, some data will be lost in the conversion to a range image. 

Also, if the ranging sensor has a spatially different measurement lattice structure compared to the 2d-array structure of range image, some pixels may be left blank because of a mismatch in spacing of range measurements.

for range measurements than what the range image uses, then some pixels in the range image will be left empty.

Since we create the point cloud with a camera system, we would not have the described projection data loss problem. Another problem with range images       \cite{Unnikrishnan08}.

\subsection{Volume Pixel Representation}
\label{subsection:volume_pixel_representation}

A volume pixel, or a voxel, is a prototype of a single entry in a 3d grid structure consisting of voxels. A voxel indicates occupancy in space, and it has at least a volume, and a location, which usually is in 3-dimensional array with constant dimensional sampling. A perfect example of a prototype voxel is a wooden box, which occupies a constant volume in space, and has a location in terms of, for example, a living room space. In digital systems, a voxel is more of a conceptual item that has a location, size, and optional services, such as statistical information about the encompassed voxel subspace. 

A 3d scene captured with a stereo camera system can be represented with a voxel grid. A standard voxel grid will have a constant sampling grid, thus, the scene will be composed of same-sized cubes. Anoter option is to use a varying sampling grid structure, or an octree structure (described in...?).

limitations: grid resolution, high-resolution grid consumes a lot of memory

\subsection{Octree Representation}
\label{subsection:octree_representation}
% multi-resolution octree representations

Octree is a tree structure whose nodes always have exactly eight child nodes. The octree structure is used to partition 3d data recursively into smaller partitions of space around a point region. The point region and the sub-quadrants of an octree node are symmetrical cubic volumes also known as voxels. In an octree, the size of the voxels change depending on the depth of the tree search at hand according to Figure \ref{fig:octree_quadrants}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{octree_quadrants.pdf}
    \caption{Bla bla bla.}
    \label{fig:octree_quadrants}
  \end{center}
\end{figure}

Octrees are used in 3d computer graphics applications for nearest neighbour search, efficient collision detection, and set operations (e.g. union, intersection, and difference) among other uses. The nodes of the octree include states for the regions of space with a free, occupied, or unknown status. This property makes the octree structure able to support a detection, classification and tracking of moving objects scheme in a 3d environment \citep{Azim12, Ouyang12}.

Octree representation is suitable for fast interactive physics simulation purposes or feedback control, because the memory efficient structure enables a detection of virtual object collisions in just milliseconds \citep{Noborio99}. Using these properties, detection and tracking of dynamic moving objects in a static environment can be done.

%opinion
Octree would be a suitable representation format for load object environment data because it supports many of the operations needed to realise a collision avoidance system, such as the collision detection possibility.


Octree structure is a 3d extension to the quadtree, which is a four-pointer binary tree. Originally, the idea to use octrees for spatial representation was introduced by Donald Meagher in 1982 \cite{Meagher82]. Today, many different implementations exist, for example, a probabilistic occupancy estimation approach of the Octomap library, \citep{Hornung10}.

% how to build an octree
The octree structure may be computed from non-convex polygonal contours from multiple different viewpoint cameras (structure from silhouette), but up to 5 - 15 different viewpoints are needed to attain good precision \citep{Noborio88}.

%Octomap
+ Octomap can model occupied space, free space, and unknown space all at the same time.
We can assume that in all cases of range measurements a lot of noise and uncertainty will be introduced in the measured values. Thus, it is a good idea to represent the range measurements in a map that is probabilistic in nature.
Octomap
    + memory-efficiency principle (memory and disk space handling)
    + differentiates unmapped and obstacle-free areas
    + supports probabilistic sensor fusion
    + can represent arbitrary shapes in space (elevation map cannot)
    
Octrees are used in
    + id Tech 6 game engine
    
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{velodyne_octree.jpg}
    \caption{An octree whose leaf nodes are visualised with a coarse resolution. The red object represents a large standard box (see section \ref{subsubsection:analysis_overheadcrane}) lifted with an overhead crane. The green object represents a crane operating person near the load object. Data from Velodyne HDL-32e LIDAR scanner.}
    \label{fig:velodyne_octree}
  \end{center}
\end{figure}

\section{Geometric Environment Acquisition Techniques}
\label{section:environment_acquisition_techniques}

In this work the focus on environmental acquisition is on systems capable of 3d modeling the geometry of surrounding environments. Other environmental variables, such as temperature, wind conditions, humidity or other qualities of the environment are not considered in the context of 3d acquisition, although they could possibly have an effect on the perception sensor parameters in a real-world acquisition system. 

\subsection{Object And Environment Modeling}
\label{subsection:object_and_environment_modeling}
%Objects that are considered as load objects in this thesis cover a range of custom manufactured products whose size is unknown prior to the lifting operation. Since each item moved with the crane is unique, non-standard size object, an understanding of the load object size would be valuable to a software system that operates autonomously.

In general, a real-world scene is difficult to model using perception sensors. A lot of study in real-world object modeling has been done using multiple sensors including laser range scanners, multiple viewpoint cameras, color cameras, inertial measurement units (IMU) and global positioning system (GPS). Multiple perception sensors and large image quantities from a single environment are needed in order to compute a realistic virtual environment model that displays a correct geometry of the environment, and presents textured object models with realistic lighting. \citep{ElHakim98}

Quality of modeling may degrade in bad weather, such as fog, mist, rain, or snow. Especially camera systems are affected by inhomogenous degradation in image quality. Different types of degradation in image quality are caused by issues such as lens flare, condensation of water on lens, changes in illumination, shadows, and noise from droplets of water moving on the lens. Weather also affects the surrounding environment by e.g. changing the surface reflectance properties in rainy weather. If colored markers are used for load object tracking, sensitivity to natural light in outdoor environments occur. \citep{Kawai12} 

If multiple depth maps are used to acquire a complete model of an object, then data acquisition, registration between different viewpoints, and integration of the different registered views are compulsory steps in modeling.  Multiple camera view registration is a classic technique that can estimate geometric surfaces of objects in more complete detail than a single camera view depth mapping. Multiple view registration is outside of the scope of this thesis, but an interested reader may find extensive research in literature on complete object modeling using multiple registered camera images. \citep{Chen91}

All geometric acquisition techniques either use an active perception sensor that emits energy in the environment, or use a passive perception sensor that only detects energy reflected or radiated from the environment. Triangulation and structure from motion (SFM) are techniques that uses passive sensing of the environment with cameras. Laser scanning and structured light patterns use active sensing of the environment using light sources and cameras.
% kinect
Triangulation is the basic mechanism that most 3d acquisition techniques are based on, which is why it is discussed before other techniques are introduced.

\subsection{Triangulation}
\label{subsection:triangulation}
%The lenght of the camera separation in a stereo camera system, also called the baseline width, affects the triangulation accuracy of the depth measurement setup to some extent.

Triangulation solves distance z to a point by measuring two angles from known points that are located at two ends of a fixed baseline. Thus, the known camera viewpoints and the unknown point form a triangle. Before the invention of the Global Positioning System (GPS), mapping of land areas (calculating distances and directions of landmark features) was performed using triangle meshing with the triangulation technique.

The term triangulation was originally used in trigonometry calculus, but in machine vision systems it refers to the indirect process of measuring image feature depths using two sensors. Triangulation system that uses two cameras is called a passive triangulation system, and a system that uses a single camera and a light pattern projector is called an active triangulation system. In this thesis only passive triangulation techniques are used, which is why the two camera simple stereo geometry will be introduced next.

\subsection{Structure From Motion}
\label{subsection:structure_from_motion}

Structure from motion (SFM) is a machine vision technique that uses only a single camera and a sequence of images to  reconstruct a 3d geometry from a scene. Many other similar techniques exist, for example, structure from silhouette
, and structure from structured light, which are just applications of the SFM technique.

Structure from motion is similar to biological visual systems that easily infer 3d structure from motion with very little a priori knowledge of the world. For example, if a person closes one eye, it is possible to understand the 3d geometry of the surrounding environment only by moving in it, and the structure from motion mechanisms in the brain computes the correct geometry without binocular vision.

A simple structure from motion system is able to reconstruct static scenes with a moving camera, but reconstructing a moving scene e.g. a busy process hall with a crane load object in the middle is a more difficult problem. In the case of reconstructing dynamic environments with a single camera, some additional problems must be addressed, such as what image regions correspond to moving objects and which do not, and how does the illumination change spatially and temporally.

%Motion field
\subsubsection{Motion Field And Optical Flow}
\label{subsubsection:motion_field_and_optical_flow}

In SFM processing, the reconstruction of a 3d scene from a sequence of images is based on motion field computation. Motion field is a 2d vector field of velocities of image points induced from a scene viewed by the camera. It represents the projections of 3d motion of visible features. Only approximations of the ideal motion field can be computed, for example the optical flow is such an estimate of the motion field.

Optical flow describes the apparent motion of an object in a visual scene. It can be estimated using differential methods, feature-based methods, or phase correlation methods. In practice, a good differential method that is easy to implement and use is a method originally published by Lucas and Kanade, which is referred to as an optical flow algorithm in \citep{Trucco98, Lucas81}

In Lucas's method the optical flow is estimated using the image brightness constancy equation. The image brightness constancy equation computes the optical flow only in the direction of the normal of the spatial image gradient as shown by the aperture problem. The aperture problem shows in more detail how it is not possible to understand the actual direction of movement of a spatial gradient seen through an aperture.  

The optical flow is well estimated using the brightness constancy equation for some cases where the objects are assumed to exhibit lambertian surface properties, be illuminated by a point light source at the distance of infinity, and  no photometric distortions are present. Under the previously mentioned assumptions the error of the estimated normal of motion field is small at image points with high spatial gradient. The error is zero only for components whose motion is translational only, or whose illumination direction is parallel to the angular velocity. 

Motion field of an SFM system is similar to a disparity map in a stereo vision system: they both represent a point displacement map. While motion field is a differential concept, the disparity map is not, which means that the motion field is an estimate of the point displacement map.

\subsubsection{Structure From Motion Challenges In Crane Environment}
\label{subsubsection:structure_from_motion_challenges_in_crane_environment}

Structure from motion is a mathematically challenging approach to reconstruct a 3d geometry using a single camera, mostly suited for applications of reconstructing static 3d geometries from aerial images, city streets or landmark items. It is not well-suited to solve the problem of load object measurement in an overhead crane environment for three main reasons.

First, the load object measurement must be available when the camera is not moving and the load object is not moving. Structure from motion reconstruction will not be available in such an event. 

Secondly, the overhead crane environment is dynamic containing multiple moving objects, and a camera platform that moves relative to the objects. In such dynamic cases, a simple SFM process described earlier cannot solve a rigid transformation between consecutive frames since the environment is morphing between frames. A general multibody structure from motion solution has yet not been formulated, but theoretical approaches and requirements for a system that can process dynamic multibody environments using SFM have been researched\citep{Ozden10}.

Last, an empty overhead crane hall contains a planar scene, which is difficult to reconstruct correctly in a SFM system. Coplanar points have similar depth in an image sequence, and for example, for a dense 3d motion and structure algorithm it is assumed that local observer motions include large variations in depth, which is not the case in a planar scene\citep{Trucco98}.

% another snippet
%A histogram is a two-dimensional graphical representation of an image grey-level distribution. The grey-levels correspond to the image intensity. As depicted in figure X, the X axis represents the number of independent grey levels that the image contains. The grey-levels range from 0 to N levels. For example, in an 8-bit single channel image there can be up to 255 independent different tones of gray. The Y axis represents per centage information about a single grey-level ranging from 0\% to 100\%. Thus, the histogram essentially displays a distribution of all grey-levels.
%end of chapter 3
\chapter{Stereoscopic Machine Vision}
\label{chapter:stereographic_machine_vision}
% stereoscopic means to perceive and/or see in 3d | stereographic means projecting a sphere onto a plane
Stereoscopic perception is a natural phenomenon first discovered in human and animal vision systems. Stereoscopy in human vision is based on stereopsis effect, which means perceiving depth in a scene using binocular vision with two monocular vision eyes. The same depth perception can be programmed to a computer, but instead of eyes, cameras are used. Instead of the brain, a PC is used to recover and process depth information from a pair of camera images. A calibrated two-camera system that is capable of extracting depth information from objects in such a way is most often called a \emph{stereo vision system} or a \emph{3d vision system}.

While people perceive and estimate object sizes with the help of their previous knowledge of the world and their implicit stereo vision calibration by the brain, a machine vision system must be carefully calibrated to output proper metric measurements for viewed objects. For simplicity, in this thesis a single-time calibration is considered and no machine learning techniques are used to improve the calibration at a later time.

After capturing a pair of images and processing them, it is possible to request a 3d coordinate value for almost any feature found in one of the captured stereo images. In this way, the machine vision system can deliver coordinates for many object features in a scene simultaneously, which is quite impossible for a human to do. By selecting visible features of the load object in the 2d image, the 3d coordinates for those points are recovered. Finally, the load object dimensions are recovered by taking difference of the minimum and maximum coordinates along all axes. Transforming the 2d image into a partial 3d feature model is important since the measurement is ultimately done by requesting depths of points of interest and measuring their difference. While the approach does have its limitations, a partial 3d model is acquired that can be used to successfully find a bounding volume that contains the load objects. Improvements and limitations are discussed more in depth in chapter \ref{chapter:evaluation}: evaluation.

\section{From 2d to 3d}
\label{section:from_2d_to_3d}
%Different 3d information recovery techniques are introduced in section \ref{section:environment_acquisition_techniques}. 

Depending on the scientific application at hand, 3d vision systems used in engineering applications solve problems of 3d scene reconstruction, understanding of object properties, or minimisation problems. In this thesis, a calibrated stereo camera setup is used to solve the problem of a 3d scene reconstruction, which is a very typical machine vision application suited for solving the load object geometry measurement problem. The two other 3d vision problem domains, understanding of object properties, and minimisation computation are briefly visited in sections \ref{section:point_cloud_library_operations} and \ref{section:bounding_volume_computation}. \citep{Sonka07}

The theory of understanding 3d objects out of a 2d image and processing them into representable information was first described by Marr in 1982 in his work \emph{Vision}. Marr ideated a description framework for creating 3d models from 2d data, which the implementation chapter \ref{chapter:implementation} loosely supports. The field of machine vision has advanced a lot since 1982, which is why parts of Marr's theoretical approach are less relevant today. Still, the ideas of a description framework are valid even today.

According to Marr's theory, machine vision systems should aim for a more general representation of the visualised environments in general, and not just create another vision application suited for a single specific purpose. In his work it is suggested that representation of objects will improve when a pixel-based viewer centered intensity image description is transformed into a feature-based object centered description.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{marrs_framework.jpg}
    \caption{Marr's representational framework. Image from \citep{Marr82}.}
    \label{fig:marrs_framework}
  \end{center}
\end{figure}

The transformation from 2d to 3d contains 3 steps. The first step transforms the pixel representation of a 2d image into a surface representation called primal sketch. The second step transforms the primal sketch into an oriented surface model that maps depths of visible surfaces in a 2.5-dimensional description. The third and final step transforms the surface orientation model into a full feature based 3d model. The equivalent steps are presented in figure \ref{fig:marrs_framework}.

In this work, the implementation of the 3d reconstruction skips primal sketching, and processes the pixel-based description straightforwardly into a disparity depth map, and onto a partial 3d model. In a primal sketch, there is knowledge available about surface boundaries in the depth map, which is not the case with the disparity map in the current implementation. The 3d data is reprojected from the disparity map into a point cloud as a single entity, and only processed after modeling. Object models or feature surfaces are not known during the reconstruction, which means that the machine vision process only produces 3d data and does not track 2d image features at all. The object-centered descriptions of the reconstructed scene described in Marr's framework are partially implemented later in section \ref{section:point_cloud_library_operations}.

\section{The Pinhole Camera Model}
\label{section:the_pinhole_camera_model}
%1. pinhole camera model
%2. orthographic projection
%3. scaled orthographic projection
%4. paraperspective projection
%5. perspective projection
%source: NUS CS4243 camera.pdf slide
%The real-world object projection on the camera image plane is a lossy transformation where a lot of information is lost due transformation. For example, when a cube is projected on the image plane, we cannot compute the texture of all 6 cube faces anymore because the data of all non-visible cube faces is lost in the image plane.
%In general, a perspective projection, or central projection, is the de facto type of projection for human beings. 
%In perspective projection objects that are far away appear smaller than objects that are near. This phenomenon is called a perspective. 

The pinhole camera model maps 3d objects onto a 2d image plane using simple pinhole camera geometry. A three-dimensional object in real space $\mathbf{R}^3$ is mapped into a two-dimensional projection in the subspace $\mathbf{R}^2$ according to $\mathbb{R}^3 \mapsto \mathbb{R}^2$, also known as a perspective projection. The depth component z of a 3d coordinate is lost in the transform, which is natural for any projective camera models. The transform is irreversible, which means that the original 3d coordinates cannot be recovered from a projective 2d image. The pinhole camera model is introduced because it is the model used in OpenCV implementation (in chapter \ref{chapter:implementation}) and it is the simplest approximation that can be used to describe the mathematics behind a real camera image formation.\citep{Sonka07, OpenCVWeb}.

%Typical camera model is usually based on ortographic projection or perspective projection models. In 3D imaging a suitable camera model would be the perspective projection model with lens distortion modeling included. The selection really depends on the applications and its accuracy requirements for model accuracy, but in simple computer vision applications the linear ortographic model would be appropriate. 

\subsection{Pinhole Camera Geometry}
\label{subsection:pinhole_camera_geometry}
%\begin{itemize}
%\label{list:intrinsiccameraparameters}
%\setlength{\itemsep}{0pt}
%\item -fa = represents scaling in the $u_a$ axis
%\item -fb = shear coefficient that gives the alignment difference of camera coordinate system axis $x_c$ and affine image coordinate system axis $u_a$ at the length of focal length in pixels in the direction of affine image coordinate system axis $v_a$.
%\item -fc = represents scaling in the $v_a$ axis
%\item $u_0$ = u coordinate for the optical axis intersection on the image plane on the $u_a$ axis
%\item $v_0$ = v coordinate for the optical axis intersection on the image plane on the $v_a$ axis
%\end{itemize}
%The width of the image sensor can be related with field of view (FOV) angle by
%\begin{equation}
%w = 2f\tan{\frac{\theta_{fov}}{2}}
%\label{equation:width}
%\end{equation}
%If we consider the effect of resolution on the disparity computing, or depth value computing, we can start with the number of pixels in the image sensor by equation
%\begin{equation}
%number of pixels = wh = \frac{w^2}{a}
%\end{equation}
%Furthermore, we can substitute width \emph{w} from \ref{equation:width} getting the equation for the number of pixels as in \cite{Gallup08}:
%\begin{equation}
%nop = \frac{{z_{far}}^4}{{\epsilon_z}^2} \frac{4\tan^2{\frac{\theta_{fov}}{2}}}{b^2 a}
%\label{equation:zdepth}
%\end{equation}
%As we can see from equation \ref{equation:zepth} the increase in pixel resolution to match a specified accuracy also must match the depth resolution proportionally to ${u_{0},v_{0}z$_{far}}^4$. Consequently, the required increase in resolution may be impossible due to engineering limitations of new hardware or computations limits of increased overheard due to increase in number of pixels.
%The increase in stereo processing computation overhead is tightly coupled with the selected image resolution and depth error. According to \cite{Gallup08} the number of pixel comparisons needed is in the growth range of $ \Omega({z_{far}}^6 {\epsilon_z}^-3)$. For example, if we wish to extend the depth range by a factor of 3 the required number of comparisons would be $3^6 = 729$ times more computationally expensive.
%When faced with this kind of image computation problem that should ideally be run in real-time the design of the imaging system must be carefully planned. It should be noted that increase in image resolution seems not to solve a stereo correspondence problem, but it may severely increase the computation time. 

The pinhole camera is a simple box that has an ideal pinhole aperture on one of its sides, see Figure \ref{fig:pinhole_camera}. The camera forms an upside down image of an object on the imaging plane $\pi$ on the opposite side of the aperture. All objects located at a distance further than $f$ units away from the pinhole will be imaged sharp. The positive z axis of the camera frame $O_c$ ideally coincides with the camera optical axis $I_o$ (figure \ref{fig:pinhole_camera dashed line}) so that the optical axis is perpendicular to the image plane and pointing towards the scene. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{pinhole_camera.pdf}
    \caption{A simple pinhole camera projects imaged objects upside down on an image plane. The image formation is based on similar triangles between the optical axis $I_o$ and the red light ray (hypothenus).}
    \label{fig:pinhole_camera}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{central_projection.pdf}
    \caption{A central projection model equivalent of a simple pinhole camera image formation. The model projects imaged objects on an image plane $\pi$ in front of the focal point (center of projection).}
    \label{fig:central_projection}
  \end{center}
\end{figure}

Figure \ref{fig:central_projection} shows a \emph{central projection model} that is often used to simplify formulation of the mathematics in the pinhole camera image formation. In a pinhole camera, the real image is formed behind the lens system (pinhole in this case) at $z = -f$, whereas in the central projection model, the focal point is used as the center of projection and the virtual image is formed in front of the lens system at $z = +f$. Using the simpler central projection model, the object and its projected image are related by a thin lens equation

\begin{equation}
\frac{1}{f} = \frac{1}{p} + \frac{1}{q}
\label{eq:lens_law}
\end{equation}

where f is the focal length of the thin lens. The image is formed on the image plane in sharp detail as long as the 3d object is not nearer than $f$ units from the focal point of the lens. Using similar triangles it can be shown that the point $P$ coordinates are projected onto the image plane point as $p$ according to

\begin{equation}
x = f\frac{X}{Z}
\label{eq:image_projection}
\end{equation}

\begin{equation}
y = f\frac{Y}{Z}
\label{eq:image_projection2}
\end{equation}


In homogenous form, a projected point $\tilde{p}$ = ($\tilde{x}$,$\tilde{y}$,$\tilde{z}$) can be further formulated as

\begin{equation}
\tilde{x} = f\frac{X}{\tilde{z}}, \tilde{y} = f\frac{Y}{\tilde{z}}, \tilde{z} = Z
\label{eq:image_projection}
\end{equation}

where the origin of the homogenous coordinate system is point 

\begin{equation*}
$\tilde{O_c} = \begin{bmatrix}
0 \\
0 \\
1 \end{bmatrix}
\end{equation*}
.

This coordinate system can be shown in homogenous matrix form as 

\begin{equation} \tilde{p} =
\label{eq:homogenous_matrix_form}
\begin{bmatrix}
f & 0 & 0 \\
0 & f & 0 \\
0 & 0 & 1\end{bmatrix}
\begin{bmatrix}
X \\
Y \\
Z \\
\end{bmatrix}
\end{equation}

Point P can be written in homogenous matrix form as

\begin{equation*}
\tilde{P} = (X,Y,Z,1)
\end{equation}

so that the projection becomes a linear matrix operation

\begin{equation} \tilde{p} = 
\label{eq:homogenous_world_point}
\begin{bmatrix}
f & 0 & 0 & 0 \\
0 & f & 0 & 0 \\
0 & 0 & 1 & 0 \end{bmatrix}
\tilde{P}
\end{equation} 

or

\begin{equation}
\label{eq:homogenous_projection}
\tilde{p} = C\tilde{P}
\end{equation}

where C is a 3 x 4 matrix called the camera matrix, and $\tilde{p}$ and $\tilde{P}$ are the homogenous image plane projection and 3d object surface point coordinates, respectively. The camera matrix can be factored into form

\begin{equation}
\label{eq:factor_camera_matrix}
\tilde{p} = KP_{0}\tilde{P}
\end{equation}

where K is the camera calibration matrix that contains the intrinsic parameters of the camera, and $P_0$ is the projective matrix of the camera model. Since stereo calibration later requires the model for both intrinsic and extrinsic parameters, a general model including both intrinsic and extrinsic parameters can be written on the basis of equation \ref{eq:factor_camera_matrix} as

\begin{equation}
\label{eq:general_camera_model}
\tilde{p} = KP_{0}T_{c}^{-1}\tilde{P}
\end{equation}

where $T_{c}^{-1}$ is the extrinsic calibration matrix containing at least 6 parameters that describe translation and orientation of the camera from the world frame origin. All matrices K, $P_0$, and $T_{c}^{-1}$ can be combined into a single 3 x 4 camera matrix C that performs translation, rotation, scaling, and perspective projection\citep{Corke11}. 

\subsection{Extension To Pixel Frame}
\label{subsection:extension_to_pixel_frame}

After formulating the perspective projection $\tilde{p} = C\tilde{P}$ in equation \ref{eq:homogenous_projection} the projected image point $\tilde{p}$ must be related to the pixel frame grid that the digital sensor array contains. The image sensor contains light sensitive photosites that are arranged in an array whose width \emph{w} and height \emph{h} determine the resolution of the camera. In a modern digital camera, the pixels are rectangular with no skew, which means that the angle between the width and height component is strictly 90 degrees. The pixel coordinates are by convention in a non-negative $u- v-$ coordinate frame whose origin is located at the site of the top left corner pixel of the imaging array. This coordinate frame is titled the \emph{image frame} and it is coinciding with the planar image projection plane $\pi$.

The principal point $(u_{0},v_{0})$ of the image sensor is the center of the sensor array where the principal ray enters the camera through the pinhole aperture perpendicularly to the image plane. If the principal point is shifted due to lens alignment error, then the shift can be adjusted by shifting the principal point so that the new principal point site is at 

\begin{equation}
\label{eq:principal_point_u}
u_0 = 
\end{equation}



using the intrinsic camera calibration matrix K. A camera calibration matrix that can adjust for lens alignment error and take pixel skew into account is written so that

\begin{equation} K =
\label{eq:extended_camera_calibration_matrix}
\begin{bmatrix}
f_x & 0 & u_{0} \\
0 & f_y & v_{0} \\
0 & 0 & 1
\end{bmatrix}
\end{equation}

where $f_x$ and $f_y$ are the orthogonal focal length components, and $u_{0}$ and 
$v_{0}$ are the principal points for the camera. The K matrix can be written with a pixel skew correction value in component K(1,2), but with modern precise semiconductor technologies the u and v axes are very precisely orthogonal, which is why in Equation \ref{eq:extended_camera_calibration_matrix} the skew term is left zero\citep{Corke11}.

The point in the world according to the camera coordinate system now transformed to point
$X_c$. Next, the point will be projected onto the image plane $\pi$ according to the pinhole camera model. 
Point $X_c$ projects onto the image plane as Euclidean point $U_c$ where the projected point can be computed from the similar triangles of image \ref{image:projectedpoint}. 



















The image point that the camera outputs is the affine transformed $\textbf{U}_c$. Next, we must formulate a homogenous coordinate representation that allows to compute the affine transformation with a $3x3$ matrix multiplication for \ref{equation:projectedpointUc}.

We can represent a homogenous coordinate point in the image plane $\pi$ as 

\begin{equation}
\label{equation:affinedefinition}
\tilde{u} = \Biggl[ \begin{array}{c}
U \\
V \\
W \end{array} \Biggl]
\end{equation}

so that a Euclidean distance representation of point $\tilde{u}$ is 

\begin{equation}
\textbf{u} = \Biggl[ \begin{array}{c}
u \\
v \end{array} \Biggl] = \Biggl[ \begin{array}{c}
\frac{U}{W} \\
\frac{V}{W} \end{array} \Biggl]
\end{equation}

When multiple coordinate points (also known as point clouds) are perspective projected into an image frame the transformations can be computed with the same square matrix used for single point transform. If the point cloud $P$ contains $n$ points, a matrix can be written so that it contains the set of points as column vectors in format

\begin{equation}
P = \Biggl[ \begin{array}{ccccc}
X_0 & X_1 & X_2 & \dots & X_n \\
Y_0 & Y_1 & X_2 & \dots & Y_n \\
Z_0 & Z_1 & X_2 & \dots & Z_n \end{array} \Biggl]
\end{equation}

Multiplying point cloud P with the camera projection matrix will result in a 3d point cloud transformed into an image projection. Later chapters in the thesis will introduce how this property is inverted, and 2d points can be reprojected back into 3d coordinates using an inverse process.

\section{Intrinsic Camera Calibration}
\label{subsection:intrinsic_camera_calibration}

% T. Briggs article on autocalibration of a moving camera...?
%Camera calibration can be weak or strong. If strong camera parameters are known it is possible to calculate image scene Euclidian metrics. If weak camera parameters are known, only a pixel to pixel transformation from one image to another is possible, for example, an epipolar projection transformation can be done with weak parameters \cite{Rander97}.
%+ Why do we have to calibrate the cameras?
%Many fast but inaccurate algortihms for calibration parameter estimation have been introduced in the literature. The increase in processing power of computing platforms helps to develop new non-linear camera calibration methods that are more accurate. A subpixel accuracy of 1/50 of a pixel can be achieved with a modern CCD imaging cell if such non-linear calibration is used\ref{Heikkila00}.
%\begin{itemize}
%\label{list:intrinsic_parameters}
%\setlength{\itemsep}{0pt}
%\item Focal length
%\item Principal point
%\item Skew coefficient
%\item Distortion coefficients
%\end{itemize}
%Focal length is a 2x1 vector that reports the linear focal length components for projection of the final pixel coordinates $x_p$ and $y_p$ onto the image plane. Principal point is a 2x1 vector that reports the intersecting point of the optical axis and the image plane. After a distortion model is applied using the 5x1 vector to a normalised pinhole image projection an equation for the distortion corrected projection is
%follow the notation and the workflow of Heikkilä's paper, and the camera calibration toolbox provides its implementation\citep{Fetic12}. 

Intrinsic camera calibration is the process of estimating the unknown coefficients of the camera calibration matrix K (\ref{eq:extended_camera_calibration_matrix}). Ideally, the calibration technique should estimate the camera parameters in an unbiased manner. Currently used calibration methods assume unbiased estimation, which is not true for most techniques, but the results can still be good. In this thesis, a MATLAB Camera Calibration Toolbox by Jean-Yves Bouguet was used to calibrate the stereo camera heads individually. The camera calibration toolbox includes a simple polynomial lens distortion model that extends the pinhole camera model introduced in section \ref{section:the_pinhole_camera_model}.

\subsection{MATLAB Camera Calibration Toolbox}
\label{matlab_camera_calibration_toolbox}

The intrinsic calibration parameters for the stereo cameras used were computed using Jean-Yves Bouguet's Camera Calibration Toolbox for MATLAB. The camera calibration toolbox uses internal camera model described by Heikkilä and Silven \citep{Heikkila97}, which supports the pinhole camera model introduced in section \ref{section:the_pinhole_camera_model}. The toolbox uses rectangular or circular calibration grids for calibration. A rectangular standard 9x6 crossings calibration grid was used to 

When a real imperfect camera lens system is used for image capture, lens distortions must be compensated by software to enable successful stereo block matching after image rectification. Lens distortions may include geometric distortions, such as tangential and radial distortions, and other imperfections, such as optical aberrations (chromatic aberration, spherical aberration, coma) and astigmatism. Geometric distortions hinder the performance of machine vision processes most, which is why the distortion model mainly corrects for the geometric qualities of the image. The need for different distortion model components depends on the used lens system, and it is probable that a low-quality webcam calibration requires a more complex lens distortion correction model than a high-quality built camera. The lens distortion model is added to the perspective projection 

\begin{empheq}[left=\empheqlbrace]{align}
\label{eq:projection_to_image_plane}
&x_p = f_{x}\Big(x_{d}+\alpha_{c}*y_{d}\Big)+pp_{x} \\
&y_p = f_{y}*y_{d}+pp_{y}
\end{empheq}

and added to normalised image coordinate compomenents $x_d$ and $y_d$ following the calibration procedure described in \citep{Heikkila97}. 


Bouguet's camera calibration toolbox provides lens distortion models up to a 6th order polynomial, but in the calibration of the stereo pair in the \emph{Himmeli} platform (see section \ref{section:perception_sensor_platform_himmeli}) only a 4th order model was needed, leaving the 5th distortion coefficient in the 5x1 distortion coefficient vector $kc$ zero. Thus, the distortion corrected normalised point coordinates $x_{d}$ and $y_{d}$ follow:

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{extrinsic_parameters_left_cam.pdf}
    \caption{Left camera locations seen in a calibration grid centered world frame.}
    \label{fig:extrinsic_parameters_left_cam}
  \end{center}
\end{figure}

\begin{table}\centering
\ra{1.3}
\caption{Results for the left GigE $\mu$Eye camera calibration parameters from MATLAB Camera Calibration Toolbox (rounded with 7 digits).}
\begin{tabular}{@{}lcclc@{}}\toprule
Left camera calibration results\\
\midrule
Focal length &  $f_x$ & 470.5395354 \\
 & $f_y$ & 470.0528248 \\
Principal point & $u_0$ & 361.4851212 \\
 & $v_0$ & 297.1134158 \\
Skew coefficient & $\alpha$ & 0.0000000 \\
Distortion coefficients (5x1 vector) & $k_{c}$ & [-0.0073703; \\
&  & 0.0248006; \\
&  &  0.0050783; \\
&  &  0.0011502; \\
&  &  0.0000000] \\
Focal length uncertainty & $\delta f_x$ & 3.1313055 \\
 & $\delta f_y$ & 3.0924522 \\
Principal point uncertainty & $\delta u_0$ & 1.8698244 \\
 & $\delta v_0$ & 1.9314148 \\
 Skew coefficient uncertainty & $\delta \alpha$ & 0.0000000 \\
Distortion coefficients uncertainty & $\delta k_{c}$ & [0.0036557; \\
&  &  0.0032499; \\
&  &  0.0009925; \\
&  &  0.0009502; \\
&  &  0.0000000] \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}\centering
\ra{1.3}
\caption{Results for the right GigE $\mu$Eye camera calibration parameters from MATLAB Camera Calibration Toolbox calibration (rounded at 7 digits).}
\begin{tabular}{@{}lcclc@{}}\toprule
Right camera calibration results\\
\midrule
Focal length &  $f_x$ & 483.9712800 \\
 & $f_y$ & 483.2710360 \\
Principal point & $u_0$ & 379.5935499 \\
 & $v_0$ & 308.8688683 \\
Skew coefficient & $\alpha$ & 0.0000000 \\
Distortion coefficients (5x1 vector) & $k_{c}$ & [-0.0002056; \\
&  & 0.0181529; \\
&  &  0.0047928; \\
&  &  0.0000514; \\
&  &  0.0000000] \\
Focal length uncertainty & $\delta f_x$ & 2.9385693 \\
 & $\delta f_y$ & 2.8655623 \\
Principal point uncertainty & $\delta u_0$ & 2.0605689 \\
 & $\delta v_0$ & 2.0783195 \\
 Skew coefficient uncertainty & $\delta \alpha$ & 0.0000000 \\
Distortion coefficients uncertainty & $\delta k_{c}$ & [0.0036847; \\
&  &  0.0026363; \\
&  &  0.0010737; \\
&  &  0.0010460; \\
&  &  0.0000000] \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{distortion_model_right_cam.pdf}
    \caption{Right camera lens distortion model with vector fields.}
    \label{fig:distortion_model_right_cam}
  \end{center}
\end{figure}

\begin{empheq}[left=\empheqlbrace]{align}
\label{eq:normalised_distortion_corrected_pinhole_projections}
&x_d = \Big(1 + kc_{1}r^2 + kc_{2}r^4 + kc_{5}r^6\Big)x_{n}+dx \\
&y_d = \Big(1 + kc_{1}r^2 + kc_{2}r^4 + kc_{5}r^6\Big)x_{n}+dy
\end{empheq}

where dx and dy are the tangential distortion vectors

\begin{empheq}[left=\empheqlbrace]{align}
\label{eq:tangential_distortion_vectors}
& dx = 2kc_{3}xy+kc_{4}\Big(r^2 + 2x^2\Big) \\
& dy = kc_{3}\Big(r^2 + 2y^2\Big)+2kc_{4}xy
\end{empheq}
.

The parameter $r$ in equation \ref{eq:normalised_distortion_corrected_pinhole_projections} is short for $r^2 = x^2 + y^2$. The tangential distortion coefficients calibrate decentering of the image and other defects in compound lens structure. The radial distortion coefficients corrects a barrel or pincushion distortion depending whether a telephoto lens system or wide angle lens system is used. A more in-depth analysis of the distortion modeling of radial and tangential distortions may be found in the original publication by Brown\citep{Brown71}.

The output of the calibration procedure will be a camera matrix K that relates the pixel coordinates in the image reference frame to the distorted projections of the actual objects in the camera reference frame. The camera matrix K is defined as

\begin{equation} K =
\label{eq:camera_matrix_k}
\begin{bmatrix}
f_x & \alpha_{c}*f_x & pp_x \\
0 & f_y & pp_y \\
0 & 0 & 1\end{bmatrix}
\end{equation}

where $\alpha_{c}*f_x$ is the skew coefficent stating the angle between the u and v pixel axes. K matrix relates the image reference frame and the camera reference frame so that

\begin{equation}
\label{eq:image_ref_frame_relation_to_camera_ref_frame}
\begin{bmatrix}
x_p \\
y_p \\
1\end{bmatrix}=K\begin{bmatrix}
x_{d} \\
y_{d} \\
1\end{bmatrix}
\end{equation}

Modern cameras do not have all the distortion properties that older cameras had, and some coefficients, such as the skew coefficient (suggesting rectangular pixels), and tangential distortion are normally defaulted to zero. For example, it is not recommended to use the 6th order distortion model for normal field of view cameras, but instead a 4th order polynomial should be used \ref{BouguetRef}. In most cases the principal point of the camera is located in the center of the sensor array, and its estimation is not recommended because it is difficult to estimate correctly.  
% number of calibration images available affect the selection distortion models
The coefficients of the camera calibration matrix K \ref{equation:cameracalibrationmatrix} contain the intrinsic camera parameters. If the coefficients of K are known, it is possible to extract metric values from the image plane \cite{Sonka07}. If a more complex camera model is used, the matrix K may have larger dimensions, but the intrinsic parameters still are the coefficients of K.  

\section{Extrinsic Camera Parameters}
\label{section:extrinsic_camera_parameters}

Since the camera lies in some arbitrary world coordinates, and the detected scene changes according to the camera orientation, we can easily see that the extrinsic camera parameters change when the camera pose changes in the world coordinate system. The relation between the world coordinate system and the camera coordinate system can be computed with an Euclidean transformation consisting of a translation and rotation. 

Extrinsic camera parameters describe the position and orientation of all the camera locations in a multiple camera stereo system relative to a world coordinate frame $W$. In stereo vision applications, extrinsic camera parameters are additionally used to describe the relative transformation from the left camera frame origin to the right camera frame origin. Thus, the extrinsic parameters can relate two viewpoints of the stereo camera setup to each other enabling a depth map computation. If an arbitrary world point $P_w$ is found in a world coordinate frame whose origin $O_w$ is explicitly defined in some world location, the translation and rotation into some camera coordinate system can be formulated as   

\begin{equation}
P_c = \begin{bmatrix}
x_c \\
y_c \\
z_c\end{bmatrix} = R(P_w - t)
\end{equation}

where $x_c$, $y_c$, and $z_c$ are the coordinates of point $P_w$ transformed into a camera frame C. The homogenous coordinate frame transformation is formulated similarly

\begin{equation}
\tilde{P_c} = \begin{bmatrix}
\tilde{x_c} \\
\tilde{y_c} \\
\tilde{z_c} \\
1 \end{bmatrix} = \tilde{R}(\tilde{P_w} - \tilde{t})
\label{eq:homogenous_transformation}
\end{equation}

where R is a $4x4$ matrix concatenating multiple Euler angle rotations. Since rotations in Euler angle system are applied to the rotating coordinate frame, the order of the axis rotations is crucial unless the matrices are commutative. Commutativity is not a common property in transformation matrices, thus, the order of applying of rotation matrices changes the orientation outcome accordingly. The combinations of Euler angle rotations are presented in detail in Introduction to Robotics by Craig in Appendix B \emph{The 24 angle-set conventions} \citep{Craig05}. 

%If only the extrinsic parameters of a multiple camera system are known and no intrinsic parameter knowledge is available, 3d reconstruction of the scene is possible with some limitations. In case of a two camera system, the scene 3d geometry can be recovered only up to a scale, 
%at least 5 cameras must be used to reconstruct a scene  
    %If the intrinsic parameters are known, then a metric reconstruction may be obtained using a set of calibration points. The very minimum amount of points in a two viewpoint stereo setup is 5 points, but usually stereo calibration objects, such as calibration grids, contain more points. There is a risk  of acquiring multiple solutions using 5 points (the very minimum).
    %TODO Tähän metrisiä rekonstruktio tekniikoita niin perkeleesti

\subsection{Coordinate Frames}
\label{subsection:coordinate_frames}

The scene that the camera sensors are viewing is called \emph{world frame} (or \emph{scene} interchangeably), which uses a standard (x,y,z) right-hand 3d coordinate convention. The origin of the world coordinate system is $O_w$ located at a corner of the overhead crane hall floor, for example. In the works, the origin of the world frame was coincident with the \emph{camera frame} (or \emph{sensor frame}) origin $O_c$ since the load object measurement computation only requires relative coordinates to function properly. Relative coordinates can be used as long as metricity of the scene geometry is guaranteed, which means that the translation of the camera frame to the world frame need not be precisely correct. Thus, the world frame origin and the camera frame origin both are located at the camera sensor array center. Third frame, the \emph{image frame}, is needed to convert between metric camera frame locations and the pixel array values of the image. 

The world frame depicts a three-dimensional system where a point lies in an Euclidian space $R^n$ and its span is $R^3$. The world coordinate frame follows the framework used in a report by Terho in 2010 \cite{Terho10}. A precise transformation from the world frame to the camera frame requires position and orientation information of the camera, which can only be estimated to a certain accuracy. Two options could be used for positioning: GPS signal or crane end effector position services. GPS signal is not very good for an indoor application with the accuracy of 1-10 meters, and as such the crane positioning services should be used instead. The orientation of the camera can be acquired using an inertial measurement unit (IMU) installed in the camera platform. In this thesis, the world frame transformation was not solved since it was not needed. The transformation $T_{c}^{w}$ from camera frame to the world frame would be needed if the software is integrated with other software and a common coordinate frame is required.

The camera sensor array defines the first two orthogonal dimensions of the camera coordinate frame: X-Y- axis plane is parallel to the camera image array, and the positive Z-axis points out from the image plane perpendicularly towards the lens system of the camera. The camera frame positive Z-axis is the optical axis of the camera by definition. The origin $O_c$ of the camera frame is located in the focal point of the camera, that is the point through which all the light rays reflected through the lens system go.

The image frame is the 2d projection plane where the three-dimensional point of the imaged scene is transformed with perspective transformation and pixelation process. The plane of the image frame is fully coincident with the camera frame image. The origin of the image frame is in the upper left corner of the sensor array, thus, having only non-negative integer values that map pixel intensity values to the pixel grid of the sensor array.

\section{Stereo Triangulation}
\label{section:stereo_triangulation}

%todo joku intro tähän alkuun

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{simple_stereo_geometry.pdf}
    \caption{Simple stereo geometry shown in a central projective model. Distance z to an arbitrary point P may be solved using similar triangles. Image adapted from Trucco and Verri \citep{Trucco98}.}
    \label{fig:simple_stereo_geometry}
  \end{center}
\end{figure}

%is a passive range acquisition mechanism that produces a range measurement from the solution of the correspondence problem from two differing images. The difference between the images should be a translational shift along a camera baseline axis, for example a stereo pair using simple stereo geometry will suffice. Tri
%Often, a stereo camera is used, and the correspondence problem is solved using rectification and block matching techniques.

Triangulation is highly dependent on the outcome of the correspondence problem solution (see section \ref{section:stereo_correspondence_problem} for details). Assuming that the correspondence problem is solved, the distance to a point P may be calculated using a two-camera geometry according to figure \ref{fig:simple_stereo_geometry}. $\pi_L$ and $\pi_R$ are the image planes for the left and right cameras accordingly, and $p_r$ and $p_l$ are the projections of the point P on the image plane. 

Figure \ref{fig:simple_stereo_geometry} shows two similar triangles on the left camera geometry whose hypothenuses are vectors ($O_l$, $p_l$) and ($O_l$, $P$). The z coordinate of the point $P$ is related according to the similar triangles so that

\begin{equation}
\frac{\frac{1}{2}b-x}{Z} = \frac{x_L}{f} 
\label{eq:similar_triangles1}
\end{equation}

\begin{equation*}
x_L = p_l - c_l
\end{equation}

where $b$ is the baseline between camera focal points, and $x_L$ is the distance of the projection $p_l$ from principal projection point $c_l$. Similarly, the right camera geometry will give a relation

\begin{equation}
\frac{\frac{1}{2}b+x}{Z} = \frac{x_R}{f} 
\label{eq:similar_triangles2}
\end{equation}

\begin{equation*}
x_R = c_r - p_l
\end{equation}

By definition, disparity is the shift of the image feature in the $x$ coordinate direction, which can be written

\begin{equation}
d = x_R - x_L
\label{eq:disparity_shift}
\end{equation}.

according to \citep{Sonka07}. Now combining equations \ref{eq_similar_triangles1} and \ref{eq_similar_triangles2} and eliminating x gives

\begin{equation}
z(x_R - x_L) = bf
\label{eq:depth_solution1}
\end{equation}

and taking equation \ref{eq:disparity_shift} into account gives

\begin{equation}
z = \frac{bf}{d}
\label{eq:depth_solution2}
\end{equation}

where $f$ is the focal length of the parallel stereo camera pair, $b$ is the baseline of the stereo pair, and $d$ is the measured disparity shift between the stereo images. The result \ref{eq:depth_solution2} suggests that when disparity is zero, the depth z will be at infinity. According to this model, disparity cannot increase when distance to the measured point P increases. This is a simplification, since for example when a converging camera setup is used, the disparity property behaves so that the disparity is zero at the optical axes convergence point, and as the distance from the convergence point increases, the disparity increases.

%when a corresponding point shift information is known for a feature in a scene. 

%Reconstruction by triangulation is the process of generating a depth map from two camera images e.g. parallel stereo cameras. If both intrinsic and extrinsic parameters of the camera are known, the reconstruction problem can be solved unambiguosly producing absolute coordinates in the world coordinate frame. As a simplification, two other cases would be that only intrinsic parameters are known, a

%The results of triangulation mainly depend on the solution of the correspondence problem. 


%In order to better understand the triangulation technique the simple stereo camera geometry will be introduced next.




%+ lidar cannot measure the depth of highly reflective materials, or very low-reflectance materials such as glass or paint-black steel.
%+ Mechanical coordinate measuring machine (CMM) (3D modeling)
%    + DCC CMM

%+ Structured Light Triangulation
%    +laser plane range finder

\section{Epipolar Geometry}
\label{section:epipolar_geometry}
%+ reference to the optical centers C and C', and baseline of the camera setup
%An interested reader can find the full formulation of the fundamental matrix in Sonka et al. book \cite{Sonka07}. The fundamental matrix F is according to the bilinear relation introduced by Longuet and Higgins
%Butterworth 97 also 
%The solution is minimised with least squares (Frobenius norm) effort. 
%+ at least 8 points for a linear solution in 2 image system \cite{Sonka07}
%+ at least 7 points for a linear solution in 3 image system (trilinear tensors used)

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{epipolar_geometry.pdf}
    \caption{Epipolar geometry (central projection). The image planes are non-parallel so that the epipoles $e_1$ and $e_2$ stay inside the projective image area. In a parallel configuration, the epipoles are ideally located at infinity. Adapted from \citep{Corke11}.}
    \label{fig:epipolar_geometry}
  \end{center}
\end{figure}

Epipolar geometry is a two-view camera geometry where the corresponding image point locations are restricted by an epipolar plane. In figure \ref{fig:epipolar_geometry} the epipolar plane is the plane spanned by both camera focal points (camera frame origins $O_{c_{1}}$ and $O_{c_{2}}$) and an arbitrary world point P. The epipolar plane $PO_{c_{1}}O_{c_{2}}$ intersects the image planes $\pi_L$ and $\pi_R$ at epipolar lines (see red lines in figure \ref{fig:epipolar_geometry}). A single point P is projected as points $p1$ and $p_2$ in two different camera frames. These projected points are called conjugate points that are found for all points P visible in both cameras. The most important feature of the epipolar geometry is that the conjugates of a point P in the camera frame must lie on corresponding epipolar lines, a fact which reduces the correspondence search space from 2d to 1d in block matching processing (see the correspondence problem in section \ref{section:stereo_correspondence_problem}).

In figure \ref{fig:epipolar_geometry} the epipoles $e_1$ and $e_2$ are located along the epipolar line, marked in red dots. An epipole is always found on the stereo camera pair baseline (see Figure \ref{fig:simple_stereo_geometry}), thus, by definition the epipolar line and the camera baseline intersect at an epipole. In a parallel stereo camera configuration the epipoles lie ideally at infinity, because the image plane is parallel to the stereo baseline, and the lines never intersect. In reality, the epipoles may be found at finite distance outside the image projection plane, but for simplicity of describing the concept of the epipolar geometry the figure \ref{fig:epipolar_geometry} shows a a verged (toed-in) stereo setup where the camera baseline and the epipolar lines in the image planes intersect inside the imaging area. If the epipole is formed inside the image plane, then the other camera optical center, thus the camera, must be visible in the picture.

A stereo camera setup where the camera optical axes are in parallel is called a \emph{canonical configuration}. In the canonical configuration the epipoles move to infinity, and the effect known as disparity can be seen on the two resulting images. Disparity means the shifting property of image features taken with a canonical stereo pair setup. For true disparity effect the camera must only shift along a single coordinate axis of a camera frame. Since the cameras project the world in a perspective projection on the image plane, the disparity effect follows the rules of perspective projection, for example, the object further away from the camera optical center shift less in pixels than objects in the near field of the camera. Any stereo configuration with non-parallel optical axes can be transformed into canonical one by image rectification processing as long as the camera images overlap. 

A precise relationship between a conjugate pair $p_1$ and $p_2$ is expressed using $3 x 3$ fundamental matrix $F$ 

\begin{equation}
\tilde{p_1}^T F \tilde{p_2} = 0
\label{eq:fundamental_matrix}
\end{equation}

where $\tilde{p_1}$ and $\tilde{p_2}$ are the projected (conjugate) points expressed in homogenous coordinates. The line equations for the epipolar lines are solved using the fundamental matrix so that epipolar line $l_2$ is a function of point $\tilde{p1}$ 

\begin{equation}
\tilde{l_2} = F \tilde{p_1}
\label{eq:epipolar_line_1}
\end{equation}

and accordingly 

\begin{equation}
\tilde{l_1} = F^T \tilde{p_2}
\label{eq:epipolar_line_2}
\end{equation}

A lot of different techniques have been discovered by different researchers of how to recover the fundamental matrix $F$. If the camera intrinsic parameters are known, then the camera calibration matrix $K$ may be used to recover $F$. If no calibration is available, then $F$ can be recovered using equation \ref{eq;fundamental_matrix} with a calibration technique, such as the normalised 8-point algorithm, or other methods\citep{Salvi01}. Since $F$ is a $3 x 3$ matrix that is defined only up to a scale, it must have 8 degrees of freedom. Thus, the 8-point algorithm will provide a minimum of 8 linear equations that will solve the coefficients of the matrix. Other methods of estimating the fundamental matrix include the 7-point algorithm and random sample consensus based parameter estimators, such as RANSAC, MSAC, and PROSAC\citep{Carro12}.

\subsection{Image Rectification}
% loss of resolution due resampling

%Image rectification is the process of virtual optical axis parallelisation done by software. Rectification transforms the image planes so that the same features seen in both stereo images are perceived on the same epipolar lines. The epipolar geometry in chapter \ref{subsection:epipolar_geometry} can explain how the rectification is done, but the interesting result is that the epipolar constraint makes a 1-dimensional correspondence search space possible. After a successful image rectification, the corresponding feature points are found on the same vertical lines, and the correspondence search space is reduced to 1-d search space instead of heavier 2-d search space. This property of the epipolar geometry reduces computation time a lot for the correspondence search implementation.

%The epipolar geometry in chapter \ref{subsection:epipolar_geometry} can explain how the rectification is done, but the interesting result is that the epipolar constraint makes a 1-dimensional correspondence search space possible. 

Planar image rectification (or stratification) is the process of virtual optical axis parallelisation for projective planar camera geometries done by software. Rectification transforms the image planes so that the same features seen in both stereo images are perceived on the same epipolar lines (more information on epipolar geometry in section \ref{section:epipolar_geometry}). Since optical axes are perpendicular to the image planes, the $u$ and $v$ axes of the image frames are also being parallelised into a coplanar camera configuration. Besides planar image rectification, also cylindrical and polar rectification can be used for non-planar camera projective geometries.\citep{Pollefeys99}

\begin{figure}%
    \centering
    \parbox{5cm}{\includegraphics[width=\linewidth]{harris_feature_detector_left.jpg}}%
    \qquad
    \begin{minipage}{5cm}%
    \includegraphics[width=\linewidth]{harris_feature_detector_right.jpg}
    \end{minipage}%
    \caption{A rectified image pair where the epipolar lines are horizontal. Red dots represent the Harris corner detector output of 200 feature points.}
    \label{fig:rectified_image_pair}
\end{figure}

Rectification of an image pair requires an estimated fundamental matrix $F$ and a set of corresponding feature point coordinates found from both images. Using $F$ and the corresponding points the epipolar line equations will be computed according to equations \ref{eq:epipolar_line_1} and \ref{eq:epipolar_line_2}. The original non-rectified image is warped to an epipolar-aligned destination image where the pixels that do not receive a mapped value are interpolated using a bilinear interpolation \citep{OpenCVRemap}. The effects of the image synthesis in the destination image depend on the used interpolation method, and more information on different interpolation methods can be found in Grevera's study on interpolation techniques used in medical imaging \citep{Grevera98}.

% interpolation methods available:
% nearest-neighbour interpolation, a bilinear interpolation (default), inter-area interpolation, a bicubic interpolation over a 4x4 kernel, a Lanczos interpolation over 8x8 kernel

A stereo camera pair can never be fully parallel due to limits in mechanical alignment of the lens systems and the construction of the stereo pair. Software rectification compensates for such errors, thus, a very accurate positioning of the cameras is often not necessary. If one of the horizontal stereo pair cameras shifts vertically, then some vertical disparity is introduced in the image features, which usually has only minor effect on the image rectification. If one of the horizontal stereo pair cameras tilt around the camera frame X-axis so that principal rays' nearest interdistance becomes large, the rectification corrects the tilt but the usable image area (ROI) in the rectified image will become smaller. Also, the more the keystone correction is needed, the more interpolation will affect the quality of the output rectified image. Furthermore, if the load object features in the rectified image are located on the border of the image, the lens distortions will have a larger effect on the stereo matching. In general, the performance of rectification processing is better with as parallel stereo setup as possible.  

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{bad_calibration.jpg}
    \caption{An unsuccessful camera calibration result generates large deformations in the rectified image.}
    \label{fig:bad_calibration}
  \end{cente}
\end{figure}

A successful image rectification can be seen in figure \ref{fig:rectified_image_pair} where the corresponding feature points are found on the same horizontal lines. The correspondence search space is reduced to a single horizontal line in both images. One indicator of the goodness of the end result is the region of interest (ROI) values that are output by the rectification process. The larger the ROI is in the output, the more there is image detail left, and the image usable area loss (black margins) is minimised. It is easy to see from the output of the rectification process whether the camera calibration has been successful or not. For comparison, a rectification result after an unsuccessful stereo calibration is shown in figure \ref{fig:bad_calibration}.

\section{Stereo Calibration}
\label{section:stereo_calibration}

A stereo calibration computes an extrinsic calibration that relates two camera viewpoints of a stereo camera pair to each other. The calibration output is a rotation matrix $R$ and a translation matrix $T$ according to equation \ref{eq:homogenous_transformation}, and the combined transformation $T$ can be seen in figure \ref{fig:epipolar_geometry}. Stereo calibration techniques are based on fundamental matrix estimation techniques that use epipolar geometry and constraints (see section \ref{section:epipolar_geometry}). 

If the stereo camera pair is used to produce metric data output, then both single camera intrinsic calibration parameters are required as an input to the stereo calibration. The same algorithms that are used to estimate the fundamental matrix in section \ref{section:epipolar_geometry} can be used to solve for the stereo calibration including the 7-point algorithm, the 8-point algorithm, and random sample consensus based parameter estimators methods. The stereo calibration used in the prototype software that was implemented in this thesis work uses the OpenCV stereo calibration workflow, which will be introduced next.

\subsection{OpenCV Stereo Calibration Workflow}
\label{subsection:opencv_stereo_calibration_workflow}

The OpenCV stereo calibration uses the same implementation used by the MATLAB Camera Calibration Toolbox by Bouquet. Bouquet's implementation mixes multiple techniques from other camera calibration original research including papers from Zhang, Heikkilä, Silven, and Tsai \ref{BouquetRef}. Basically, the technique used estimates the fundamental and essential matrices between two stereo images who contain calibration chessboard patterns. The calibration is based on the knowledge of chessboard crossing locations in high accuracy.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{stereo_calibration.jpg}
    \caption{A $9 x 6$ crossings standard chessboard laser printout is being detected simultaneously in both camera images by the OpenCV calibration function.}
    \label{fig:stereo_calibration}
  \end{center}
\end{figure}

\subsection{Stereo Assumptions}
\label{subsection:stereo_assumptions}

Some basic assumptions about the measured crane working area must be made in order to use stereo correspondence algorithms. These assumptions operate on the collected images from the two stereo cameras. 

In general, the assumptions hold for any stereo camera setup that work on disparity principle unless otherwise stated. 

A single pixel must correspond to a single surface point in the world in order to construct a linear mapping function from the left stereo camera image to the right camera image. This restriction means that no opaque, or see-through, materials can be accurately measured with the used technology. Additionally, occluded and self-occluded surface pixels will be discarded in order to comply with the single pixel correspondence assumption.

+ A single pixel corresponds to a single surface point (restriction: no opaque materials | fish + fish bowl)
+ Disparity values are generally continuos (smooth within a local neighbourhood | discontinuities occur only at object boundaries)
+ Ordering constraint (if an object A is to the left of the object B in the left image, then the object A will be left of the object B also in the right image | sometimes violated by pole-like objects)
+ Lambertian surfaces => surfaces do not change appearance when viewed from another angle.
    + opaque
    + ideal diffusion
    + reclefts light energy in all directions

Stereo vision produces a dense disparity map . (If we compare to laser scanner sensor, the denseness of the produced point cloud is much higher)
Some requirements for the disparity map are elaborated in Zitnick et al. report, namely the disparity map should be smooth and detailed . The desirable result would provide smooth continuos mapping that detects small surface elements as separable regions. It turns out that these requirements are opposing to each other: a smooth continuos disparit map tends to filter out small details, and a detail preserving mapping is affected by noise \cite{Zitnick00}.



\subsubsection{Calibration Targets}
\label{subsubsection:calibration_targets}

Planar calibration grids are the most common calibration method for a stereo camera setup. Mainly checkerboard patterns and rectangular grid patterns are used for calibration purposes.

\subsubsection{Calibration Errors}
\label{subsubsection:calibration_errors}
%In most cameras the scenes are projected on a planar image plane. For perspective projection cameras the borders of the image are less illuminated due to light refraction in the lens due the law of cosines. 
%lähde Kuvatekniikan perusteet


+ Focal length estimation errors
It was found out in study by Kytö that the focal length affects the depth threshold of a stereo setup more dominantly than the baseline separation of the cameras. %what is really depth threshold 

+ Tangential distortion
+ Radial distortion (barrel effect)
It is important to compensate for lens system radial distortions, because they cause vertical disparity on the edges of the image. \ref{Kyto14}

+ Chromatic aberration

+ Affine distortion (aspect ratio)

+ Camera electronics, light intensity changes affect the phase locked loop (PPL) in the electronics => line jitter with sync signal systematic or random noise change

+ Calibration target location

\section{Stereo Errors And Accuracy}
\label{section:stereo_errors_and_accuracy}

A stereo 3d reconstruction system accuracy is limited, which means that any geometry output will include additional noise and errors in the discrete point entries. If only uncertainty in the measured disparity is regarded, an uncertainty model can be formulated using partial derivative of $d$. Starting with equation \ref{eq:depth_solution2} a partial derivative of $z$ over $d$ gives

\begin{equation}
\frac{\partial z}{\partial d} = -\frac{fb}{d^2}
\label{eq:disparity_uncertainty}
\end{equation}.

A depth uncertainty $\Delta{z}$ can be approximated as

\begin{equation}
\Delta{z} \simeq \frac{\partial z}{\partial d}\Delta{d}
\label{eq:uncertainty_approximation}
\end{equation}

and combining equations \ref{eq:disparity_uncertainty} and \ref{eq:uncertainty_approximation} yields

\begin{equation}
\Delta{z} \simeq -\frac{fb \Delta{d}}{d^2}
\label{eq:triangulation_uncertainty}
\end{equation}

where $\Delta{d}$ is the resolution of the disparity difference detection in the block matching, down to e.g. 0.1 pixels. It can be seen from equation \ref{eq:triangulation_uncertainty} that the accuracy of the z measurement is linearly proportional to the disparity measurement error. Additionally, trade-offs exist between most other factors. On one hand, an increase in the baseline distance between the cameras will increase the measurement error in equation \ref{eq:triangulation_uncertainty}. On the other hand, the disparity will increase, too, and its term in the denominator is quadratic, which will ultimately decrease the error. In standard stereo processing the object measurement error increases with items that are located far away. If a single baseline and resolution is used the error grows quadratically with increasing depth \cite{Gallup08}. Increasing the baseline width will in general decrease the error as a function of distance although the change will affect the size of the overlapping image area, and the disparity space. With a normal parallel stereo setup it is not possible to 


A stereo setup that has a baseline much higher than the human vision system baseline width is called a hyperstereo setup.

\subsection{Hyperstereo}
\label{subsection:hyperstereo}

A hyperstereo setup makes feature matching much more difficult since the changes between the images are not incremental, but larger changes. A hyperstereo will distort motion of objects, which makes matching of a moving human or a swaying tree very difficult. Moreover, perception of glossiness of materials is distorted, which is why normal image formation according to X is not supported in a hyperstereo setup.
%=> also crosstalk = image leaks to the area of the other eye => visible ghosting in stereo perception





If the cameras are taken far apart, the near-field will lose image coverage, thus, nearby objects can be reconstructed only using converging camera setup. If a converging camera setup is used and the correspondence search space is not adjusted to support negative disparities, the nearby objects will still not be found due disparity space search being limited to only positive values. The effects of the aperture size and the focal length were discussed in more detail in section \ref{subsection:lens_optics}, and the effects on optical axes convergence is discussed in more detail in section \ref{subsection:converging_camera_setup}. 

The baseline width can be chosen arbitrarily as long as the two images include enough overlapping image area so that the correspondence problem can be solved. The stereo solution resolution depends on the overlapping image area, which is why a large baseline is not usually favorable. As the baseline increases, the overlapping image area becomes smaller, thus, the rectified image becomes smaller, thus, the resolution of the stereo solution becomes smaller. Although the accuracy might increase with a large baseline, the effective ROI in the rectified image becomes so small that it is not useful in applications that use the full field of view of the camera. For far-field distance measurement applications a large baseline is obviously favourable.

If the rectified images are supposed to be part of a visualisation that people look at, then the baseline width should be selected according to the human eyes baseline width. Stereographic images viewed from a hyperstereo setup will not fuse nicely using a smaller baseline width system, such as the eyes of a human.

A comprehensive study of the effects of subpixel interpolation, vertical camera misalignment, matching kernel size, focus, maximum disparity, and stereo baseline width on the stereo correspondence can be found in a Jet Propulsion Laboratory article from 2005 \citep{Kim05}. The JPL paper is very applicable to the case in this thesis since the camera resolution is similar with a 1024 x 768 resolution compared to 768 x 576.

%A simple equation for the depth error is introduced in \cite{Gallup08}:
%\begin{equation}
%\epsilon_z = \frac{z^2}{bf}\dot \epsilon_d
%\end{equation}
%where \emph{z} is the depth value, \emph{b} is the baseline value, \emph{f} is the focal length, and \epsilon_d is %the disparity matching error (in pixels). The resulting depth error $\epsilon_z$ 

\subsection{Occlusion}
\label{subsection:occlusion}

Occlusion is another error source in the reconstructed geometry. Surfaces that lie inside the camera 3d frustum, but are not visible in the projected 2d image, are called occluded objects. Occlusion 

%The stereo cameras will reconstruct all surfaces whose image is projected onto the 2d image plane, but surfaces that are not visible in the image, for example: behind other surfaces are not recorded: these are called occluded objects or surfaces. In practice, all visible objects will cast shadows behind them, and only a partial 

\begin{figure}[ht]

  \begin{center}
    \includegraphics[width=\textwidth]{occlusion_pollefeys.pdf}
    \caption{An occlusion front is seen in scenes with depth differences and planar features. Image from \citep{Pollefeys04}.}
    \label{fig:occlusion_pollefeys}
  \end{center}
\end{figure}

Occlusion means partial or full loss of line of sight of an object in an image. In stereo matching, an object may be partially occluded by another object, which means that a surface that is visible in the left camera image is not visible in the right camera image. The correspondence search should not return a corresponding point for an occluded surface, but in real machine vision systems occlusion tends to generate erroneous correspondence matches. An object may be occluded in only left or right stereo image, or partially occluded in both. In both cases, the other image has visible object surface that cannot be matched in the other image.

In figure \ref{fig:occlusion_pollefeys} an example of an occluded part of wall is shown on the left side. The right side depicts a correspondence search space along epipolar lines with an ideal correspondence correlation path for the rectified occluded scene. It can easily be seen from figure \ref{fig:occlusion_pollefeys} that when on occlusion is found in the scene, the correct corresponding image point is found further away from the optimal path. Furthermore, the correspondence search space is restricted along the epipolar lines with certain disparity difference limit (bandwidth) because the depth range of the imaged features is limited\citep{Pollefeys04}. 

Most solutions do not explicitly detect occluded regions, thus errors may be found in the disparity output of block matching if bidirectional matching is not enforced. Bidirectional matching searches corresponding surface points from both images, thus, occluded regions may be identified if there is certain surface areas are detected as missing. Also, an occluding boundary is found on the perimeter of objects, for example, a cylinder surface will curve until the other camera view becomes occluded. Thus, on the edge of objects that curve out of view there is always some occlusion in the scene. Occlusion induces errors in disparity map estimation, which can be improved if occlusion detection systems are used in the block matching process. Occlusion detection is out of scope of this thesis, but basically the occlusion can be detected using energy functions that optimise the correspondence search path, for example Kolmogorov et al. describe such a technique that uses graph cuts\citep{Kolmogorov01, Woo-Seok11}. 

\subsection{Converging Cameras}
\label{subsection:converging_cameras}

The convergence distance of the stereo system optical axes affects the perceived location of the disparity range. The convergence distance can be computer controlled \ref{Chen et al. 2010 from Kyto14}, but in this work the convergence distance is not approximated accurately since the collected stereo video is not meant to be recorded for human vision system viewing. Increasing the convergence depth will also increase possibility for frame violations for negative disparity values near the edges of the screen. A frame violation would break the 3d perception illusion for a viewer, but for a computer system a violation will result in only an erroneous depth match that is eliminated from the final disparity image.
%what is negative disparity? => in front of the screen => does this matter in a measurement system

+ toed-in or parallel camera setup

The stereo cameras can be physically rotated towards each other, or they can be fixed in parallel configuration so that the optical axes intersect at infinity.

+ convergence distance of the stereo system optical axes: approx. 264cm

\section{Stereo Correspondence Problem}
\label{section:stereo_correspondence_problem}

Stereo imaging is divided into two different categories by the used algorithm. The first category is dense matching algorithms where stereo matching algorithms run on all pixels. The second category is sparse matching algorithms where only distinctive features will be matched\cite{Terho10}.

The correspondence problem is a general multiple camera system problem regarding whether multiple images contain a same 2d feature or not. A feature is an understanding of some feature that has a visible shape response in its 2d projection on the image according to natural image formation in a camera. A simplification of the shape may be a result of image segmentation using histogram or color based thresholding, k-means clustering, a water-filling algorithm, region growing algorithm or some other segmentation method.

On one hand, in figures \ref{fig:harris_features_left} and \ref{fig:harris_features_right} it is easily seen that the same lifted box is seen from different viewpoints, but on the other hand, the machine vision system does not understand this unless some feature descriptor is formulated that returns a similar response for both images of the lifted box.

A feature descriptor measures the shape of a segmented image region using invariant moments of the image, or other constant shape descriptors. A good feature descriptor response is invariant to changes in scale, orientation, and viewpoint, and it should tolerate changes in the camera system used for imaging, such as white balance, exposure, and illumination changes (specular reflections, shadows etc.).\citep{Corke11}

Image feature detection is a large field on its own, which is why only a few general feature descriptos are introduced that can be used to track features and solve the correspondence problem. Some full-fledged feature descriptor techniques include scale-invariant feature transform (SIFT), speeded up robust features detector (SURF), and histogram of oriented gradients (HOG). Also blob detectors, such as the maximally stable extremal regions (MSER) could be used to solve a general correspondence problem \ref{Karlson09}. The Harris corner detector (see Figure \ref{fig:harris_features_left}), and Shi and Tomasi corner detector are useful for tracking the corners of a lifted load objects as part of the measurement software. Moreover, the features returned by a corner detector could be used as control points in the point cloud verification process introduced in section \ref{section:data_quality_in_the_machine_vision_process}.

In the implementation of the prototype software the correspondence problem is solved using the epipolar geometry property that enables a 1-dimensional correspondence search space using image rectification. If the geometry of the stereo setup is unambiguosly known, the extrinsic calibration knowledge makes image rectification possible. 

(UNEDITED)It is important to choose a proper window size for stereo matching. In low contrast regions of the image too small a window cannot guarantee a unique match because of too little intensity variation. 
In general, a small window is desirable to avoid unnecessary smoothing, but optimal window size depends on the region intensity variation, texture, and disparity \cite{Zitnick00}.
The problems of window size can be solved with iterative window size methods, but increase in computation overhead or problems at occlusion boundary are still left unsolved \cite{Zitnick00}..

% correspondence problem
+ the ambiguity of the correspondence problem can be reduced using one or multiple constraints such as
disparity limit constraint (disparity must be < limit, e.g. finger in front of the face cannot be imagined in stereo)
disparity smoothness (disparity changes only a little in any direction)
epipolar constraint (search space for matching pixels 1D on the epipolar line)
figural disparity constraint (corresponding elements should lie on an edge element in both images)
feature compatibility (physical origin of the matched points should be the same)
geometric similarity constraint (geometric features differ only a little)
mutual correspondence constraint (occluded points are not found thus ruled out)
ordering constraints (typically points lie on the same epipolar line for similar depth items, exception narrow close-up sticks)
photometric compatibility (little intensity differences)
uniqueness constraint (1 pixel can only correspond to 1 pixel in the 2nd image, exception two or more points on 1 ray in 1 image)

+ specularity edges cannot be used for feature matching because specular lighting changes depending on the viewpoint \cite{Sonka07}

\subsection{Block Matching}
\label{subsection:block_matching}

\section{Depth From Disparity Map}
\label{section:depth_from_disparity_map}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{disparity.jpg}
    \caption{A disparity image. Near field objects are bright, and far field objects are darker. The image has only one intensity channel with 256 different brightness levels.}
    \label{fig:disparity}
  \end{center}
\end{figure}

After the image rectification is done for a stereo image pair captured by the calibrated stereo camera setup, then the 3d point cloud generation becomes a problem of extracting the estimate of the disparity map (d(x,y).

Q is the reprojection matrix that encodes information about the stereo camera setup. Q matrix will provide information about camera optical axis convergence (parallel or cross-eyed), image plane principal points, and the stereo setup baseline width b.

\chapter{Implementation}
\label{chapter:implementation}
%test overhead crane dimensions: width 40 meters, length 10.7 meters, height 5.5 meters

The software implementation for the thesis work was prepared with the FAMOUS project team in Generic Intelligent Machines (GIM) research group. The implemented software includes many software nodes, mainly the stand-alone load object measurement tool that processes point cloud data into load object bounding volume information. Additionally, the point cloud data publisher node was implemented that integrates the HIMMELI camera platform hardware as the sensor input platform for the load object measurement tool.

The programming work done for the thesis was carried out from March 2013 to February 2014. In addition to standard C and C++ libraries, external libraries specialized in image processing and point cloud processing were used. OpenCV library by WillowGarage Inc. was used for image processing and stereo rectification purposes, Point Cloud Library (PCL) was used for all data intensive point cloud processing, and Boost library was used for all file system operations. All the data networking, runtime configuration, and other online networking solutions were implemented using the Robot Operating System (ROS).

The design of the software was finished in December 2013 after successful compatibility testing between library functionalities and proofing of concepts with offline data sets. The final version of the load object measurement system that supports online perception sensors in the ROS network was finished in May 2014.

During the implementation period, new versions of most of the libraries became available, but the implementation uses latest currently stable versions: ROS Groovy and PCL 1.6 (compatible with ROS Groovy). If the software would be upgraded, then a newer version of ROS (Hydro) can only be used with the next version of PCL (1.7). PCL 1.7 does simplify some things, for example PCL visualization camera controls, and brings some extended features that could benefit the load object measurement system (for example, the new moment of inertia and eccentricity based descriptors functionality). Other than that, the old versions have all the functionality that the system needs, and they are successfully used to run the software at the time of writing this.

\section{Overall System Design}
\label{section:overall_system_design}

\subsection{Software Architecture}
\label{subsection:software_architecture}

The software component design in this thesis was planned modular so that different parts of the measurement, e.g. stereo rectification and object segmentation, can be developed independently. The full system design will consist of many sequential components, some of which will be designed and implemented by other researchers in the FAMOUS research project.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{software_architecture.pdf}
    \caption{Software architecture for the load object measurement workflow using robot operating system ROS.}
    \label{fig:software_architecture}
  \end{center}
\end{figure}

\subsection{Use Cases}
\label{subsection:use_cases}

Performance of the implemented software is tested with four different use cases. The first use case has an overhead crane setup with a top-down camera viewpoint. The stereo camera rig is attached to the crane trolley with cameras facing down so that the crane trolley can move freely with cameras attached, and the camera image will show the crane load approximately in the center of the image at all times. Hoisting the load up will enlarge and slightly decenter the image of the load object.

The second use case has an overhead crane setup with a bird's eye viewpoint. The stereo camera rig is attached to the traveling beam below the driving rails. The cameras are tilted vertically so that they are facing the load object with some parts of the opposite wall, and most of the floor visible. Hoisting the load up will show the load moving towards the upper part of the image. Hoisting movement does not enlarge the image of the load object, but driving the crane bridge trolley near the cameras will enlarge it.

The third use case has a small telescopic crane installed on a forestry machine with frog perspective camera viewpoint. The cameras are attached on the side of the forestry machine chassis facing the crane working area. The telescopic crane boom may be visible in the camera image. The camera FOV covers most of the working area, but the most extreme parts of the working area are not inside the image frustum. It is assumed that all the load objects include only cylindrical wooden logs whose size must be measured.

The fourth use case has a small telescopic crane installed on a forestry machine with bird's eye perspective camera viewpoint. The cameras are attached on top of the operator cabin so that the crane boom is visible in the image on the right or the left side. The cameras are facing down towards the crane working area so that the FOV covers all of the working area. The crane load is mostly visible, but occluded by the telescopic boom on one side of the working area. Once again, it is assumed that the load objects are only cylindrical logs. 

\subsection{Prototype Software}
\label{subsection:prototype_software}

A full recovery of a 3d object model is not needed to solve the problem of load positioning and dimensional measurement.

\subsubsection{End Effector Tracker Integration}
\label{subsubsection:end_effector_tracker_integration}

%Machine learning techniques can be used in tracking objects from 2d images. A common machine learning technique for facial detection in images is Haar training that can be utilised for other purposes, too, for example end effector detection. Haar training was originally introduced by Viola et al., and improved by other researchers with additional ... features. Haar training uses boosting algorithms, which are adaptive parameter learning algorithms. A basic cascade classifier cannot detect scaling of images, but a cascade of classifiers can be constructed that detects features scale-invariably. A cascade of classifiers is a decision tree structure which is constructed out of simple classifiers called stage classifiers. The cascade of classifiers results in a positive detection of a feature if the test image can pass all the stages of the cascade. Accordingly, we may estimate the probability for a false alarm rate given the probability of a single weak classifier detection and the number of stages \ref{Lienhart03}.

Object recognition works on a classification principle: a classifier detects attributes in an object and works out into which object class it belongs to.
+ machine learning
    + neural nets
    + genetic algorithms
    + fuzzy systems

%machine learning
A classifier is a concept of machine learning that is utilized in Haar training. A single classifier itself is a weak member of a more powerful committee of cascaded classifiers \cite{Freund96}. 
A number of simple, computationally light classifiers seem to outperform strong classifiers such as neural networks in principle \cite{Lienhart03}. 

\section{Perception Sensor Platform \emph{Himmeli}}
\label{section:perception_sensor_platform_himmeli}
%HIMMELI sensor platform provides means for creating digital models of the environment with stereo cameras and a LIDAR laser scanner. The environment modeling accuracy depends on the distance of the target area to the stereo cameras, and consequently the area of the modeled environment depends on the distance to the cameras. The laser scanner can model larger areas with higher accuracy.
%Histogram equalization maps the current image intensity response over the full available intensity range. 

Himmeli sensor platform is a modular sensor rig that was engineered in Aalto university department of automation and systems technology in a research project in 2012. The sensor platform was created for use in future automated machinery research and its purpose is to provide flexible sensing of environment with multiple sensors. Himmeli sensor platform was used as the perception platform for the implementation work done in this thesis. It can collect data from the crane environments using stereo cameras or laser scanner, and it can provide orientation change data using an inertial measurement unit \ref{Karhu13}.
%todo reference from Sami

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{himmeli_modules.jpg}
    \caption{Himmeli platform includes a sensor module, a computation module, and a support module with IR light source.}
    \label{fig:himmeli_modules}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{himmeli.pdf}
    \caption{Himmeli platform perception sensor schematic\ref{Terho10}.}
    \label{fig:himmeli_schematic}
  \end{center}
\end{figure}

The Himmeli platform includes three modules who are namely the sensor module, a computation module, and a support module with an IR light source seen in Figure \ref{fig:himmeli_modules}. On the front side of Himmeli, the sensor module includes a high resolution stereo camera pair, a single high resolution camera, a thermal imaging camera, an automotive ultrasound RADAR, and an inertial measurement unit (IMU). There is an optional Velodyne HDL-32E LIDAR sensor on top of the sensor module. The Velodyne HDL-32 can be used to scan the environment geometry with 32 micromirror controlled lasers. The schematic of all the sensors can be seen in Figure \ref{fig:himmeli_schematic}.

The automotive radar installed in Himmeli was not used to model the environment because of its low accuracy of 5 cm to 1 m depending on band used\ref{Ahtiainen12}. In general, radars are not often used in machine perception systems due to their low resolution output. They can operate in adverse weather, such as fog, so for some outdoor applications the radar might be useful. For example, if the load object measurement system was used to measure geometry of container cranes in a seaport terminal, the radar could be used for partial modeling of the load object under harsh weather circumstances.

\subsection{Stereo Camera Technical Details}
\label{subsection:stereo_camera_technical_details}

The Himmeli sensor module includes two high resolution GigE uEye UI-5120 RE HDR cameras with industrial grade casings. The uEye UI-5120 RE HDR camera is a high dynamic range camera with a CMOS (complementary metal oxide semiconductor) sensor array with a PAL resolution of 768 x 576 pixels seen in Figure \ref{fig:gige-camera}. The model in use (UI-5120RE-M-GL Rev.2) has a monochrome sensor that transmits intensity images at a maximum of 50 frames per second using Gigabit Ethernet data connection. 

If the 175 gram GigE uEye cameras are installed in a vibrating environment (e.g. a rotary crane boom) the camera's image output quality may suffer, because the optics do not include image stabilisers. Using the stereo pair installed in the Himmeli sensor module may improve the image quality in some environments due to passive damping properties of a larger mass of a sensor module body.

The cameras are installed with lens tube shieldings (class IP65/67) so they can operate even in outdoor winter conditions. The rest of the components of the camera comply to the splashproof and dustproof IP65/65 class so the camera and its casing are well-suited for industrial imaging purposes.

The camera CMOS mono sensor is manufactured by NIT and has a pixel size of 10 $\mu$m and an optical size of 5.75 mm x 7.68 mm. It has a colour depth of 12 bits, and the dynamic range of the camera is logarithmic with a 120dB sensitivity\ref{IDSImagingWeb13}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=5cm]{camera-gige-ueye-se-cmos-1.jpg}
    \caption{The GigE uEye HDR camera with no protective casing. Image from \cite{IDSImagingWeb13}.}
    \label{fig:gige-camera}
  \end{center}
\end{figure}

\subsection{Installation}
\label{subsection:installation}

The Himmeli platform was installed in two different test environments as described in the use cases.

\subsubsection{Overhead Crane Installation}
\label{subsubsection:overhead_crane_installation}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{himmeli_installation_12.jpg}
   \caption{The Himmeli sensor module installation in the overhead crane trolley in a test session.}
    \label{fig:himmeli_installation_option_1}
 \end{center}
\end{figure}

In the overhead crane environment the Himmeli perception platform was installed modularly so that the sensor module was installed in the overhead crane, and the computation module was used at the operator balcony of the process hall. Since most overhead cranes have little available space to install anything, only the sensor module was installed in the overhead test crane. Two installation options were available for the sensor module: first option was to install the sensor module to a moving trolley, and the second option was to install the sensor module to the side of the bridge. 

In the first installation option the sensor module was attached directly to the moving crane trolley housing so that the camera units were facing down towards the crane working area in a top-down manner with a slight angle to the warehouse floor as seen in figure \ref{fig:himmeli_installation_option_1}. 
%In option 1 the laser scanner rotation axis points towards the floor, aligning the laser scan lines along the shorter length of the warehouse space (Y axis of the warehouse coordinate system).

In the second installation option the sensor module was attached to the side of the moving bridge using a custom made metal frame. In this installation option the camera units were facing towards the crane working area from side and above in a bird's eye view manner. 
%The laser scanner rotation axis points towards the opposite end of the moving bridge, aligning the laser scan lines along the longer length of the warehouse space (X axis of the warehouse coordinate system).

\subsubsection{Forestry Crane Installation}
\label{subsubsection:forestry_crane_installation}

In the forestry crane environment the installation of the Himmeli platform is even more constrained by spatial limitations. In an actual log picker truck the installation location options for the camera module would include the operator booth rooftop and the base of the rotary crane boom. In testing situation a ground installed rotary crane boom was used and the Himmeli platform was installed first near the base plate of the rotary crane boom, and then on the rooftop of the operator booth.

In the first installation option the cameras are facing the working area so that the crane boom is slightly visible when it reaches for logs from above. In the second installation option where the cameras are located on the rooftop of the operator booth the crane boom is fully visible. This is a somewhat occluded viewpoint since the crane boom can be blocking the view of the load object.

The mass of a GigE uEye camera is 0.175 kg, which means the camera is susceptible to vibrating environments, and the image quality may be affected if no active or passive damping is used in a vibrating crane environment. While overhead crane environments dampen the vibrations passively, the smaller forestry crane rotary booms do not have mass that would damp vibrations as well, which is why the selection of the install location is important. According to Pedro et al. it is not possible to directly install electronic sensors (e.g. encoders, resistive elements) on a log picker truck rotary actuators because they break due roughness of the surrounding environment\ref{Pedro09}. Similarly, a camera would break, or produce low quality images in a vibrating environment if attached to locations that are in direct mechanimal contact with the rotary actuators. Since the log picker truck operator's booth is isolated from the vibrating environment, and may even have active vibration damping, it could be beneficial to install any camera systems on the rooftop of the driver's booth in an actual log picker truck.

\subsection{Computation Module}
\label{subsection:computation_module}

The Himmeli platform computation module includes different components that control the power and the data flow of the perception platform. The integrated computers run on 230 volts AC power with an option for 24 volts DC operation. The PC that controls the camera software runs on Windows 7 operating system. There is also a Linux distribution PC that runs the sensor fusion software for the captured images. The computing module can be remotely operated via a wireless local area network. We do not consider the details of the Himmeli platform in more detail, except some issues that were encountered will be reported in the evaluation chapter \ref{chapter:evaluation}.

%\begin{itemize}
% item separation length in a list
%\setlength{\itemsep}{0pt}
%\item HDR stereo camera pair
%\item High resolution camera
%\item 3D LIDAR laser scanner \url{http://velodynelidar.com/lidar/hdlproducts/hdl32e.aspx}
%\item Infrared (IR) camera
%\item Automotive radar
%\end{itemize}
%\begin{table}
%\caption{Device and its connector type in HIMMELI sensor rig.}
%\label{table:connectortypes} %lable just after caption
%\begin{tabular}{|p{6cm}|p{5cm}|}
%\hline % The line on top of the table
%\textbf{Device} & \textbf{Connector Type} \\
%\hline
%High dynamic range camera & Gigabit Ethernet \\
%\hline
%High resolution camera & IEEE 1394b (FireWire) \\
%\hline
%Automotive radar & CAN bus \\
%\hline
%Thermal camera & CVBS (PAL) or serial bus \\
%\hline
%LIDAR laser scanner & LIDAR \\
%\hline
%GNSS receiver & Serial bus 1 \\ \hline
%\end{tabular} % for really simple tables, you can just use tabular
%\end{table} % table makes a floating object with a title

%\begin{figure}[ht]
%  \begin{center}
%    \includegraphics[width=\textwidth]{himmeli_modules.jpg}
%   \caption{HIMMELI platform modules: a PC module, a support frame module, and the sensor module.}
%    \label{fig:himmeli_modules}
% \end{center}
%\end{figure}

\section{Point Cloud Library}
\label{section:point_cloud_library}

Point Cloud Library, or shortly PCL, is a fully templated modern C++ library for 3d point cloud processing purposes. 
PCL uses optimizations such as Intel SSE with Eigen library backend, GPU CUDA processing, and parallel programming in order to maximize the computation efficiency on specific computation platforms.

\subsection{Limitations of Point Cloud Computing}
\label{subsection:limitations_of_point_cloud_computing}

Although PCL library is optimized for point cloud computing, the biggest limitation in point cloud processing currently is the slow processing speeds of some operations. The number of data entries per point cloud has a big effect on some operations, such as the K-nearest neighbour search.


One estimate for the processing speed of a single cloud into a bounding volume output can be done by looking into other implementations in other research teams' efforts. As all the computation for PCL processing is done using an Intel Core i5 3.2GHz CPU on a 64-bit system with 



\section{Robot Operating System}
\label{section:robot_operating_system}


%Additionally, different versions of ROS distributions are not cross-compatible in the same ROS network. That means we need to decide upon a single ROS version which we are using. This is also a problem, because it is easiest to run PCL on a ROS Hydro distibution, where the other parts of the software (e.g. the TLD tracker) runs on older ROS Fuerte distribution. The old software component code base may be difficult to upgrade, so quite probably we will use ROS Fuerte and run older versions of PCL on it.   

Robot Operating System, or shortly ROS, is an open source operating system-like framework targeted specifically for robotics application use. It was originally developed by Stanford AI laboratory in 2007 and currently supported by the open source community. The latest release was ROS Hydro release in September 2013. ROS is structured with a hierarchy of nodes, packages, stacks (legacy), and community-supported repositories, who provide the user with a large code base of readily available robotic applications. ROS system can also utilize a heterogenous computation network which is administered using a master-slave network setup.

ROS is mainly used in this thesis for its excellent capability of networked messaging through topics and subscribers. With a little effort we may use readily available data types for point cloud data networking, and interaction between popular operating system platforms, such as Windows and Unix environments. Data can be published via topics if we wish to disseminate it in the ROS network without receiving any feedback. In case we need feedback whether someone uses the data or not, ROS provides with a server-client scheme. 

ROS also includes a multitude of tools that can be used for data traffic verification.

For the networked data approach, ROS was considered as the number one framework choice in terms of cross-platform communications for the load object measurement system. Other options considered were an own implementation of the UDP communication protocol, some other real-time operating system, such as FreeRTOS, or an implementation with no networking capabilities at all. The other options were not as readily available for a full-fledged PC network in an industrial LAN setting as ROS was, thus, ROS was selected for use in the software development effort.

\subsection{ROS Naming Conventions}

The current naming convention is meant to protect from colliding topic, node, and parameter names. 

\subsection{Parameter Server}
\label{subsection:parameter_server}

ROS parameter server is a cross-platform shared dictionary service that enables parameter retrieval at runtime in a ROS network. It is one of the advantages of using ROS since it can be used for reliably configuring a multi-device network. The parameter server is implemented usig XMLRPC libraries in the ROS master node.

\begin{table}\centering
\ra{1.3}
\caption{The data types supported by the ROS parameter server XMLRPC library.}
\begin{tabular}{@{}lll@{}}\toprule
Java type & XML name tag & Description \\
\midrule
Integer & int & 32-bit signed integer, non-null \\
Boolean & boolean & 0 or 1, non-null \\
String & string & A string, non-null\\
Double & double & A 64-bit signed floating point number, non-null \\
java.util.Date & dateTime.iso8601 & A ISO860 timestamp with milliseconds and time zone information missing \\
java.util.List & array & An array of objects \\
java.util.Map & struct & Key-value pairs \\
byte[] & base64 & Base64-encoded byte array \\
\bottomrule
\end{tabular}
\label{table:parameterserver}
\end{table}
%\begin{tabular}{|p{7cm}|p{3cm}|}

Setting parameters in the ROS parameter server is done using an XML launch file when launching a ROS node. All the variables are evaluated before launching any nodes, and all information is uploaded to the parameter server before launching the nodes. We can force certain data types for the parameters if the data types are not unambiguos. Supported types in the launch file are \emph{str}, \emph{int}, \emph{double}, and \emph{bool}. Additionally, we may set parameters in child namespaces or fully upload the contents of a parameter file to the parameter server as text or binary.

\chapter{Computation}
\label{chapter:algorithms}

This chapter introduces computation techniques used to recover the load object measurement from the 3d data provided by the stereo cameras. The data generated according to Chapter \ref{chapter:stereoscopic_machine_vision} is processed using Point Cloud Library and into a 3d bounding volume measurement.

\section{Point Cloud Library Operations}
\label{section:point_cloud_library_operations}

\subsection{Point Cloud Preprocessing}
\label{subsubsection:point_cloud_preprocessing}

PCL provides many filter implementations in its filter module for pre-processing the point cloud data. Since ROS transportation layer required an organised point cloud for the ROS message, the point cloud arrived at the client software including invalid values, namely NaN (not a number) values or InF values in the cells that were not assigned a value in the point cloud generation step. All invalid values were removed using NaN removal filter (pcl::removeNaNFromPointCloud) and an InF value filter. The Inf value filter was implemented in the work by iterating through all data points and removing all points marked as infinity (std::numeric\_limits<float>::infinity()).

After using the NaN removal filter, the data becomes unorganised, which drops the number of points needed to process from 442368 points to a lower number of approximately 250000 valid points. Furthermore, the point cloud is being downsampled using the PCL voxel grid filter (in pcl::VoxelGrid class) so that the number of output points stay in a range of 2-20\% of the original number of data points. The range was found out by testing different values of the voxel grid leaf size parameter. In the implemented software a value of 120 millimeters was selected as the voxel cube length value, which produced a point cloud downsampled down to a 2.2\% size of the original point cloud.

The used value in the work was 120 millimeters, which was found using a slider that changed the value in real time in the software system. The voxel grid leaf size has an effect on the object segmentation process, too, which is why it was compulsory to visually inspect the parameter effect on the output object segmentation. 

A grid of cubic 3d volumes covers all the data, and all points residing in a single voxel cube are replaced by their centroid or the center of the voxel. The current implementation uses the centroid (center of gravity) approach, which is slower than outputting only the center coordinate of the voxel. By using the centroid approach the underlying 3d surface is being represented more accurately. \citep{VoxelGrid14} The voxel grid leaf size is the only parameter in voxel grid filtering that controls the coarseness of the downsampling result besides the selected approximation method.

The voxel grid filtering can be used to compute different lower fidelity levels for the environment representation analogous to the Gaussian image pyramids technique for 2d images. The downsampled cloud that is significantly smaller than the original still contains the geometry data in smoothed detail. The subset of the downsampled point cloud that represents the load object is used to produce the load object measurement.

\subsection{Object Segmentation}
\label{subsection:object_segmentation}
% Euclidean Cluster Extraction
% the used scale of the point cloud point entries does have an effect on the computation time
% a computation processed using meters is much faster than computation on the same point cloud represented in micrometers, a million times larger values

After the preprocessing of the point cloud, the 3d data is being segmented into separate objects using PCL Euclidean cluster extraction. The cluster extractor uses k-means clustering technique to cluster n-dimensional data points into k different spatial clusters. For 3d point cloud data, a Euclidean metric is used in measuring distance between cluster candidates. The extractor is implemented in pcl::EuclideanClusterExtraction class, and the steps of the algorithm in Algorithm \ref{algorithm:cluster_extraction} follow the presentation of Rusu's dissertation. \citep{Rusu09} 

\begin{algorithm}
\KwData{point cloud $P$}
\KwResult{object clusters $C$ extracted from $P$}
1. create a k-D -tree representation of input point cloud $P$\;
2. set up empty list of clusters $C$ and empty queue $Q$ for points to be checked\;
\While{not all points $p_i \in P$ have been processed and are part of list of point clusters $C$}{
    \For{every point $p_i \in P$}{
    add $p_i$ to the current queue $Q$\;
        \For{every point $p_i \in Q$}{
            search for the set $P^i_k$ of point neighbours of $p_i$ in a sphere with radius r $<$ d\;
            \For{every neighbour $p^i_k \in P^k_i$}{
                \eIf{point has been processed}{
                do nothing\;
                }{Add point $p^i_k$ to $Q$}
            }
        }
    }
}



\caption{Nearest neighbour algorithm that utilises a k-D -tree structure.}
\label{algorithm:cluster_extraction}
\end{algorithm}

\citep{Rusu09}

The visual indicator that was added in the object segmentation visualisation turned out to be very useful in understanding where the load selection coordinate was, and it was used to move the proximity based selection center using ROS rqt\_reconfigure sliders.

\subsection{Ground Plane Estimation}
\label{subsubsection:ground_plane_estimation}

A parameter estimation technique was used to find the ground plane that is visible in the test data images. Parameter estimation is a technique for finding models in data sets. In this case, the parameter estimation tries to fit a model of a geometric primitive shape to a geometry data set. The geometric shape that is selected for the parameter model is in this thesis a plane or a cylinder model depending on the function. Shapes such as planes, cylinders, and spheres are usually used, but any model that can be presented implicitly using parameter model may be used. The existence of the shape in the data set is tested by finding inliers and outliers that determine whether the data set supports the model or not for some initial guess.

Classic parameter estimation techniques, such as least squares minimization, effectively optimize the fit of the selected parameter model using all points in the data set. In 1981, Fischler and Bolles presented a new parameterized model estimation technique called random sample consensus (RANSAC) that can improve upon classic techniques by detecting outlier points in the data, and by taking into account the gross errors they induce to the fitting algorithm \ref{Fischler81}.

While classic parameter estimation techniques incorrectly assume that most outliers are smoothed out in favor of good results, the RANSAC technique initializes the inlier point search with a small initial data set, and accumulates inliers when consistent data points are found. This way, not all data set points are used in the geometric fitting computation, and the solution is found easily even when gross errors are present in the data set. In the implemented test software, the RANSAC algorithm from PCL package is used, which finds the ground plane from point cloud data sets successfully for most cases. 

Originally, the RANSAC algorithm was used to solve the location determination problem (LDP), which states that for a set of control points whose 3d location is known, a location of the camera is computed when a sufficient set of control points are visible in the projected image taken from the unknown location. The LDP problem is actually the camera calibration problem, which was presented in chapter \ref{subsection:single_camera_calibration}. RANSAC can solve the external location of the camera according to landmarks seen in the image, but in this thesis RANSAC is used to find the ground planes from the geometric data sets. If the ground plane data points are located correctly and removed from the point clouds, then the remaining points are parts of visible objects in the scene. 

Some variants of the RANSAC algorithm are 
\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item M-estimator Sample Consensus (MSAC)
\item Maximum Likelihood Estimator Sample Consensus (MLESAC) 
\item Progressive Sample Consensus (PROSAC)
\end{itemize}

but there are many more depending on the estimator technique used. When RANSAC algorithm produces poor results in parameter estimation application, other variants can be used if they are more suited for the data set. Many options can be found in the literature. \ref{Torr00} \ref{Huber81}

While most of the time the RANSAC algorithm finds the ground plane correctly, there are some conditions that must be fulfilled in order to find a correct solution. First, the ground plane must contain the largest number of points in a planar fashion. Otherwise, some other plane will be found by the algorithm, such as a large warehouse wall.

\subsection{Cylinder Fitting}
\label{subsection:cylinder_fitting}
%+ a great cylinder fitting scheme can be found in Pekka Forsman doctoral thesis

% Large images of a single cylinder fitting result
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{cylinder_fitting_pt1.jpg}
    \caption{Cylinder fitting image 1/3 shows the original point cloud data and the bounding volume of the found cylinder in red.}
    \label{fig:cylinder_fitting_pt1}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{cylinder_fitting_pt2.jpg}
    \caption{Cylinder fitting image 2/3 shows the segmented load objects assigned with colors, and the length of the found cylinder in millimeters.}
    \label{fig:cylinder_fitting_pt2}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{cylinder_fitting_pt3.jpg}
    \caption{Cylinder fitting image 3/3 shows the selected load object point cloud and the extracted cylinder.}
    \label{fig:cylinder_fitting_pt3}
  \end{center}
\end{figure}

\section{Bounding Volume Computation}
\label{section:bounding_volume_computation}

+Results from the software:
+ Big standard box dimensions
+ Mean X: 1.67 meters
+ mean Y: 1.47 meters
+ mean Z: 1.06 meters
+ Real values:

% oriented bounding box tree => find minimum bounding box, cut with a plane against main axis, find more minimum bounding boxes

%Keywords: bounding volume, collision detection, urban simulation, AABB (axis-aligned bounding box)

Bounding volume is the main product we wish to generate out of all the computation done in the PCL library previously. The format of the information output from the underlying software depends on the user, but we can present some suggestions for standard solutions. 

Usually, a volume of an arbitrary object is presented as a linear combination of simple geometric objects. These prototypes of geometric  objects support some or all of the linear operations available, such as summation, subtraction, multiplication and division.

One of the simplest bounding volume outputs is an axis-aligned bounding box, or AABB, applied in 3 dimensional Euclidean space. AABB is the minimum perimeter along the directions of the axes that span the Euclidean space that fully contain the target object. AABB is straightforward to find by computing minimae and maximae along each axis and by spanning a convex polyhedron so that we select all minimum values in one corner of the polyhedral graph, and we select all maximum values for the opposing farthest possible corner of the polyhedron.

AABB computation can be easily optimized, but the biggest drawback of this technique is the large amount of empty space not occupied by the object that the AABB volume contains. In the special case of stick-like objects, or planar objects, whose orientation is maximally off-axis aligned, an AABB volume representation will show a volume that is occupied less than 10 \% by the actual object.

Another simple solution is an oriented bounding box. If we first specify a main longitudal axis for an object, and then calculate its orientation, we can iterate the smallest possible bounding volume that is oriented along the object axis. This will tremendously help to more accurately represent a volume of special shaped objects, such as cylinders and planar items.

We can find more advanced bounding volume techniques for even more accurate object volume representation, such as bounding volume shapes, bounding volume hierarchies (BVH), discrete oriented DOPs, k-DOPs and boxtrees. For example, BVH can be used to detect collisions, or object interference, and it is computed using raytracing and culling. 


\subsection{Bounding Volume From 3d Axis-aligned Bounding Box}
\label{subsection:bounding_volume_from_3d_axisaligned_bounding_box}

+ possibly previously unknown object who is suspended from the crane end effector tool. The form of the output information should be simple while still carefully thought in order to 


It is computationally expensive to update the AABB bounding box as the load orientation changes via rotation, and it is suggested that in order to fight excessive computation, the bounding box should be loosely defined so that with small angle changes there is no need for recomputation \cite{Ericson05}.

\subsection{Bounding Volume From 3d Oriented Bounding Box}
\label{subsection:bounding_volume_from_3d_oriented_bounding_box}

The 3d oriented bounding box (OBB) is similar to the axis-aligned bounding box (AABB) with one difference: the bounding volume fills the cubic shape of the OBB optimally so that the bounding volume is minimized. This means that the OBB first computes a main axis for the load object and then the object frame is rotated along the axis. Now, the bounding box for the load object is seen as rotated in the world frame along the object, and the bounding volume becomes invariant to load rotation in the world frame.\par
The PCL library versions 1.7 and higher support a version of the oriented bounding box computation using moment of inertia and eccentricity based descriptors. With these descriptors the eigen vectors of the point cloud object can be computed, and the bounding box can be oriented along the largest eigen vector in a right-hand normalised coordinate system. The OBB along the major eigen vector axis does not guarantee minimality of the bounding volume, but the method does make the bounding volume computation more invariant to changes in the load object orientation in the world coordinate space.\par
Unfortunately, the current version of ROS Groovy does not include the moment of inertia and eccentricity descriptors required to compute the OBB. Consequently, future versions of the software running on ROS Hydro with PCL 1.7 and higher can improve the bounding volume accuracy by implementing the oriented bounding box computation. The AABB bounding volume computation currently used should be replaced with the OBB computation if the reported bounding volume accuracy is increased.

\section{Algorithms}
\label{section:algorithms}

\subsection{Load Selection Algorithm}
\label{subsection:load_selection_algorithm}

\subsection{Segmentation Center Selection Algorithm}
\label{subsection:segmentation_center_selection_algorithm}

\subsection{Cylinder Growing Algorithm}
\label{subsection:cylinder_growing_algorithm}

The Point Cloud Library cylinder fitting process outputs a parameter set pcl::ModelCoefficients that contains 7 parameters: 6 of them cover the coordinates of two random points on the main cylinder axis, and the final parameter reports the radius of the cylinder. Thus, the starting point and the ending point of the cylinder are not computed at all by the RANSAC computation. To solve the issue of missing cylinder length, an algorithm was written that solves the cylinder length. The algorithm projects points on the main cylinder axis using standard dot product computation, and grows the cylinder length until all the points in the cylinder data are iterated through. The working of the algorithm is as follows:

\begin{algorithm}[H]
 \KwData{Load Object Cylinder Segmented Data From RANSAC}
 \KwResult{Fitter cylinder end point coordinates}
 compute N units long vector along the main cylinder axis so that all data points project on it\;
 compute closest data point to centroid of data\;
 initialise a start point at the closest data point from centroid projected on the main cylinder axis\;
 initialise two subcylinders who represent distance along the cylinder main axis starting from start point to the end points in two opposing directions\;
 \While{data points left}{
  iterate to next point cloud data entry\;
  compute projection on cylinder main axis using dot product\;
  \eIF{projection is in negative coefficient vector direction}{
       eIF{check if distance to start point greater than current subcylinder 1 height value}{
        set projection point as the new end coordinate for subcylinder 1\;
        }{
        continue\;
        }
   }{
       eIF{check if distance to start point greater than current subcylinder 2 height value}{
        set projection point as the new end coordinate for subcylinder 2\;
        }{
        continue\;
        }
   } 
} 
return subcylinder 1 top coordinate and subcylinder 2 top coordinate\;
\caption{Cylinder growing algorithm}
\end{algorithm}

\chapter{Evaluation}
\label{chapter:evaluation}

\section{Evaluation Of Software Design}
\label{section:evaluation_of_software_design}

The design of the load object measurement system evolved a lot during its implementation phase. The original design was a non-linear, component-based design running on a C++ code base. Originally, a network of atomic processing components was designed where data would be sent to all the required processing stages using ROS topics and subscribers. Since the amount of data generated in the stereo cameras was quite large, the design had to be changed to a more linear design where the bandwidth of the ROS networked messages per stereo pair was minimised. The final design included more logical, larger processing components that had many positive effects on the overall performance of the software compared to the first designs: 

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item The overall workflow was more usable and easier to launch (less nodes were started)
\item Less bandwidth was used in the ROS network (less publishers and subscribers needed to be configured)
\item Data reduced and information increased per component principle was enforced by the new design   
\end{itemize}

This chapter evaluates the implementation of the final software schematic presented in Figure \ref{fig:software_architecture}. The design proved to enable stable heterogenous PC network communications with the Himmeli platform on wired LAN architectures in tests. Tests were run in an actual overhead crane environment, and also in laboratory conditions with simple networks (no hardware routers). Many readily available ROS software packages for visualization, configuration and debugging tools were used to get the networks running as designed. These utilities proved to be of tremendous help in configuring and troubleshooting the network settings. No tests were run in a wireless network environment, but it is quite possible that the current system would be bottlenecked in a wireless network due high data bandwidth requirement.

For the selected stereo camera setup and tested networks, any amount of data could be relayed from any software component to the next one successfully by using networking with topics and subscribers. No issues were found, although some limitations posed by the architecture were discovered: 

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item The ROS network introduced varying length delays, as expected
\item Custom ROS message contents (LoadArea.msg) could not be viewed on subscriber machine that had no message headers built
\item Implemented PCL visualisation introduces large periodic delays of more than 1000 milliseconds 
\end{itemize}

All the ROS installations used in the PC network were running on ROS Groovy, and no compatibility issues were found in the bounding volume measurement system network even when different build environments were mixed (catkin and rosmake). It is reported that different versions of ROS distributions are not cross-compatible, thus, ROS Groovy was selected as the version to be installed on all systems.

Currently, the ROS distribution functionality poses most issues for consideration in the overall design. The biggest risk in the project start was missing ROS knowledge in the project team, and it was known that the ROS programming learning curve is steep for beginners. The missing knowledge risk realised as longer development time of the system, but as such it was an acceptable risk to take. Since ROS is a community-based effort to provide software nodes for robotic programming, a well-maintained index of all the available content did not exist, and it was often difficult to find the right tools and learn how to use them. Tutorials and examples were used to implement all ROS node functionality, and some features, for example, the dynamic reconfigure server (rqt\_reconfigure package), suffered from unstable functionality at the time.

Then again, the PCL library documentation proved to be excellent, easy to navigate, and very well documented. All the point cloud processing features were tested using PCL tutorials, which helped a lot in understanding point cloud computation. Then, the code was further advanced with the help of the PCL API documentation. All point cloud processing functions are optimised for fast processing of data, and interactive visualisations are easy to implement using the PCL visualisations library. 

The only problem in the PCL design was the visualisation introducing quite periodic delays as long as 1000 milliseconds. This may be due large amounts of memory reserved and freed periodically, but a further analysis of the source of the delay could not be done in the time used for software development. A suggestion for improvement regarding the software design would be to run memory management and process thread bug testing for the software to counteract errors in continuos operations. Another suggestion would be to branch out a version of the software where the visualisation may be disabled after the machine vision system is properly configured. This would effectively make the software faster, because it takes most time to render point clouds for the visualisation window.

The evaluation of the load object measurement system was done using offline data sets that were recorded from use cases in real crane environments. The results of the evaluation will be presented in Section \ref{section:experiments}. The online implementation was finished but not tested during the time of writing this, and as such the effects of online performance are not included in the review of software architecture.

\section{Evaluation Of 3d Data Quality}
\label{section:evaluation_of_3d_data_quality}

The point cloud data quality was affected by the overall machine vision process including stereo calibration, disparity tuning, and the hardware setup. The hardware setup and the stereo calibration were used as monolithic initial calibration items. The calibration was done automatically for the hardware setup using OpenCV as described in Section \ref{subsection:opencv_stereo_calibration_workflow}, and after a good basic point cloud output generation was achieved, no further engineering was done for the hardware or the stereo calibration. After an initial point cloud output was generated, the problem of understanding the data quality could be presented: how accurately does the point cloud data resemble the real world scene?

The generated point cloud resembles the real scene, but some noisy parts or parts that are difficult to model, such as hoisting wires in a crane environment, do not look like their real-life counterparts. Accuracy of the point cloud representation can be quantified using point cloud data quality indicators. The most important indicators in point cloud data quality assessment are

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item location accuracy of the surface, 
\item surface noisiness,
\item speckle regions, misinterpreted Z depth regions, and artefacts of any kind,
\item and number of finite data point entries.
\end{itemize}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{riegl_reference.jpg}
    \caption{A point cloud of the forestry crane boom environment taken with an industrial Riegl terrestial laser scanner.}
    \label{fig:riegl_reference}
  \end{center}
\end{figure}

Point cloud data quality indicators are used in modeling of urban constructed environments and in airborne LIDAR mapping. In these fields, the quality evaluation is done using two different goodness analysis mechanisms: spatial structural analysis and positioning analysis. The need for different quality indicators depends on the application, but the constructed industrial crane environment encourages to try spatial structure analysis for quality evaluation. Positioning analysis cannot be done in this work since multiple view registration is not used \citep{Feng08}.

In literature, also reverse engineering of shape primitives from point cloud response have been used for point cloud data quality verification in industrial settings. In industrial environments, a lot of box-shaped or cylinder-shaped objects can be found and be used for verification. Also, for any primitive geometric shapes it is fairly simple to quantify a goodness of fit value using geometric fitting schemes via least squares error minimization \citep{Fidera04}.

In spatial structure analysis, some real world feature qualities are measured from the environment, and the same qualities are then computed from the scene's point cloud response. Such qualities are for example: location, orientation, volume, smoothness of surface, or some other attribute of the item. Next, these measures are compared and it is possible to determine a goodness of fit -value that can be further processed into a goodness of data quality information. Formulation of a meaningful goodness indicator may be difficult in presence of missing surfaces, measurement noise, and increasing depth estimation error with distance among other problems with a point cloud presentation.

For position accuracy verification, landmark objects may be installed in the environment, and their correct locations can be searched for and verified in the point cloud data. For orientation verification, eigenvectors may be calculated for a point cloud describing an object and the largest eigenvector direction can be compared to the real-world object main axis. For volume verification, an oriented bounding volume may be computed for fully visible objects using the previously mentioned eigenvector orientation, and be compared to the actual real-world object volume.

In this work, installed landmarks or beacons would have been useful in the offline data sets since they could have been used to verify the correct scale of the point cloud, and correctness of local measured lengths. Since the data set environments did not contain any installed landmarks, a scale verification is not available. A suggested improvement would be to install spherical 3d landmark objects in the crane environment prior to dataset capture, so that spatial structure analysis could be done using the landmarks and their known locations.

%In this thesis, such objects whose qualities are known a priori are being called standard objects. For example, in the process hall data sets, a standard object titled \emph{large box} with dimensions of 1.52 x 0.94 x 0.92 meters was used. It can be seen in most process crane data sets as part of the setup. The large box object was constructed out of metal sheets, and it was a box-shaped, partially see-through mass weighing in 3.97 tonnes. In the forestry crane measurements, no standard objects were used, because the tree stumps were not measured nor tracked as individual standard objects. As such, the forestry crane data sets cannot be used for point cloud data quality control. In case a CAD model of the forestry crane boom was available, spatial structure analysis could be done for forestry data sets, too, but in this work no CAD model was available.

A single effective spatial structure analysis that was possible to do with the captured offline data sets was to check how big was the angle between standing cabinets and the floor in the process crane data sets. For example, a cabinet might be known to form a 90 degree angle with a floor, and such a scene would generate a point cloud response where the angle is not exactly 90 degrees (it could be e.g. 85 degrees). Obviously, this will give indication of goodness of structural integrity only for the local neighbourhood of the feature in the point cloud. For example, good structural integrity in the near-field feature does not guarantee similar accuracy for far-field features.\par
The effects of different stereo camera hardware setups, for example toed-in optical axes installation against parallel installation, were not researched and no definite results can be said of different stereo camera setups on the point cloud quality. A toed-in hardware installation was used in this work, and some vertical disparity definitely affected the point cloud quality, but it was a minor hindrance. By visual inspection, physical moving of the cameras and recalibration of the system generated no visible change in the point cloud quality. The toed-in setup does enable negative disparity values for z-depths that do not exceed the crossover depth for optical axes point of intersection, which was approximately at 1.6 meters. Toeing-in made the overlapping image areas smaller, but it also increases the stereo imaging volume to more near the camera than a parallel setup. Of course, unless disparity values less than zero are enabled, objects that are located in between the cameras near the image plane will still not generate points to the output point cloud. Using the disparity search space property in stereo processing, all the objects near the camera can be removed from the point cloud data response. This property was used to remove the hoisting wires from the output point cloud in the overhead crane data sets where the camera is installed in the trolley facing downwards. Hoisting wires that were more than 1.6 meters away from the camera still could make it into the generated point cloud, but their location was computed erroneously most of the time.\par
After the camera hardware setup was reconfigured, the toe-in angle became larger then previously. This resulted in a smaller region of interest in the rectified images, which means that the usable image area becomes smaller as the toe-in angle is made larger. OpenCV takes this change into account in the stereo rectification process, but as the rectified image becomes more and more distorted with increased toe-in angle, the region of interest becomes smaller, resulting in less accuracy in the reprojection phase.


It should be reported that one time after the right stereo pair camera was moved due a collision of the platform, the cameras were re-installed and a new stereo calibration was done for the HIMMELI platform. In the new hardware setup, the toe-in angle between the camera optical axes was larger then previously, resulting in a smaller overlap of the image regions because of very cross-eyed camera images. The point cloud data quality was affected so that the effective region of interest in the rectified stereo image pair was smaller than previously due to the toe-in angle increase. This was discovered when comparing the calibration files from the original camera setup to the calibration files computed from the new installation. According to the current knowledge by the author, the point cloud data quality was affected very little by the hardware setup change and a re-calibration. Still, more testing and a point cloud quality goodness measure needs to be formulated in order to verify the point cloud data quality indifference towards different hardware setups with calibration.

Measured height of the cameras installed from the floor was 545 cm.

\section{Parameter Tuning Evaluation}
\label{section:parameter_tuning_evaluation}

% reference Kim05 | Jet Propulsion Lab  

\subsection{Disparity Tuning}
\label{subsection:disparity_tuning}

The disparity map generated with standard OpenCV functions needed parameter tuning to provide the best possible reprojection of geometric data from a 2d image to a 3d point cloud. The basic tuning of the block matching algorithm (BM) parameters was done first, and then a semi-global block matching algorithm (SGBM) was tuned with additional parameters. The tuning was done with a custom software tool that displays slider ranges for each parameter. The tool was made for what you see is what you get (WYSIWYG) operation, and the resulting disparity image is displayed instantaneously next to the sliders.
    
\image{disparity_space_from_zitnick_report}

\subsubsection{Block Matching Algorithm Tuning}
\label{subsubsection:block_matching_algorithm_tuning}

The block matching algorithm is tuned with 10 parameters who reside in a BM state object. The parameters are listed in table \ref{table:block_matching_state_parameters}. Additionally, a pair of regions of interest (ROI) are set up in the BM state to mark off the new effective image regions after rectification process. To be able to understand the parameter effects on the block matching performance, a quick note about the inner works of the algorithm is introduced next.

The feature matching works in three steps:

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item pre-filtering to normalize image brightness
\item correspondence search with SAD window along the epipolar lines 
\item post-filtering to remove bad matches
\end{itemize}

The tuning of the BM parameters was done against offline data collected from both selected use cases. The forestry crane data was preferred in the tuning effort since more objects can be seen in the images, thus, the saliency content of the captured video is higher compared to the process hall crane area. High saliency image regions produce good results in block matching, and a rule of thumb would be: more visible 2d objects result in more correctly matched points in the region for the 3d point cloud. For example, the texture rate of change for a matt painted warehouse floor is small, and in the process hall crane case the collected data had minimal floor feature matching response for some BM tuning sets.

\subsubsection{Block Matching Parameter Effects on Feature Matching Performance}
\label{subsubsection:block_matching_parameter_effects_on_feature_matching_performance}

First, the tuning was started with a coarse selection of parameters to get a visible disparity map. Especially, the SAD window size was kept higher than normal to produce a lot of visible matched points. The most important parameters in the final BM state tuning set are SAD window size, number of disparities, and speckle range parameters. These parameters should be tuned first, and other parameters can be used for fine tuning of the disparity map afterwards. 

The SAD window size, or sum of absolute differences window size, affects the size of the sliding kernel window used for correspondence search in the 1d epipolar search space of the rectified images. The smaller the value, the smaller the sliding smoothing kernel used is, and with a minimum value the search is called pixel-to-pixel correspondence search. For metric measurement purposes the disparity map should be as accurate as possible, thus, the minimum value of the SAD window size will be used at all times. If the rectified images are texture rich, then a match can be found for all pixels on an epipolar line. Then again, if the image pair is less rich in texture, for example, a matt painted warehouse floor, the process will find less matches, which usually is the case. A low texture environment will produce little found matches because the local neighbourhoods of the sliding window are too similar in order to confirmed a match. If the point cloud is used for 3d visualization purposes, a higher SAD window size value selection should produce more points for the delight of the viewer.

The number of disparities parameter sets the maximum disparity difference in pixels for a search space in a disparity map. The higher the value, the higher a pixel difference value is allowed in the disparity map. For example, if a very near-field object has a disparity difference of N pixels, and number of disparities property is set smaller than N, then the near-field object is not detected at all in the disparity map leaving a 'hole' in it. This property can be used to remove near-field occluded objects that are impossible to triangulate successfully. For example, in the process hall crane case, the hoisting wires are problematic since they are occluded and visible only from one side in each camera. With number of disparities parameter it is possible to remove the wires from the disparity search space, and altogether from the final point cloud depicting the crane working area. 

The wires are included in the disparity search from certain height onwards, but with a proper selection of number of disparities it is possible to minimize the erroneous reprojection of the hoisting mechanism in the final output point cloud. The user can use the number of disparities property in combination with the minimum disparity property for setting the horopter - the 3d volume that is covered by the stereo matching algorithm \ref{Bradski08}.

Horopter can be enlarged towards the camera image plane with changes in the stereo system parameters. Some changes that enlarge the horopter are decreasing the stereo camera baseline width, decreasing the focal length of the cameras, and increasing the disparity search space.

Speckles are high and low disparity patches generated by the block matching algorithm near object boundaries. The block matching sliding window will catch object foreground and background in the image pair, which is the problem that generates speckle in the disparity map. Thus, the third important parameter is the speckle range parameter. The speckle range controls the threshold for letting the local patches of speckle be matched to the resulting disparity map. If the threshold is met, then the local region is eliminated and no disparity matches will be available for that region. The speckle detector that uses the speckle range value also requires the speckle window size parameter. The value for the window size should be small enough to keep the computation to a minimum, but large enough to detect speckle regions properly.

For each use case, multiple tuning sets were designed, which can be visually inspected by viewing at the point cloud output quality. On one hand, some processes will not work with a small number of output points, such as the RANSAC iteration for a ground plane search. On the other hand, the metric accuracy of the point cloud points decrease as the number of points in the cloud increase. The resulting point cloud from stereo cameras is always a best possible trade-off between accuracy, quality, and size of the cloud. Since the amount of point cloud points heavily affects the time used for further processing of the cloud, accuracy is the number one priority decreasing the computation times simultaneously.

Over time, the quality of the bounding volume measurement depends on many things: primarily the amount of noise in the point cloud object over time, and secondarily on the orientation of the object over time. 

\begin{table}
\centering
\ra{1.3}
\caption{List of parameters in a block matching state object and their value ranges and requirements in OpenCV C++ image processing library.}
\begin{tabular}{@{}llll@{}}\toprule
\textbf{Parameter name} & \textbf{Minimum} & \textbf{Maximum} & \textbf{Other} \\
\midrule
Pre-filter size & 5 & 21 & Odd number \\
Pre-filter cap & 1 & 63 & \\
SAD window size & 5 & 255 & Odd number \\
Minimum disparity & -100 & 100 & \\
Number of disparities & 16 & 256 & Divisable by 16 \\
Texture threshold & 0 & no upper limit & \\ 
Uniqueness ratio & 0 & 255 & \\
Speckle window size & 0 & 100 & \\
Speckle Range & 0 & 100 & \\
Disp12MaxDiff & 0 & no upper limit & \\
\bottomrule
\end{tabular}
\label{table:bm_state_parameters}
\end{table}

The tuning effort resulted in best possible BM parameters for each use case. The process hall crane parameters are listed in table \ref{table:bm_parameters_overhead}.

\begin{table}
\centering
\ra{1.3}
\caption{Block matching parameters (OpenCV) used in the overhead process crane case. Parameters reported from early development phase and not currently associated with any dataset.}
\begin{tabular}{@{}ll@{}}\toprule
\textbf{Block matching parameter} & \textbf{Value} \\
\midrule
Pre-filter size & 9 \\
Pre-filter cap & 63 \\
SAD window size & 5 \\
Minimum disparity & 0 \\
Number of disparities & 96 \\
Texture threshold & 270 \\ 
Uniqueness ratio & 20 \\
Speckle window size & 42 \\
Speckle range & 10 \\
Disp12MaxDiff & 10 \\
\bottomrule
\end{tabular}
\label{table:bm_parameters_overhead}
\end{table}

\begin{table}
\centering
\ra{1.3}
\caption{Block matching parameters (OpenCV) used in the forestry crane case. Parameters reported from a forest case dataset 1377169118 on February 25th 2014.}
\begin{tabular}{@{}ll@{}}\toprule
\textbf{Block matching parameter} & \textbf{Value} \\
\midrule
Pre-filter size & 9 \\
Pre-filter cap & 63 \\
SAD window size & 5 \\
Minimum disparity & 0 \\
Number of disparities & 144 \\
Texture threshold & 30 \\ 
Uniqueness ratio & 1 \\
Speckle window size & 42 \\
Speckle range & 14 \\
Disp12MaxDiff & 1 \\
\bottomrule
\end{tabular}
\label{table:bm_parameters_forestry}
\end{table}

%\begin{table}
%\caption{Best BM parameter tuning set in forestry crane case.}
%\label{table:forestry_crane_bm_parameters} %label just after caption
%\begin{tabular}[c]{|p{4cm}|l|}
%\hline % The line on top of the table
%\textbf{Parameter Name} & \textbf{Value} \\
%\hline

%\end{tabular} % for really simple tables, you can just use tabular
%end{table} % table makes a floating object with a title

\section{Experiments}
\label{section:experiments}

\subsection{Overhead Crane Measurements}
\label{subsection:overhead_crane_measurements}

All the analysed overhead crane image data sets were captured in an actual overhead crane test environment in Finland in August 2013 as part of the GIM project. A total of 13 data sets were captured that contain different tests revealing how the dimension measurement of the load object can function in different scenarios. Environmental conditions did not change during the tests in the indoor process crane hall, which is why all the test were run in the same conditions.

A manual load selection approach was used since no end effector location service was available at the time of data set recording. Lack of 4d tracking of objects was compensated by selecting a pre-defined coordinate that selects the nearest (Euclidean distance) object as the load object. This proximity-based solution was easy to implement, but the output of the load object selection selects wrong objects occasionally as load objects. Mainly, the effect of proximity load object selection can be seen in the data visualisations, which seem cluttered because of sequentially changing objects are measured and report differing values.

The manual selection coordinate was set in the center of the captured image, where a load object usually appears in e.g. the top-down camera configuration. The manual load object selection introduced a lot of erroneous load object selections, such as people walking nearby the center of the image, or new objects selected as they came in the view as the platform moved in the process hall.

The coordinate system was not transformed to the world coordinate frame in any data sets, but kept in the sensor frame at all times during data processing. The sensor frame was used for processing because the sensor frame implicitly encodes the moving platform signal as part of the data. An external end effector location signal and a camera viewpoint signal are needed if the transformation to the world coordinate frame is done. The data in the sensor frame did not introduce any singularities or other mathematical problems, thus, no normalisation was done to the point cloud data before processing. If the data is transformed in the world coordinate frame, care must be taken to normalise the data if any computational singularities are introduced because of a coordinate origin change.

The effect of the fluorescent lighting in the environment was not analysed since the lighting conditions could not be controlled. The tests were done in 50 Hz fluorescent lighting conditions that may be found in most warehouses. The fluorescent lights have some effect on the image quality, but it is unknown how the HDR camera parameters change in different lighting conditions, thus, modeling would be difficult without better knowledge of the HDR camera image formation. The effects of lighting conditions on the image quality in machine vision solutions are a vast research topic on their own and not detailed in this work.

\subsubsection{Findings From Top-down Viewpoint Datasets}
\label{subsubsection:findings_from_topdown_viewpoint_datasets}

%1372841000
From dataset 1372841000 it was found out that the items residing in the process crane hall are segmented in good detail as separate object entities. The geometries of items are well-preserverd in the top-down camera setup. Accordingly, the objects in the near field of the camera, such as the upper balcony, are surface reconstructed in fine detail by the stereo vision system. Finally, the disparity map includes some artefacts that generate erroneous 3d constructs in the point cloud data. The best example of an artefact is the large grey bar that can be seen on the right side of the disparity map. The grey bar is reprojected as a curved tall beam-like feature and it can be seen on the right side of the original 3d data point cloud output. This artefact could be effectively removed by using a smaller ROI before reprojecting the data into 3d.

Another issue found in the geometry reconstruction was caused by a horizontally aligned striped black and yellow tape, which resulted in a depth measurement error. Although the tape was part of the factory floor and should be removed from the 3d data in the process, it remained in the data as a separate object seen below the floor level because of unsuccessful block matching.

%1372842312
From dataset 1372842312 it was found out that objects suffer from enlarging effect that will enlarge objects due to disparity map processing. The enlarging effect will affect groups of objects placed near each other the most, causing fusing of objects into single object entities as seen in figure \ref{fig:enlarging_effect}. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.3\textwidth]{enlarging_effect.jpg}
    \caption{Enlarging effect seen in dataset 1372842312. The operators are separated with a gap between each other, but the segmented object view shows the two objects fused as one object due to disparity enlarging effect.}
    \label{fig:enlarging_effect}
  \end{center}
\end{figure}

%1372842573
From dataset 1372842573 it was noticed that the small end effector hook used in this dataset gives a small point response of approximately 60-80 data points when the end effector is lowered 2 meters above the factory floor. The end effector response is so small that it is discarded from the segmented data as noise with the current segmentation tuning. If the end effector is raised near the ceiling, the response becomes larger. The end effector response could be tracked with an extended Kalman filter, because the small response is consistent throughout the data set.

The dataset also shows that the operator and the load object will merge as a single object when the operator stands near the load object or touches it. In the event of the operator and the load object fusing, the bounding volume will become too large accordingly. In general, any objects will merge into nearby objects when the Euclidean distance between two objects becomes smaller than the threshold value used in object segmentation.

Finally, when the load object was lifted very high up near the stereo camera platform, it occluded most of the factory floor seen in the image. If the floor is not visible in the left camera image, the ground plane segmentation step will fail. The event of ground plane removal failure was not encountered in any of the datasets although there is a danger that the ground plane will be fully occluded. The perception platform installation location in the trolley seems good according to this data set since ground plane estimation did not fail during testing.

%1372842898
%\item The reprojection issues while the platform is moving depict the floor both too far away and too close in turns. This suggests that the HIMMELI platform shutters do not exposure images in the same left to right, or right to left order, but the order changes randomly due to network lag or other sync issues.

In dataset 1372842898 the overhead crane was moving while stereo images were captured, which introduced problems in the estimation of ground plane depth. It was noticed that the ground plane geometry showed large depth projection errors in frames where the overhead crane was moving. It was concluded that since the stereo camera head shutters are strictly not enforced to expose the scene at the same time, the platform has time to move before the second camera shutter is activated. This results in an increase or decrease in the stereo baseline width $b$ randomly between the frames (depending on which stereo head exposes the image first). The changing baseline will result erroneous depth values for all motion field vectors when the depth computation assumes a constant baseline width. This also implies that only the moving parts of the image are affected, for example, the ground plane depth is highly affected. The load object geometry measurement is not affected since the image of the load object is not moving relative to the cameras.

To counteract moving platform inaccuracy, the platform should not move while the environment is being modeled, or the shutter behaviour should be synchronised with a clock generator signal. 

%1372843166
In dataset 1370843166 it was noticed that when people walk exactly under the camera view in a top-down camera setup, the point response for a person will become so small that it is discarded as noise in the point cloud data. On one hand, this result suggests that the limit of the point cloud object size should be made smaller than the currently used value (80 data points) to be able to detect small targets. On the other hand, making the limit smaller will allow small disparity speckles clutter the 3d data as objects. Since the original point cloud is downsampled using a voxel grid filter, such a scenasrio may not be problematic at all, because noise speckles are filtered away from the downsampled point cloud.

\subsubsection{Findings From Bird's Eye Viewpoint Datasets}
\label{subsubsection:findings_from_birdseye_viewpoint_datasets}

%1372851907
A bird's eye viewpoint is introduced in the dataset 1372851907. It was found out that the load object being lifted near the opposite side wall of the process hall results in a smaller point response than the load object in the same spot using the top-down image. It is also very likely that a load object situated near the opposite wall will fuse with the wall point response. The load object could not be measured if it merged with the back wall. The more distance to the cameras, the more easily merging of objects happens since the measurement error increases, and point entries will easily deviate from their actual values and cause merging. The distance to the opposite wall was quite large, even 10 meters, which caused less accurate depth estimation and object merging.

Another problem with the bird's eye viewpoint was that the backside of the load object was not visible at all, thus, the backside of the load object was not inside the computed bounding volume.

Finally, the hoisting wires output a clearly visible point cloud response that was included in the load object bounding volume. Inclusion of the hoisting wires in the load object is not preferable when the geometry of the load object is measured since the hoisting wires increase the bounding volume a lot. The extra reported volume could be removed using knowledge about the end effector location and heuristics. It must be noted that from this viewpoint the 3d modeling of the hoisting wires works correctly although the wires suffer from quite large enlarging effect.  

%1372852148
% todo: computation time increase as a function of data points in the model in forestry case!
It was found out from dataset 1372852148 that the 3d model of the scene includes many more data points when the cameras are in a bird's eye configuration. This is because the large back wall generates a lot of points. It was also noticed that computation times increase considerably because the amount of data increases. 

%1372852419
In dataset 1372852419 the ground plane was not fully removed by the RANSAC estimation procedure at the furthest process crane hall location. The cause of this is mainly induced by the disparity mapping in the interface of the floor and the wall where a sharp 90 degree turn is not well modeled. In reality, the disparity map can only approximate sharp turns with smooth trajectories, thus the floor seems to curve up and continue as a wall section. The modeling gets less accurate as the distance from the stereo cameras increase, and at 10 meters distance the smooth curving effect is so large that the ground plane is not fully removed because of curving near the wall section. Still, the ground plane estimation works flawlessly, and the curving effect does not cripple RANSAC estimation.

Figure \ref{fig:rigth01372852616451} shows that the problematic curving of the opposite wall is generated by image regions in the lower part of the image. According to figure \ref{fig:distortion_model_right_cam} the lens distortion is not very large in the lower part of the image, which means that the effect is mostly caused by the disparity mapping smoothing property. Section \ref{section:depth_from_disparity_map} details disparity mapping and its trade-offs in more depth.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{rigth01372852616451.png}
    \caption{Right camera rectified image from dataset 1372852419. Image was captured on June 3rd, 2014 in a test overhead crane environment.}
    \label{fig:rigth01372852616451}
  \end{center}
\end{figure}

%1372852802
Dataset 1372852802 proves that it is possible to lift the load object fully out of the stereo camera image if the load object is located far away enough. The minimum visibility of the load object is a function of world frame coordinate axis $Y$. It was also found out that if the load object is located near the opposite wall, the load object will easily merge with the curving floor section even though the load object is already lifted high in the air. Finally, when the load object was lifted near the cameras, the visible surface model of the load object was recovered accurately with high resolution. The accuracy of the stereo reconstruction depends on the distance to the load object and its effective image size.

%1372853387
In dataset 1372853387 some partially see-through objects were registered. It was found out that partially see-through items cannot be stereo matched, thus, they are invisible to the machine vision system implemented. Also shiny metallic materials may have an effect on visibility in the images, because they violate the assumption of lambertian reflectance model used. Another remark from this dataset was that upright pole-shaped objects can be easily modeled from the bird's eye view used. Top-down cameras have difficulty on modeling other than the top part of a standing pole. Such objects include road construction cones e.g. It is easy to generalise that objects whose main axis is perpendicular to the camera optical axis are better modeled than objects whose main axis is in parallel with the optical axis of the stereo vision system.

%1372853605
In dataset 1372853605 two non-reflective large markers were added on the see-through load object. The load object then generated two visible point response speckles as output. Still, measuring a geometry of an object that is spanned between two marker point responses is not supported by the current software used in the thesis. 
%end of overhead crane datasets

\subsection{Forestry Crane Measurements}
\label{subsection:forestry_crane_measurements}

The forestry crane image data sets were captured from an actual rotary boom crane test outdoor environment in August 2013 in Finland as part of the GIM project. Another dataset collection session was done during March 2013 in snowy winter conditions, but there was depth information loss due to a perception platform malfunction. The setback was unfortunate, thus, no findings from reflective snow and winter conditions are included.

In the forestry crane data sets, an automatic TLD tracker was used in combination with a cylinder fitting scheme to provide more meaningful results. The TLD tracker provided the location of the end effector tool with great accuracy using 2d image feature tracking. A simple cylinder fitting scheme was used, which was further detailed in section \ref{subsection:cylinder_fitting}. It is good to understand that some of the findings in this section are exclusively based on the cylinder fitting results and the implemented load selection algorithm that was further detailed in section \ref{subsection:load_selection_algorithm}.

\subsubsection{Findings From The Frog Perspective Viewpoint Datasets}
\label{subsubsection:findings_from_the_frog_perspective_viewpoint_datasets}

It was found out from dataset N that objects too close to each other will merge into single objects similarly as in the overhead crane case. The merging of objects can be controlled using the k-D -tree search cluster tolerance parameter, which affects the segmentation threshold distance. Occlusion made the problem of object segmentation more difficult to solve since usually the occluding object will fuse with the occluded object given the objects are near each other. 

Heuristics should be used to prevent wrong feature matches in the cylinder fitting step. A check that the cylinder centroid is not located above the end effector in the sensor frame was used, which only prevents matching errors caused by the crane boom.
%image of the frog perspective and boom here

It was noticed in dataset N+1 that if the end effector tool is empty, the cylinder fitting procedure does not produce meaningful results. Currently, the fitting will result in the volume of the end effector tool or parts of the crane boom depending on where the RANSAC search finds a cylinder fit in the data. This behaviour adds noise and erroneous measurements in the bounding volume measurement visualisations of forestry crane tests.

A rotation of the log perpendicularly to the optical axis will report different bounding volumes for different angles of rotation in dataset N+2. The measurement was most accurate when the maximal length of the log is shown in the image. Let this angle be the 0 degrees angle. A rotation of 0-65 degrees will give similar results, but a rotation of more than 65 degrees will cause self-occlusion of the log and generate a very poor object surface model. In the case of 90 degree rotation (maximum self-occlusion) only the other end of the log is visible, and in this situation it was not possible to measure the size of the log at all.

High smoothness constraint of the disparity map fills holes that are present in the environment. For example, the loops formed by the wiring seen above the end effector tool will generate a solid surface point cloud response. This phenomenon is present in the SGBM algorithm implementation, but not in the BM algorithm implementation. It was concluded that while the SGBM algorithm produces better 3d models of the environment with more data points in the output, the BM algorithm actually can have finer detail in some features with faster computation speeds. In the end, the SGBM algorithm was more suitable for the development of the prototype software. The algorithms can be used interchangeably depending on the dataset and suitability of each algorithm.
%add an image of the wiring loop in the end effector

The cylinder fitting can handle picking of multiple logs at once if multiple logs are modeled as a single large cylinder. Multiple logs tend to be in a more hour-glass shape than a cylinder shape, which can be seen in figure \ref{fig:left1377162788433}. Because of this, the cylinder fitting can fit most volume of the logs inside a single cylinder, but the furthest ends of the logs will stay outside the bounding volume. It is possible that the enlarging effect of the disparity map generation increases the bounding volume so that the logs actually stay inside the bounding volume. It remains to be verified whether the actual logs would fit inside the computed bounding volume or not when using a single cylinder approximation.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{left1377162788433.jpg}
    \caption{Multiple log objects form an hour-glass shaped pack of logs. A left camera image from dataset 1377162465. Image was captured on August 22nd 2013 in a rotary boom crane test site.}
    \label{fig:left1377162788433}
  \end{center}
\end{figure}

In dataset N+3 it was discovered that when the crane end effector is brought in proximity of logs on the ground it is possible to measure a single log dimension even before a log is grasped by the end effector tool. When the cylinder fitting computation is used the log does not need to be gripped and lifted before it can be measured. This phenomenon is useful in speed up of the load object measurement. This property possibly eliminates the need to rotate the log object after grasping in order to measure a correct geometry.

In dataset N+3 a log was picked up from a pile of logs. The segmentation will group all logs in a pile as a single object, and the detection of a single log remains to be solved by the RANSAC estimation purely. In general, if the measured log is occluded by other logs, or nearby other logs, the cylinder fitting will easily return wrong matches without heuristics. It was noticed that it is especially easy to fit a cylinder on a person walking nearby. This could be solved once again with heuristics, for example, a good heuristic rule would be "cylinders that are in 90 degree upright position in the world frame are not load objects".

People walking in the area are not easy to distinguish as separate objects if they are nearby load objects similarly to the overhead crane case. For example, a person walking over a pile of logs will be grouped as a single object with the pile of logs.

It was noticed that goodness of 3d geometry data can be assessed using any non-moving objects. The average number of points and their average noise can be statistically measured in a point 3d region by using statistical methods. A statistical method was not implemented in the course of this work.

It was noticed from dataset N+4 that the load object cylinder fitting introduces errors because the backside of the cylinder is missing from the 3d point cloud data. A 3d reconstruction only generates a surface model, which means that the cylinder RANSAC computation estimates the cylinder model parameters using partial data. The outcome may or may not describe the actual cylinder, but possibly the cylinder translation is incorrectly estimated as too near the cameras. Missing backside of the log object does not affect the orientation estimation. The estimated position, orientation and volume of the log are useful, but the estimates could be further improved if another camera viewpoint was used. If multiple stereo camera viewpoints are used, the point cloud registration problem is introduced.

\subsubsection{Findings From The Bird's Eye Viewpoint Datasets}
\label{subsubsection:findings_from_the_birds_eye_viewpoint_datasets}

In dataset M the crane boom is fully visible in both stereo images since the perception platform is installed on top of the forestry machine driver's booth. When the crane boom is completely visible in both images it can be 3d modeled and the crane's kinematic model can be estimated. In normal operation, the crane partially moved out of the picture, which introduced challenges to continuos kinematic modeling with MATLAB. If the view would be adjusted so that the crane boom stays in the image during operation, then kinematic modeling would be used, but now it was discarded as a nice but unnecessary feature. Kinematic modeling can be used to calculate the 3d location of the end effector tool from the image data.

It was noticed from dataset M+1 that it is possible to measure the current state of the end effector tool from the 3d geometry data. It is possible to report whether the end effector is closed or open, and whether it carries a log or not. In some tool orientations the object state estimation is an easy problem, but for some orientations the problem can become very difficult.
% picture pair here of the end effector tool

It was discovered in dataset M+2 that a partially occluded log only returns the geometry measurement for the visible part of the log. For example, if 50\% of the length of the log is visible in the image, then approximately 50\% of the actual length of the log is reported by the machine vision system. Occlusion is typical in situations where the log is moved behind the crane boom, and the occluding object is the crane boom itself.

\section{Effect Of Sensor Frame Orientation On Bounding Volume}
\label{section:effect_of_sensor_frame_orientation_on_bounding_volume}

In the overhead crane case, only the bounding volumes provided by the top-down camera viewpoint produced meaningful results, thus, the results of the top-down viewpoint will be primarily presented.
Figures x and y are screen captures from the data set 1372841000, which was used to analyse the bounding volume performance for the large standard box. As seen in figure x, the actual object lied on the ground so that the object was rotated about Y axis in a 34 degree angle compared to X-Y -plane. Now, the axis-aligned bounding box computes the maximum and minimum values along an axis, which results in a maximum projection along the axes in each dimension. The axis projection makes it difficult to verify that the measurement actually is correct, but we may estimate how much the measurement differs from reality for example in per centage. If a working online version of the software was available, it would be easy to rotate the standard box into correct orientation, and tune the system so that a correct measurement is given, provided that the calibration is good. Unfortunately this is not the case, and only offline data set may be used for verification in the thesis.
Now, a comparison criterion may be formulated that relates the output value of the geometry measurement software and the actual measures of the large standard box. The mean output values computed using MATLAB for the large box are shown in table \ref{table:large_standard_box_mean_values}.

% put here a table that shows delta X: 1671.6 mm delta Y: 1473.2 mm delta Z: 1058.5 mm
% put here a table that shows X = 152 cm Y = 94 cm Z = 92 cm
% why is the delta Z smallest when delta Z is in the sensor frame.... is it?

If the standard box is rotated 34 degrees, a geometric calculation using Pythagoras theorem results a correct bounding volume size of 1787 mm x 1875.42 mm x ???

In dataset 1372852802 an interesting phenomenon is visible in the visualisation of the data. In the end of the data set it seems like there is an object that continuosly changes size especially in the Z-axis direction. When the data was reviewed, it turned out that the load object was lifted near the ceiling, and as the load object was lowered, the hoisting wires become visible more and more as the load object is being lowered. Thus, the viewpoint generates this kind of continuously changing parameters that describe the bounding volume of the visible load object and the hoisting wires of the crane. 

\chapter{Discussion And Conclusions}
\label{chapter:discussion}
% For Conclusions:
%Write down the most important findings from your work.
%Like the introduction, this chapter is not very long.
%Two to four pages might be a good limit.

\section{Discussion On Overhead Crane Load Object Measurement}
\label{section:discussion_on_overhead_crane_load_object_measurement}

In the overhead crane case, the performance of the load object measurement software was tested extensively using two selected viewpoints: the top-down viewpoint and the bird's eye viewpoint. On the basis of all findings in Chapter \ref{chapter:evaluation} the top-down viewpoint performed faster, and resulted in a successful load object measurement more often than the bird's eye viewpoint. From the bird's eye viewpoint, the load object measurement was especially difficult for locations near the opposite wall. In light of all findings and analysis the top-down camera view is recommended for load object measurement purposes in an overhead crane machine vision setup.

\subsection{Effects Of Selected Viewpoints}
\label{subsection:effect_of_selected_viewpoints}

The output AABB bounding volume from a top-down camera was usable as is, and only a little extra volume was induced by a slight 6 degree tilt of the sensor frame in tests. In general, the load object measurement output was consistent with the top-down camera configuration, and even though the ground plane was not always fully modeled due to the moving crane and changing lighting conditions, the measurement was not affected at all by such problems.

The bird's eye view suffered from increased distance to objects near the opposite wall causing smaller point responses for the load object (due to effectively smaller image area). Additionally, the bird's eye view suffered from inclusion of hoisting wires in the segmented load object, and merging of objects into the point response of the back wall. A lesser problem with the bird's eye viewpoint was the possibility of lifting the load object out of picture. In comparison, it was not possible to lift the load object out of the picture in a top-down camera configuration. Finally, there was less object occlusion in the top-down camera, because moving objects do not occlude the space between cameras and the load object.

In the overhead crane case, the decision to use a proximity based load object selection instead of a 2d image tracker was far from perfect, but in the end no information about the end effector location was required to select and measure the load object. It was guessed that a load object would be found near the center of the image, which was only approximately true for the top-down view datasets. For example, in some bird's eye viewpoint datasets, the load object can be seen in the lower part of the image far off the center. Because of such assumptions, often wrong objects were reported as the load object, which can be seen as clutter in the dataset visualisations.

\subsection{Overhead Crane Load Object Measurement Results}
\label{subsection:load_object_measurement_results}

The measurement values produced by the software could be compared with the actual dimensions only after the orientation of the data was taken into account. The computed AABB bounding volume is a function of the orientation of the camera frame relative to the world frame even though the load object actual volume does not change. In top-down camera overhead crane tests the axis-aligned bounding volume reported on average approximately 5-10\% larger values than the actual values of the dimensions, which is an acceptable result.

It was also deduced that a single measurement will not report the correct bounding volume, but it is a better idea to report the median or average bounding volume of a series of e.g. 10 to 15 measurements. Since the Himmeli platform can successfully deliver 4 frames per second without skipping frames, a load object measurement would currently take 3-5 seconds of time. The decision of how to interpret the constant flow of load object measurement data in the ROS network is left as responsibility of the software that subscribes to the provided data feed.

\section{Discussion On Forestry Crane Load Object Measurement}
\label{section:discussion_on_forestry_crane_load_object_measurement}

In the forestry machine crane case, the performance of the load object measurement system was tested using two viewpoints: a frog perspective viewpoint and a bird's eye viewpoint. On the basis of all findings in Chapter \ref{chapter:evaluation}, the bird's eye viewpoint may be better for measuring load objects in a forestry machine. The frog perspective viewpoint does not cover the sides of the working area very effectively, and the load object can be moved out of the picture. Moreover, the installation of the perception platform may not be possible because of the limited space in the base of the rotary boom crane, and on the vehicle chassis itself. Also the bird's eye viewpoint has its flaws: the crane boom is visible in the image and occludes the load object during normal operation. Since hoisting wires are not used to lift the load object, the segmented load object usually includes the crane boom that is visible in the image. This must be taken into account, and the cylindrical load object must be extracted from the data that contains parts of the crane boom. The bird's eye viewpoint has one major advantage over the frog perspective: if the forestry machine driver's booth has active vibration canceling technology, and the stereo cameras are mechanically attached to the vibration free rooftop of the booth, then the image will be much more stabilised than in a forestry machine chassis installation.

\subsection{Log Measurement Results}
\label{subsection:log_measurement_results}

Log measurement using simple cylinder fitting was implemented successfully, and the resulting cylinder extraction result can be seen in Figure \ref{fig:log_detected}. The orientation of the log was found approximately within a few degrees error when the load object was lifted high in the air, and within a 10 degree error window when the log was located near the ground and the data was cluttered because of other logs. There are no exact statistics on the extracted log orientation error, but the results can be reviewed in Figures \ref{fig:log_detected} and N. The position of the log object included a translation error in the direction of the cameras that was caused by the missing data from the back side of the log. The amount of erroneous translation towards the cameras was not measured. Both the translation error and the orientation error affect the output bounding volume measurement, but only a minor effect can be seen in the output load object measurement values.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{log_detected.jpg}
    \caption{Log object detected using data from BM algorithm in block matching (low quality data).}
    \label{fig:log_detected}
  \end{center}
\end{figure}

Better heuristic checks for correct load object selection are suggested as an improvement in the log object extraction and measurement processing. Currently, the correct log is extracted most of the time, and extraction of the cylindrical crane boom is prevented with simple constraints, such as, preventing selection of cylinders found above the end effector. Still, it is possible to select a wrong load object located below the end effector, for example, data generated by a pile of logs. Two checks that would improve the current cylinder selection significantly would be 

\begin{itemize}
\setlength{\itemsep}{0pt}
\item to check that the shortest distance from the end effector to the closest point along the cylinder main axis does not exceed a pre-defined threshold 
\item and to check that the distance between the centroid of the segmented cylinder and the end effector does not exceed a pre-defined threshold.
\end{itemize}

For example, if the distance between the candidate cylinder and the end effector is too large, then the candidate should be rejected. Improvements in the load selection algorithm are easy to implement, but due to limited time the checks were not further developed.

In Figure \ref{fig:log_detected}, the stereo parameters and object segmentation parameters were not optimally tuned. Still, the correct cylinder extraction was quite robust in the presence of missing data, noisy 3d data, and non-optimal parameter tuning. Especially the OpenCV stereo calibration that was described in Section \ref{section:stereo_calibration} did a good job on estimating parameters with little a priori information, which worked a lot towards robustness in the software.

The most significant problem in the log object measurement was the grouping of multiple objects in the load object point cloud. While the cylinder extraction does well on selecting and extracting the correct feature, it remains unknown whether the object segmentation step is needed at all in the forestry crane case since objects tend to group as a single large point cloud most of the time. The load object point cloud usually contains the crane boom, the end effector tool, and the picked up log. Sometimes, the load object point cloud additionally contained objects lying on the ground.

A minor problem with measuring logs was introduced when the bird's eye viewpoint was used and the logs were occluded by the crane boom. A log may be seen on both sides of the crane boom, which means that in the 3d data two point clouds are seen that represent partial surfaces of the load object. In these cases where the log object was occluded by the crane boom and divided into multiple point clouds, it was not possible to measure the load object size correctly. 

It could be assumed that the load objects are always found below the end effector tool, which would lead to removing the data points above the end effector before processing. While this would radically reduce the number of points and lead to faster computation of the measurement, it can be easily shown that in normal operation this assumption is not valid. The log is often tilted so that the other end of the log is located above the end effector tool. This was verified to be true during the forestry crane testing sessions. It was decided that in order to avoid accidental cutting or filtering of the load object feature data, the assumption of a load object located only below the end effector tool cannot be used. Consequently, only object centroid proximity information should be used to decide whether the selected load object is actually grabbed by the end effector tool or not.

There are no major improvement suggestions for the log object measurement process. One improvement option would be to implement a more complex cylinder structure that would better approximate the actual shape of the log object (in case a high detail log object model is needed). A model that approximates the log object with segments of cylinders is introduced in Forsman's thesis \citep{Forsman10}, which could be implemented. At this point, a simple cylinder with a linear axis is used, because a more complex log object model would significantly slow down the computation, and the effect of the accuracy of the model on the bounding volume is minimal.

\section{System Bottlenecks}
\label{section:system_bottlenecks}

Since the used stereo cameras can deliver images at a steady rate of 30 FPS, the perception platform software was one of the major bottlenecks with its capability of transmitting stereo images only at a rate of 4 FPS. Another bottleneck was detected in the client software object segmentation, which randomly takes a lot of time, almost a full second for a single measurement. Mostly, the time needed for processing 3d data into measurement information was less than 100 milliseconds per measurement.

Software operations could be sped up by making improvements in the point cloud data processing, or by removing visualisations from the load object measurement client. It was found out that updating the visualisations in the software requires most time, thus, removing visuals will speed up the point cloud processing when there is no need to use time for on-screen point cloud rendering. Also, the C++ implementation of the software could support more parallel programming and GPU implementations for selected point cloud computations. Finally, lower resolution cameras could be used, or the image ROI could be limited to only contain the image of the load object to reduce the amount of data, which speeds up computation.

\section{3d Geometry Verification}
\label{section:3d_geometry_verification}

3d data geometry that was output from the stereo camera process was assumed to be geometrically correct when a calibrated stereo system was used. This assumption may or may have not been true, which is why 3d data geometries of the measured load objects were verified. The verification was done simply by comparing the dimension measurements produced by the load object measurement software and the actual measures of the load object. This was not good enough to validate the geometry of the other parts of the modeled surrounding environment.

A suggested improvement for environment 3d modeling is to install 3d spheres as control points in the environment before data capture. Then, the reconstructed geometry can be verified as a function of distance from the cameras by comparing the computed locations of the spheres with their actual locations. If the 3d data locations of the control points do not match their actual locations, an unsuccessful camera calibration or a scale difference could be detected. Since the emphasis on the thesis was not on environment modeling, the surrounding geometry is good enough as long as the load objects report meaningful dimensions. As a conclusion, it was assumed that a correct calibration of the stereo cameras results in a correct geometry for the load objects.

\section{Perception Platform}
\label{section:perception_platform}

Using Himmeli perception platform as the medium for stereo camera processing was a successful decision. The platform provided ROS and PCL computing hardware at the ready, and the installed perception sensors were of high quality. The stereo cameras in Himmeli were in a converging camera installation whose property of negative disparity was used to remove hoisting wires from the 3d data in the overhead crane case with top-down viewpoint.

The sturdy metal frame protected the stereo cameras from moving or dislocating during platform installations and transport from test site to another, and the modularity was very useful in installing the cameras in tight spaces. The Himmeli platform was very heavy, which helped passively stabilise the camera images in vibrating environments, but made moving the platform a bit difficult.

The platform handled temperatures below zero well, and the system can be used even in winter conditions if the camera lens casings are installed with heaters and a solution that keeps the optics clear of water, dirt, and snow. In bright daylight, the camera image was very good.

The stereo camera image capture was limited at 4 FPS, because a higher rate of image capture, even 5 FPS resulted in skipping of frames, which lead to a situation where many frames with different timestamps had the same image content. If a dataset includes such inconsistent content in frame pairs, the stereo calibration fails, and 3d reconstruction assumptions do not hold anymore.  

The platform included a LIDAR scanner that was used to collect data for object segmentation testing, which was an invaluable asset in the work. The Velodyne scanner was also a plan B perception sensor if the stereo camera setup would have proved to be not suitable for load object measuring.

Finally, the perception platform hardware suffered from minor problems, such as one of the PC computers halted during heavy processor loads due to inadequate dissipation of heat from the processor. Another problem was encountered when the stereo cameras were initialised, because of problems in left and right camera assigning order. The Himmeli platform always assigned the first camera that finished initialisation as the left camera. In case the right camera finished initialisation first, it was still assigned as the left camera, which effectively swapped the left and the right camera image randomly at startup. Swapping of the left and the right image caused failure in 3d reconstruction with a calibrated stereo system. Since the initialisation order was random, the problem was resolved by checking the left and right camera content before any recording occurred. In case of swapped left and right cameras, the cameras were restarted. Currently, the only suggestion for hardware improvement is to install a larger heat sink in the overheating processor core. 

\section{Prototype Software}
\label{section:prototype_software}

A lot of time was used in development of the load object measurement software. The software structure followed a standard ROS package implementation, and most of the required computation algorithms were readily available as part of OpenCV and PCL software libraries. The software package ran some advanced algorithms, such as RANSAC computation and k-nearest neighbour search, but they were not visible for the user. It would have been possible to implement some portions of the software better than what is currently implemented, for example, an end effector tracker in the overhead crane case would have been very useful. Due to limited resources most computations use external libraries, and only some heuristics and point cloud projections were implemented by the author. 

The PCL library handled uninitialised data well, which resulted in robust functionality of the client software. Moreover, the software architecture did not depend on sensor frame orientation information, which is why the computation works independent of perception platform orientation.

While the software functionality was verified as functional in an actual overhead crane LAN network, the online load object measurement could not be tested since the data received from Himmeli platform was incorrect at the time of the test. A network that runs the load object measurement system can be difficult to configure without good knowledge of the ROS system, and Linux operating system. To make the usage of the software easier, launch files were written for offline and online testing purposes, and they accelerated the testing a lot by e.g. launching multiple ROS nodes simultaneously.

A software version was branched out that was used to log the load object measurement data in an XML file format for processing into visuals seen in Appendices. Visualisations of the data proved to be invaluable in evaluating whether the load object measurement system worked correctly or not.

%end of the work
% Next up : references
% Load the bibliographic references
% ------------------------------------------------------------------
% You can use several .bib files:
% \bibliography{thesis_sources,ietf_sources}
\bibliography{ref}


% Appendices go here
% ------------------------------------------------------------------
% If you do not have appendices, comment out the following lines
%\appendix
% \input{appendices.tex}

%\chapter{First appendix}
%\label{chapter:first-appendix}

%This is the first appendix. You could put some test images or verbose data in an
%appendix, if there is too much data to fit in the actual text nicely.

%For now, the Aalto logo variants are shown in Figure~\ref{fig:aaltologo}.


%\begin{figure}
%\begin{center}
%\subfigure[In English]{\includegraphics[width=.8\textwidth]{aalto-logo-en}}
%\subfigure[Suomeksi]{\includegraphics[width=.8\textwidth]{aalto-logo-fi}}
%\subfigure[Pä svenska]{\includegraphics[width=.8\textwidth]{aalto-logo-se}}
%\caption{Aalto logo variants}
%\label{fig:aaltologo}
%\end{center}
%\end{figure}


% End of document!
% ------------------------------------------------------------------
% The LastPage package automatically places a label on the last page.
% That works better than placing a label here manually, because the
% label might not go to the actual last page, if LaTeX needs to place
% floats (that is, figures, tables, and such) to the end of the
% document.
\end{document}
