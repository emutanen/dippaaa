% Lines starting with a percent sign (%) are comments. LaTeX will
% not process those lines. Similarly, everything after a percent
% sign in a line is considered a comment. To produce a percent sign
% in the output, write \% (backslash followed by the percent sign).
% ==================================================================
% Usage instructions:
% ------------------------------------------------------------------
% The file is heavily commented so that you know what the various
% commands do. Feel free to remove any comments you don't need from
% your own copy. When redistributing the example thesis file, please
% retain all the comments for the benefit of other thesis writers!
% ==================================================================
% Compilation instructions:
% ------------------------------------------------------------------
% Use pdflatex to compile! Input images are expected as PDF files.
% Example compilation:
% ------------------------------------------------------------------
% > pdflatex thesis-example.tex
% > bibtex thesis-example
% > pdflatex thesis-example.tex
% > pdflatex thesis-example.tex
% ------------------------------------------------------------------
% You need to run pdflatex multiple times so that all the cross-references
% are fixed. pdflatex will tell you if you need to re-run it (a warning
% will be issued)
% ------------------------------------------------------------------
% Compilation has been tested to work in ukk.cs.hut.fi and kosh.hut.fi
% - if you have problems of missing .sty -files, then the local LaTeX
% environment does not have all the required packages installed.
% For example, when compiling in vipunen.hut.fi, you get an error that
% tikz.sty is missing - in this case you must either compile somewhere
% else, or you cannot use TikZ graphics in your thesis and must therefore
% remove or comment out the tikz package and all the tikz definitions.
% ------------------------------------------------------------------

% General information
% ==================================================================
% Package documentation:
%
% The comments often refer to package documentation. (Almost) all LaTeX
% packages have documentation accompanying them, so you can read the
% package documentation for further information. When a package 'xxx' is
% installed to your local LaTeX environment (the document compiles
% when you have \usepackage{xxx} and LaTeX does not complain), you can
% find the documentation somewhere in the local LaTeX texmf directory
% hierarchy. In ukk.cs.hut.fi, this is /usr/texlive/2008/texmf-dist,
% and the documentation for the titlesec package (for example) can be
% found at /usr/texlive/2008/texmf-dist/doc/latex/titlesec/titlesec.pdf.
% Most often the documentation is located as a PDF file in
% /usr/texlive/2008/texmf-dist/doc/latex/xxx, where xxx is the package name;
% however, documentation for TikZ is in
% /usr/texlive/2008/texmf-dist/doc/latex/generic/pgf/pgfmanual.pdf
% (this is because TikZ is a front-end for PGF, which is meant to be a
% generic portable graphics format for LaTeX).
% You can try to look for the package manual using the ``find'' shell
% command in Linux machines; the find databases are up-to-date at least
% in ukk.cs.hut.fi. Just type ``find xxx'', where xxx is the package
% name, and you should find a documentation file.
% Note that in some packages, the documentation is in the DVI file
% format. In this case, you can copy the DVI file to your home directory,
% and convert it to PDF with the dvipdfm command (or you can read the
% DVI file directly with a DVI viewer).
%
% If you can't find the documentation for a package, just try Googling
% for ``latex packagename''; most often you can get a direct link to the
% package manual in PDF format.
% ------------------------------------------------------------------


% Document class for the thesis is report
% ------------------------------------------------------------------
% You can change this but do so at your own risk - it may break other things.
% Note that the option pdftext is used for pdflatex; there is no
% pdflatex option.
% ------------------------------------------------------------------
\documentclass[12pt,a4paper,oneside,pdftex]{report}

% The input files (tex files) are encoded with the latin-1 encoding
% (ISO-8859-1 works). Change the latin1-option if you use UTF8
% (at some point LaTeX did not work with UTF8, but I'm not sure
% what the current situation is)
\usepackage[utf8]{inputenc}
% OT1 font encoding seems to work better than T1. Check the rendered
% PDF file to see if the fonts are encoded properly as vectors (instead
% of rendered bitmaps). You can do this by zooming very close to any letter
% - if the letter is shown pixelated, you should change this setting
% (try commenting out the entire line, for example).
\usepackage[OT1]{fontenc} %fontenc first, then inputenc if need be
% The babel package provides hyphenating instructions for LaTeX. Give
% the languages you wish to use in your thesis as options to the babel
% package (as shown below). You can remove any language you are not
% going to use.
% Examples of valid language codes: english (or USenglish), british,
% finnish, swedish; and so on.
\usepackage[british]{babel}


% Font selection
% ------------------------------------------------------------------
% The default LaTeX font is a very good font for rendering your
% thesis. It is a very professional font, which will always be
% accepted.
% If you, however, wish to spicen up your thesis, you can try out
% these font variants by uncommenting one of the following lines
% (or by finding another font package). The fonts shown here are
% all fonts that you could use in your thesis (not too silly).
% Changing the font causes the layouts to shift a bit; you many
% need to manually adjust some layouts. Check the warning messages
% LaTeX gives you.
% ------------------------------------------------------------------
% To find another font, check out the font catalogue from
% http://www.tug.dk/FontCatalogue/mathfonts.html
% This link points to the list of fonts that support maths, but
% that's a fairly important point for master's theses.
% ------------------------------------------------------------------
% <rant>
% Remember, there is no excuse to use Comic Sans, ever, in any
% situation! (Well, maybe in speech bubbles in comics, but there
% are better options for those too)
% </rant>

% \usepackage{palatino}
% \usepackage{tgpagella}



% Optional packages
% ------------------------------------------------------------------
% Select those packages that you need for your thesis. You may delete
% or comment the rest.

% Natbib allows you to select the format of the bibliography references.
% The first example uses numbered citations:
\usepackage[square,sort&compress,numbers]{natbib}
% The second example uses author-year citations.
% If you use author-year citations, change the bibliography style (below);
% acm style does not work with author-year citations.
% Also, you should use \citet (cite in text) when you wish to refer
% to the author directly (\citet{blaablaa} said blaa blaa), and
% \citep when you wish to refer similarly than with numbered citations
% (It has been said that blaa blaa~\citep{blaablaa}).
% \usepackage[square]{natbib}

% The alltt package provides an all-teletype environment that acts
% like verbatim but you can use LaTeX commands in it. Uncomment if
% you want to use this environment.
% \usepackage{alltt}

% The eurosym package provides a euro symbol. Use with \euro{}
\usepackage{eurosym}

% Verbatim provides a standard teletype environment that renderes
% the text exactly as written in the tex file. Useful for code
% snippets (although you can also use the listings package to get
% automatic code formatting).
\usepackage{verbatim}

% The listing package provides automatic code formatting utilities
% so that you can copy-paste code examples and have them rendered
% nicely. See the package documentation for details.
% \usepackage{listings}

% The fancuvrb package provides fancier verbatim environments
% (you can, for example, put borders around the verbatim text area
% and so on). See package for details.
% \usepackage{fancyvrb}

% Supertabular provides a tabular environment that can span multiple
% pages.
%\usepackage{supertabular}
% Longtable provides a tabular environment that can span multiple
% pages. This is used in the example acronyms file.
\usepackage{longtable}

% The fancyhdr package allows you to set your the page headers
% manually, and allows you to add separator lines and so on.
% Check the package documentation.
% \usepackage{fancyhdr}

% Subfigure package allows you to use subfigures (i.e. many subfigures
% within one figure environment). These can have different labels and
% they are numbered automatically. Check the package documentation.
\usepackage{subfigure}

% The titlesec package can be used to alter the look of the titles
% of sections, chapters, and so on. This example uses the ``medium''
% package option which sets the titles to a medium size, making them
% a bit smaller than what is the default. You can fine-tune the
% title fonts and sizes by using the package options. See the package
% documentation.
\usepackage[medium]{titlesec}

% The TikZ package allows you to create professional technical figures.
% The learning curve is quite steep, but it is definitely worth it if
% you wish to have really good-looking technical figures.
\usepackage{tikz}
% You also need to specify which TikZ libraries you use
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.pathmorphing,decorations.markings}
\usetikzlibrary{shapes}
\usetikzlibrary{patterns}


% The aalto-thesis package provides typesetting instructions for the
% standard master's thesis parts (abstracts, front page, and so on)
% Load this package second-to-last, just before the hyperref package.
% Options that you can use:
%   mydraft - renders the thesis in draft mode.
%             Do not use for the final version.
%   doublenumbering - [optional] number the first pages of the thesis
%                     with roman numerals (i, ii, iii, ...); and start
%                     arabic numbering (1, 2, 3, ...) only on the
%                     first page of the first chapter
%   twoinstructors  - changes the title of instructors to plural form
%   twosupervisors  - changes the title of supervisors to plural form
%\usepackage[mydraft,twosupervisors]{aalto-thesis}
%usepackage[mydraft,doublenumbering]{aalto-thesis}
\usepackage{aalto-thesis}


% Hyperref
% ------------------------------------------------------------------
% Hyperref creates links from URLs, for references, and creates a
% TOC in the PDF file.
% This package must be the last one you include, because it has
% compatibility issues with many other packages and it fixes
% those issues when it is loaded.
\RequirePackage[pdftex]{hyperref}
% Setup hyperref so that links are clickable but do not look
% different
\hypersetup{colorlinks=false,raiselinks=false,breaklinks=true}
\hypersetup{pdfborder={0 0 0}}
\hypersetup{bookmarksnumbered=true}
% The following line suggests the PDF reader that it should show the
% first level of bookmarks opened in the hierarchical bookmark view.
\hypersetup{bookmarksopen=true,bookmarksopenlevel=1}
% Hyperref can also set up the PDF metadata fields. These are
% set a bit later on, after the thesis setup.


% Thesis setup
% ==================================================================
% Change these to fit your own thesis.
% \COMMAND always refers to the English version;
% \FCOMMAND refers to the Finnish version; and
% \SCOMMAND refers to the Swedish version.
% You may comment/remove those language variants that you do not use
% (but then you must not include the abstracts for that language)
% ------------------------------------------------------------------
% If you do not find the command for a text that is shown in the cover page or
% in the abstract texts, check the aalto-thesis.sty file and locate the text
% from there.
% All the texts are configured in language-specific blocks (lots of commands
% that look like this: \renewcommand{\ATCITY}{Espoo}.
% You can just fix the texts there. Just remember to check all the language
% variants you use (they are all there in the same place).
% ------------------------------------------------------------------
\newcommand{\TITLE}{Crane Load Position and Orientation Measurement using Machine Vision}
\newcommand{\FTITLE}{Taakan paikan ja asennon mittaus nosturista konenäköavusteisesti}
\newcommand{\DATE}{June 18, 2011}
\newcommand{\FDATE}{18. kesäkuuta 2011}
\newcommand{\SDATE}{Den 18 Juni 2011}

% Supervisors and instructors
% ------------------------------------------------------------------
% If you have two supervisors, write both names here, separate them with a
% double-backslash (see below for an example)
% Also remember to add the package option ``twosupervisors'' or
% ``twoinstructors'' to the aalto-thesis package so that the titles are in
% plural.
% Example of one supervisor:
\newcommand{\SUPERVISOR}{Professor Ville Kyrki}
\newcommand{\FSUPERVISOR}{Professori Ville Kyrki}
\newcommand{\SSUPERVISOR}{Professor Ville Kyrki}
% Example of twosupervisors:
%\newcommand{\SUPERVISOR}{Professor Ville Kyrki\\
%  D.Sc. Sami Terho}
%\newcommand{\FSUPERVISOR}{Professori Ville Kyrki\\
%  TkT Sami Terho}
%\newcommand{\SSUPERVISOR}{Professor Ville Kyrki\\
%  Dr. Sami Terho}

% If you have only one instructor, just write one name here
\newcommand{\INSTRUCTOR}{Sami Terho D.Sc. (Tech.)}
\newcommand{\FINSTRUCTOR}{Tekniikan tohtori Sami Terho}
\newcommand{\SINSTRUCTOR}{Teknologie doktor Sami Terho}
% If you have two instructors, separate them with \\ to create linefeeds
% \newcommand{\INSTRUCTOR}{Olli Ohjaaja M.Sc. (Tech.)\\
%  Elli Opas M.Sc. (Tech)}
%\newcommand{\FINSTRUCTOR}{Diplomi-insinööri Olli Ohjaaja\\
%  Diplomi-insinööri Elli Opas}
%\newcommand{\SINSTRUCTOR}{Diplomingenjör Olli Ohjaaja\\
%  Diplomingenjör Elli Opas}

% If you have two supervisors, it is common to write the schools
% of the supervisors in the cover page. If the following command is defined,
% then the supervisor names shown here are printed in the cover page. Otherwise,
% the supervisor names defined above are used.
\newcommand{\COVERSUPERVISOR}{Professor Ville Kyrki, Aalto University, School of Electrical Engineering}
% The same option is for the instructors, if you have multiple instructors.
% \newcommand{\COVERINSTRUCTOR}{Olli Ohjaaja M.Sc. (Tech.), Aalto University\\
%  Elli Opas M.Sc. (Tech), Aalto SCI}

% Other stuff
% ------------------------------------------------------------------
\newcommand{\PROFESSORSHIP}{Intelligent Robotics}
\newcommand{\FPROFESSORSHIP}{Älykäs Robotiikka}
\newcommand{\SPROFESSORSHIP}{Intelligent Robotteknik}
% Professorship code is the same in all languages
\newcommand{\PROFCODE}{T-110}
\newcommand{\KEYWORDS}{stereo vision, depth mapping, crane load}
\newcommand{\FKEYWORDS}{stereonäkö, syvyyskartoitus, nosturin taakka}
\newcommand{\SKEYWORDS}{stereoseende, djuphet kartläggning, kran last}
\newcommand{\LANGUAGE}{English}
\newcommand{\FLANGUAGE}{Englanti}
\newcommand{\SLANGUAGE}{Engelska}
% Author is the same for all languages
\newcommand{\AUTHOR}{Erkka Mutanen}

% Currently the English versions are used for the PDF file metadata
% Set the PDF title
\hypersetup{pdftitle={\TITLE\ \AUTHOR}}
% Set the PDF author
\hypersetup{pdfauthor={\AUTHOR}}
% Set the PDF keywords
\hypersetup{pdfkeywords={\KEYWORDS}}
% Set the PDF subject
\hypersetup{pdfsubject={Master's Thesis}}

% Layout settings
% ------------------------------------------------------------------

% When you write in English, you should use the standard LaTeX
% paragraph formatting: paragraphs are indented, and there is no
% space between paragraphs.
% When writing in Finnish, we often use no indentation in the
% beginning of the paragraph, and there is some space between the
% paragraphs.

% If you write your thesis Finnish, uncomment these lines; if
% you write in English, leave these lines commented!
% \setlength{\parindent}{0pt}
% \setlength{\parskip}{1ex}

% Use this to control how much space there is between each line of text.
% 1 is normal (no extra space), 1.3 is about one-half more space, and
% 1.6 is about double line spacing.
\linespread{1.1} % This is the default
% \linespread{1.3}

% Bibliography style
% acm style gives you a basic reference style. It works only with numbered
% references.
\bibliographystyle{acm}
% Plainnat is a plain style that works with both numbered and name citations.
% \bibliographystyle{plainnat}


% Extra hyphenation settings
% ------------------------------------------------------------------
% You can list here all the files that are not hyphenated correctly.
% You can provide many \hyphenation commands and/or separate each word
% with a space inside a single command. Put hyphens in the places where
% a word can be hyphenated.
% Note that (by default) LaTeX will not hyphenate words that already
% have a hyphen in them (for example, if you write ``structure-modification
% operation'', the word structure-modification will never be hyphenated).
% You need a special package to hyphenate those words.
\hyphenation{di-gi-taa-li-sta yksi-suun-tai-sta}

% The preamble ends here, and the document begins.
% Place all formatting commands and such before this line.
% ------------------------------------------------------------------
\begin{document}
% This command adds a PDF bookmark to the cover page. You may leave
% it out if you don't like it...
\pdfbookmark[0]{Cover page}{bookmark.0.cover}
% This command is defined in aalto-thesis.sty. It controls the page
% numbering based on whether the doublenumbering option is specified
\startcoverpage

% Cover page
% ------------------------------------------------------------------
% Options: finnish, english, and swedish
% These control in which language the cover-page information is shown
\coverpage{english}
% Abstracts
% ------------------------------------------------------------------
% Include an abstract in the language that the thesis is written in,
% and if your native language is Finnish or Swedish, one in that language.

% Abstract in English
% ------------------------------------------------------------------
\thesisabstract{english}{
The purpose of this diploma thesis is to introduce a crane load position and orientation measurement using machine vision.
The measurement should describe the position and orientation of an arbitrary load lifted with a process crane or a small telescopic crane within required accuracy limits.
}

%BADThis thesis will primarily study a machine vision approach that uses stereo vision for the work environment
%epth mapping. Other optional sensors, such as a laser scanner, are considered.

%BADExisting machine vision approaches will be applied to the crane load problem. 
%The location measurement of the crane load could enable additional improvements in obstable avoidance while
%operating the crane.}

%\fixme{Abstract text goes here (and this is an example how to use fixme).}
%Fixme is a command that helps you identify parts of your thesis that still
%require some work. When compiled in the custom \texttt{mydraft} mode, text
%parts tagged with fixmes are shown in bold and with fixme tags around them. When
%compiled in normal mode, the fixme-tagged text is shown normally (without
%special formatting). The draft mode also causes the ``Draft'' text to appear on
%the front page, alongside with the document compilation date. The custom
%\texttt{mydraft} mode is selected by the \texttt{mydraft} option given for the
%package \texttt{aalto-thesis}, near the top of the \texttt{thesis-example.tex}
%file.

%The thesis example file (\texttt{thesis-example.tex}), all the chapter content
%files (\texttt{1introduction.tex} and so on), and the Aalto style file
%(\texttt{aalto-thesis.sty}) are commented with explanations on how the Aalto
%thesis works. The files also contain some examples on how to customize various
%details of the thesis layout, and of course the example text works as an
%example in itself. Please read the comments and the example text; that should
%get you well on your way!}

% Abstract in Finnish
% ------------------------------------------------------------------
\thesisabstract{finnish}{

Kivi on materiaali, joka muodostuu mineraaleista ja luokitellaan
mineraalisisältönsä mukaan. Kivet luokitellaan yleensä ne muodostaneiden
prosessien mukaan magmakiviin, sedimenttikiviin ja metamorfisiin kiviin.
Magmakivet ovat muodostuneet kiteytyneestä magmasta, sedimenttikivet vanhempien
kivilajien rapautuessa ja muodostaessa iskostuneita yhdisteitä, metamorfiset
kivet taas kun magma- ja sedimenttikivet joutuvat syvällä maan kuoressa
lämpötilan ja kovan paineen alaiseksi.

Kivi on epäorgaaninen eli elottoman luonnon aine, mikä tarkoittaa ettei se
sisällä hiiltä tai muita elollisen orgaanisen luonnon aineita. Niinpä kivestä
tehdyt esineet säilyvät maaperässä tuhansien vuosien ajan mätänemättä. Kun
orgaaninen materiaali jättää jälkensä kiveen, tulos tunnetaan nimellä fossiili.

Suomen peruskallio on suurimmaksi osaksi graniittia, gneissiä ja
Kaakkois-Suomessa rapakiveä.

Kiveä käytetään teollisuudessa moniin eri tarkoituksiin, kuten keittiötasoihin.
Kivi on materiaalina kalliimpaa mutta kestävämpää kuin esimerkiksi puu.
}
% Abstract in Swedish
% ------------------------------------------------------------------
%\thesisabstract{swedish}{
%illa Vargens universum är det tredje fiktiva universumet inom huvudfäran av de
%ecknade disneyserierna - de övriga två är Kalle Ankas och Musse Piggs
%niversum. Figurerna runt Lilla Vargen kommer huvudsakligen frän tre källor ---
%els persongalleriet i kortfilmen Tre smä grisar frän 1933 och dess uppföljare,
%dels längfilmen Sängen om Södern frän 1946, och dels frän episoden ``Bongo'' i
%ängfilmen Pank och fägelfri frän 1947. Framför allt de två första har
%sedermera även kommit att leva vidare, utvidgas och införlivas i varandra genom
%tecknade serier, främst sädana producerade av Western Publishing för
%amerikanska Disneytidningar under ären 1945--1984.

%Världen runt Lilla Vargen är, i jämförelse med den runt Kalle Anka eller Musse
%Pigg, inte helt enhetlig, vilket bland annat märks i Bror Björns skiftande
%personlighet. Den har även varit betydligt mer öppen för influenser frän andra
%Disneyvärldar, inte minst de tecknade längfilmerna. Ytterligare en skillnad är
%tt varelserna i vargserierna förefaller stä närmare sina förebilder inom den
%verkliga djurvärlden. Att vargen Zeke vill äta upp grisen Bror Duktig är till
%exempel ett ständigt äterkommande tema, men om katten Svarte Petter skulle fä
%för sig att äta upp musen Musse Pigg skulle detta antagligen höja ett och annat
%ögonbryn bland läsarna.}


% Acknowledgements
% ------------------------------------------------------------------
% Select the language you use in your acknowledgements
\selectlanguage{english}

% Uncomment this line if you wish acknoledgements to appear in the
% table of contents
%\addcontentsline{toc}{chapter}{Acknowledgements}

% The star means that the chapter isn't numbered and does not
% show up in the TOC
\chapter*{Acknowledgements}

I wish to thank all students who use \LaTeX\ for formatting their theses,
because theses formatted with \LaTeX\ are just so nice.

Thank you, and keep up the good work!
\vskip 10mm

\noindent Espoo, \DATE
\vskip 5mm
\noindent\AUTHOR

% Table of contents
% ------------------------------------------------------------------
\cleardoublepage
% This command adds a PDF bookmark that links to the contents.
% You can use \addcontentsline{} as well, but that also adds contents
% entry to the table of contents, which is kind of redundant.
% The text ``Contents'' is shown in the PDF bookmark.
\pdfbookmark[0]{Contents}{bookmark.0.contents}
\tableofcontents

% Abbreviations & Acronyms
% ------------------------------------------------------------------
% Use \cleardoublepage so that IF two-sided printing is used
% (which is not often for masters theses), then the pages will still
% start correctly on the right-hand side.
\cleardoublepage
% Example acronyms are placed in a separate file, acronyms.tex
% \input{acronyms}

\addcontentsline{toc}{chapter}{Abbreviations and Acronyms}
\chapter*{Abbreviations and Acronyms}

% The longtable environment should break the table properly to multiple pages,
% if needed

\noindent
\begin{longtable}{@{}p{0.25\textwidth}p{0.7\textwidth}@{}}
AABB & Axis-aligned bounding-box \\
AI & Artificial Intelligence \\
BM & Block Matching \\
BVH & Bounding Volume Hierarchies \\
CCIR & Comité Consultatif International pour la Radio \\
CMOS & Complementary Metal Oxide Semiconductor \\
CUDA & Compute Unified Device Architecture \\
FLIR & Forward Looking Infra Red \\
GPU & Graphics Processing Unit \\
HDR & High Dynamic Range \\
I/O & Input Output\\
LIDAR & Light Detecting And Ranging \\
OBB & Oriented Bounding Box \\
PC & Personal Computer \\
PCL & Point Cloud Library \\
PLC & Programmable Logic Controller \\
RADAR & Radio Detecting And Ranging \\
ROI & Region Of Interest \\
ROS & Robot Operating System \\
SAD & Sum of Absolute Differences \\
SGBM & Semi-Global Block Matching \\
SIMD & Single Instruction, Multiple Data \\
SOM & Self-organizing Map \\
SONAR & Sound Navigation And Ranging \\
SSD & Sum of Squared Differences \\
SSE & Streaming SIMD Extensions \\
TLD & Tracking, Detection, And Learning \\
TOF & Time Of Flight \\
UCC & Use Case Configuration \\
WYSIWYG & What You See Is What You Get \\
XML & Extensive Markup Language \\
XML-RPC & XML Remote Procedure Call \\ 
\end{longtable}

% List of tables
% ------------------------------------------------------------------
% You only need a list of tables for your thesis if you have very
% many tables. If you do, uncomment the following two lines.
% \cleardoublepage
% \listoftables

% Table of figures
% ------------------------------------------------------------------
% You only need a list of figures for your thesis if you have very
% many figures. If you do, uncomment the following two lines.
% \cleardoublepage
% \listoffigures

% The following label is used for counting the prelude pages
\label{pages-prelude}
\cleardoublepage

%%%%%%%%%%%%%%%%% The main content starts here %%%%%%%%%%%%%%%%%%%%%
% ------------------------------------------------------------------
% This command is defined in aalto-thesis.sty. It controls the page
% numbering based on whether the doublenumbering option is specified
\startfirstchapter

% Add headings to pages (the chapter title is shown)
\pagestyle{headings}

% The contents of the thesis are separated to their own files.
% Edit the content in these files, rename them as necessary.
% ------------------------------------------------------------------

% \input{1introduction.tex}


%===========================================================================================================
\chapter*{Notation and Symbols}

%\footnote{http://owl.english.purdue.edu/owl/resource/574/02/} of
%Purdue University or Strunk's Elements of
%Style\footnote{http://www.bartleby.com/141/}. Remember that footnotes

%\emph{cite}.

% An example of a traditional LaTeX table
% ------------------------------------------------------------------
% A note on underfull/overfull table cells and tables:
% ------------------------------------------------------------------
% In professional typography, the width of the text in a page is always a lot
% less than the width of the page. If you are accustomed to the (too wide) text
% areas used in Microsoft Word's standard documents, the width of the text in
% this thesis layout may suprise you. However, text in a book needs wide
% margins. Narrow text is easier to read and looks nicer. Longer lines are
% hard to read, because the start of the next line is harder to locate when
% moving from line to the next.
% However, tables that are in the middle of the text often would require a wider
% area. By default, LaTeX will complain if you create too wide tables with
% ``overfull'' error messages, and the table will not be positioned properly
% (not centered). If at all possible, try to make the table narrow enough so
% that it fits to the same space as the text (total width = \textwidth).
% If you do need more space, you can either
% 1) ignore the LaTeX warnings
% 2) use the textpos-package to manually position the table (read the package
%    documentation)
% 3) if you have the table as a PDF document (of correct size, A4), you can use
%    the pdfpages package to include the page. This overrides the margin
%    settings for this page and LaTeX will not complain.
% ------------------------------------------------------------------
% Another note:
% ------------------------------------------------------------------
% If your table fits to \textwidth, but the cells are so narrow that the text
% in p{..}-formatted cells does not flow nicely (you get underfull warnings
% because LaTeX tries to justify the text in the cells) you can manually set
% the text to unjustified by using the \raggedright command for each cell
% that you do not want to be justified (see the example below). \raggedleft
% is also possible, of course...
% ------------------------------------------------------------------
% If you need to have linefeeds (\\) inside a cell, you must create a new
% paragraph-formatting environment inside the cell. Most common ones are
% the minipage-environment and the \parbox command (see LaTeX documentation
% for details; or just google for ``LaTeX minipage'' and ``LaTeX parbox'').
% Alignment of sells: l=left, c=center, r=right.
% If you want wrapping lines, use p{width} exact cell widths.
% If you want vertical lines between columns, write | above between the letters
% Horizontal lines are generated with the \hline command:

% Place a & between the columns
% In the end of the line, use two backslashes \\ to break the line,
% then place a \hline to make a horizontal line below the row

%\begin{table}
%\begin{tabular}{|p{2cm}|p{3.8cm}|p{4.5cm}|p{1.1cm}|}
%\hline % The line on top of the table
%\textbf{Code} & \textbf{Name} & \textbf{Methods} & \textbf{Area} \\
%\hline
%T-110.6130 & Systems Engineering for Data Communications
%    Software & \raggedright Computer simulations, mathematical modeling,
%  experimental research, data analysis, and network service business
%  research methods, (agile method) & T-110 \\
%\hline
%\multicolumn{2}{|p{6.25cm}|}{Mat-2.3170 Simulation (here is an example of
% multicolumn for tables)}& Details of how to build simulations & T-110 \\
%\hline
%S-38.3184 & Network Traffic Measurements and Analysis
%& \raggedright How to measure and analyse network
%  traffic & T-110 \\ \hline
% \end{tabular} % for really simple tables, you can just use tabular
% You can place the caption either below (like here) or above the table
%\caption{Research methodology courses}
% Place the label just after the caption to make the link work
%\label{table:courses}
%\end{table} % table makes a floating object with a title


%The multicolumn command takes the following 3 arguments:
% the number of cells to merge, the cell formatting for the new cell, and the
% contents of the cell

\chapter{Introduction}
\label{chapter:introduction}
%Ongelman esittely, johdanto. Pituus noin 5 sivua.

%The art of lifting heavy objects has introduced a multitude of problems for the mankind since the beginning of times. In history, the first written mention of a hoisting mechanism is from year 530 B.C., when the temple of Artemis was being designed in Ephesus, in the present-day Turkey by the Greeks. The Greek temple was one of the seven wonders of the ancient world, and it was built using an early construction crane mechanism to hoist the heavy pillars and the roofing in their places. \ref{Zrni04}

In the modern world, goods are being moved more than ever, which brings challenges to keeping the goods handling easy and cost-efficient. Modern day goods are mostly transported from one location to another in a shipping container, such as a standard dry storage unit, or possibly a tank or a cylindrical drum depending on the actual product and the delivery distance. 
Ideas of a modern container cargo handling process started only after the second half of the 20th century. Before that the only known way of transport of goods was sea cargo packed in barrels, sacks and pallets. It was cumbersome to load and unload a ship filled with such small units - it was not unusual that ships spent more time in docks than at sea because of inefficient manhandling of small cargo.\ref{Zrni04}\par

Today, the business of moving and lifting goods is getting more and more automated as new crane systems are being developed to meet the ever increasing needs for speed, safety, and energy efficiency. Even though the cargo handling process is quite automated, a container filled with goods still has to be unloaded using manual labour, potentially halting the container in its place for a day, which is expensive. Sometimes items that need to be unloaded may arrive in a truck or a van, but nevertheless, they need to be lifted and moved to a secure storage location in this case with an overhead crane. The operators who unload shipments use the crane to move the items that arrive at the industrial site in their correct places. Speed and safety affect the productivity of the event of unloading, and with ever lowering computing prices a lot of research is going on to find ways how added programming and sensors could benefit the crane operator for increased performance.\par

While it is possible to find examples of fast, automated, optimized, profitable, and safe factory manufacturing workflows, many industrial ventures still rely on manual movement of a product on a factory floor. Such manual operation can suffer from non-optimized route selection, operator fatigue, damage to goods in collisions, safety issues for other workers, and other problems due human error. Some industries are not even profitable unless a certain speed of moving goods safely is reached, which applies for e.g. forestry log picking and tree felling work. To improve things, a real-time description of the load object and its environment could be available, so that it would be possible to prevent unwanted events using the installed crane systems with some perception sensors and software.

\section{Incentives For Measuring Industrial Goods}
\label{section:incentives_for_measuring_industrial_goods}

Industrial goods, such as beams, engines, large coils, drums of dangerous goods, and pallets of stacked items are expensive, often unique one-time orders that can cause damage if in collision with other objects, which is why an automatic route planner that avoids collisions is an interesting technical feature. Collision avoidance technology should be automatic, and it should keep the lifted goods intact, and warn the operator of imminent collisions, and possibly engage safety features when needed. New overhead crane systems do not have a collision detection feature, but if the load object is carefully measured, its location and orientation is known at all times, and also a model of the environment is known, then a collision computation can be used to detect future collisions before they happen.\par

The overall approach of accurately knowing where things move, and what size they are, and in what orientation they are can bring this kind of described safety feature for the remote operator's disposal. Even with currently used wireless remote controllers, anti-sway systems, and other improvements installed in the crane systems there is still room for improvement in safety of the crane working area. It is definitely possible to crash to a wall or in an unfortunate event, to other working people with a 3-ton metal case, or even heavier objects, which in the case of collision can cause structural damage to buildings, break goods, or injure co-workers.

Naturally, it is not possible to measure the location of the object all the time, but if an understanding of the model of the object, and its qualities is formed, then a request of the state of the model can be polled from a computer system at times when the model state is needed for decision making. With such information, tracks of objects can continuosly provide information about object's location, dimensions, and moving speeds. With the model information it is possible to add new features in an environment where they never previously existed. The added value is potentially good, which is why the measurement of the load object is worth researching.

Measuring goods while they are being moved and processed at the industrial site is also the very basis for all quality control in manufacturing industries: for example, radio frequency identification tags are used to identify and control a batch of products in bulk manufacturing, video systems measure process quality in a paper factory, and a spectral analyzer measures the concentration of foreign substances in a soda bottle washing process. In crane environment, the perception system can add quality control in the form of recordings of states of the environment, video feeds of accidents, and teaching material for new users as recorded movements. 

Load object measurement could also be used for new purposes, such as security access surveillance when the factory is shut down, or as an additional statistics data source. Size and weight statistics can help track the crane usage and lead to improvements in warehouse layout, or help a factory manager decide upon investing in a larger manufacturing floor if the capacity is at a limit.

 The biggest non-safety related features are the optimized route planning (using knowledge of geometric dimensions) and automatic moving of loads using a computer planner. During the life cycle of the crane installation such features can reduce vast amounts of used energy, possibly increase movement speeds, and sometimes increase service intervals. Optimized route planning has become available in modern process crane PLC devices, which means that it is in the best case scenario possible to test route planning right away after implementing a dimensions measurement in the research phase.

\section{Overhead Cranes}
\label{section:overhead_cranes}
% big picture
The focus of this work is on measuring the dimensions of industrial goods being lifted and moved in an industrial site using an overhead crane. A glimpse in the not so distant history reveals a development of the overhead crane, a hoisting system more commonly known as the bridge crane or the process crane. First mass manufactured overhead crane systems were commissioned in 1876 in England, and they were powered by steam engines in the beginning of the industrial revolution. Today, 138 years later the overhead crane is widely used in different industries for efficient process lifting and service duties using electricity. The development of the electric overhead traveling crane (EOT) has introduced the modern overhead crane as we know it. It is electrically powered and often used in heavy process industry, and in custom industrial manufacturing.\par
%structure of a single overhead crane
    Overhead cranes have a beam-like bridge installed between two rails. The bridge spans the gap between the rails and travels from one end of the rail to the other. Effectively, a polyhedron shaped crane working area (portal of the crane) is formed underneath the rails encompassing most space between a factory floor and the height of the installed crane. It can be easily deduced that an overhead crane has 3 degrees of freedom (DOF). The traveling bridge produces the first DOF (X axis), and the second DOF (Y axis) comes from the moving trolley mechanism that moves perpendicular to the traveling bridge direction. The third final degree of freedom comes from the hoisting mechanism that covers the Z axis direction movement of the end effector tool. Usually, a control pendant, or a wireless hand-held controller is used to move the load object to a desired location in the 3d crane working space. \par
% medium picture, applications
    Overhead cranes are used for medium to heavy lifting in industrial manufacturing and process industries. For example, in steel mills, heavy lifting is needed when cylinders of molten steel or other metal products are carried to the next process station. In heavy industry, the capacity of the crane and standard compliance is important, and the crane must operate in hazardous environments reliably throughout the day. Then again, in industrial manufacturing, the speed of the moving bridge, energy efficiency, and safety features are more important, but the crane is not operated all day but for certain periods of time. For all applications, the lifting operations are similar, but the framework for requirements for efficient and safe usage of the specific overhead crane installation vary a lot. In all applications, most important factors usually are time-wise efficient movement of the crane, safe operation of the crane, and easiness of operation.\par
% installation benefits
    The overhead crane may be installed in the factory space before the process machinery is in its place so that the actual construction of the factory is done with the newly installed crane. The installation cost of the crane in a new industrial manufacturing plant can be approximately the same as the rental cost of construction cranes. Monetarily, it makes sense to install a crane to an expensive industry plant since it will be later required for service operations of the process machines, too. In custom manufacturing and warehouse solutions, the overhead crane is often installed inside a warehouse. Sometimes, in e.g. dry docks and train depots the overhead crane can be installed outside on the yard, supported by beam walls.\par
% thesis focus on overhead cranes 
    In this thesis, the main focus of the load object dimensions measurement is on custom manufacturing load objects where the carried object size is previously unknown and may change during the lifting operation. Since each item lifted with the crane should be a unique object that was ordered by a customer, or at least a non-standard size object, it is valuable to understand what size the load object is for the software system that is moving it inside the warehouse. For heavy process industry, it is not meaningful to measure load objects (containers) whose size is known, and whose size is standard even if not all load objects handled are the same size. For practical reasons, all testing on overhead cranes have been done using a company test site installation that is capable of lifting medium heavy objects up to 15000 kilograms. All the objects lifted with the overhead crane were smaller than 2 meters for all dimensions, which is quite a normal size for a pallet. Since only the volume of the object affects its dimensional measurements, the weight of a lifted object is not considered in the works.\par

\section{Forestry Machine Cranes}
\label{section:forestry_machines_cranes}

A forestry machine load object measurement is a secondary objective that is under research in the scope of the thesis. Only forestry machines that include a small telescopic crane are included as research targets, mainly log picker trucks. In tree felling work, little automation is used currently, and in order to increase the level of automation, a load object measurement system has been suggested. Since the crane boom instrumentation is practically difficult, and the sensory platform that is used in this thesis is suitable for instrumentation of the log picking crane, it is only natural to research whether this kind of system can make teleoperation of a log picker truck easier, or help in the difficult task of automating the log picking process. 

\section{Goals And Objectives}
\label{section:goals_and_objectives}

An energy-saving, semi-automatic, safe process crane working area is the desired final outcome for upgrading a traditional manually operated process crane installation. It is under review whether a detailed, almost real-time description of the environment could provide enough information to realize the envisioned system. Currently, to the knowledge of the author, there are no commercially available systems that can provide a real-time description of an environment that integrates to a crane programmable logic controller (PLC), but it is a grand vision to start out with. The first step in realising such a system is to provide a measurement or a simple model of the load object and start aggregating other descriptions of the environment on top of it. To start with, this thesis uses a perception sensor platform solution that provides geometry knowledge of the process crane working area, and an external computer that generates the best possible geometry description of the load object for the use of the crane PLC computer. 

% person lifter



%===============================================================================================================================
\chapter{Industry Applications}
\label{chapter:industry_applications}

% stereoscopic teleoperation

%Teoriaa ja teknologioita. Pituus noin 20 sivua.
%Applications that can benefit from accurate positioning and tracking of a load object

\section{Existing Positioning And Tracking Applications}
\label{section:existing_positioning_and_tracking_applications}}

\section{Safety Applications}
\label{section:safety_applications}

Traditional and new safety applications can utilize load positioning information for added functionality.

A traditional safety application stops the crane process if a human is detected in a dangerous work zone \cite{Raula11}. Alternatively, an adaptive safety scheme can be utilised to restrict the operation range of the crane, but not halt it. If we are able to reliably detect a location for the moving load

The work area of the crane can be modelled with state-of-the-art sensing of the environment enabling for example pre-collision avoidance.

Adaptive safety techniques can be realised with the improved

\section{Mapping Applications}
\label{section:mapping_applications}


\section{Selected Use Cases}
\label{section:selected_use_cases}


%===============================================================================================================================
\chapter{Perception Sensors And Environment Representations}
\label{chapter:perception_sensors_and_environment_representations}

\section{Perception Sensors}
\label{section:perception_sensors}

+ active sensor: a sensor that controls and uses its own images. An active sensor controls its image's illumination with an electromagentic energy source, such as laser or white light.
+ time-of-flight cameras (TOF)
According to other research, the Kinect sensor cannot observe items cast in direct sunlight, preventing extensive outdoor usage besides its range limitations \cite{tikkanen13}.  

\subsection{Stereo Cameras}
\label{subsection:stereo_cameras}

\subsubsection{Photometrics}
\label{subsubsection:photometrics}

The aperture of the camera lens system controls the depth of field (DOF) perceived in a digital or analog image of a scene. The depth of field describes the range of depth that can be viewed in sharp detail through the lens system. If the DOF is large, then only a small depth range is in sharp detail, and the rest of the image is blurry for near-field objects and faraway objects. If the DOF is small, then near-field objects and faraway objects are sharply in focus alike. For a stereo camera depth measurement system, small DOFs, or extended DOFs are used to fully visualize a deep range of a scene in sharp detail. Diffraction limits the size of aperture, and small DOF images may suffer from high variance noise effects and low light conditions.

Radiance is a measure of power of light radiated from a unit surface area of an object to some spatial angle. 
Irradiance is a measure of power of light cast on a unit surface are of an object.
The relation between an object's surface that radiates onto the camera sensor array can be formulated as

\begin{equation}
\label{equation:radiance}
E = L \frac{\pi}{4} (\frac{d}{f})^2 \cos^4{\alpha}
\end{equation}

where L is object's radiance of unit surface area, and $\frac{d}{f}$ is the f-number of the lens in use. The $cos^4{\alpha}$ is the vignetting term. Not all radiating energy from the object is captured by the CMOS sensor array, but only the fraction that enters the camera lens and falls on the effective sensor array surface.

%weather conditions

Quality of measurements degrade in bad weather, such as fog, mist, rain, or snow \cite{Kawai12}. 
Especially camera systems are affected with possibly inhomogenous degradation in image quality.

Different types of degradation:
    + lens flare
    + condensation in humidity changes
    + changes in illumination
    + shadowing
    + occlusion
    + noise from moving droplets of water e.g.

If a stereo camera is used, inhomogenous degradation in image quality


Colored markers are sensitive to natural light in outdoor environments \cite{Kawai12}.

    
\subsection{Lidars And Radars And Sonars}
\label{subsection:lidars_and_radars_and_sonars}

Automotive radar is a millimeter wavelength radar that can measure ground properties or obstacles in front of a robot or a vehicle. It can measure environmental properties in the range of 1 to 120 meters \cite{Ahtiainen12}.

The advantages that the automotive radar has are weather-independent operation, and direct acquisition of range and velocity measurements \cite{Wenger07}.

The challenges that automotive radar introduces are measurement accuracy in resolution, and electromagnetic emissions on other radio frequencies \cite{Wenger07}. 

The automotive radar operates on a wide band of 5GHz to achieve a resolution of 5 cm \cite{Wenger07}.

In Europe the bands that are reserved for automotive radar operation in traffic applications are approximately 21 GHz to 27 GHz and 77 GHz to 81 GHz \cite{Wenger07}.

A world-wide harmonized channel allocation system for radar widebands has been proposed, but also considered very difficult to achieve due to national regulations \cite{Wenger07}.

Radar is not very widely used for machine perception due to low resolution and high cost. Instead, a stereo camera is often used. Still, automotive radar is an interesting add-on because of its capability of operation in adverse weathe, for example fog. 


\section{3d Scene Representations}
\label{section:3d_scene_representations}
+ possible algorithms: clustering, water-filling, and convex hull computations

In the virtual reality (VR) research the modeling of real-world items has been a big problem for many years. 
It is not easy to recreate a real-world item in a digital system, but many approaches have been suggested.

One option is to record video from multiple cameras that look at the object from different points of view. In order to obtain a digital model we may utilise a multi-camera stereo process and compute range images for further processing. Obviously, this approach is very computationally difficult and results vary depending on the object convexity, lighting environment and the used algorithms.  

A study by the Carnegie Mellon university robotics institute call the model acquired from the intensity image and the corresponding range imagery a visible surface model (VSM) \cite{Rander97}. Basically a VSM contains a subset of real-world surface features and texture in the digital format, and the VSM can be further placed into a more comprehensive representation of the world, namely a complete surface model (CSM). 

\subsection{Range Image Representation}
\label{subsection:range_image_representation}

Range image is a type of 3d data point representation that uses a 2d image array whose pixels correspond to a known location in the imaged scene. A range image is conceptually equivalent to an intensity image where image intensities are replaced with range measurements acquired from a sensor.

Range images have limitations in the ability to reproduce a detailed original point cloud. The term detailed point cloud here is a reference to an object model that loosely describes the object shape and details using a convex hull of points in space, including points that may be not visible from a single viewpoint. Then, if the detailed original point cloud includes multiple points on a line that projects to a single pixel in the range image, some data will be lost in the conversion to a range image. 

Also, if the ranging sensor has a spatially different measurement lattice structure compared to the 2d-array structure of range image, some pixels may be left blank because of a mismatch in spacing of range measurements.

for range measurements than what the range image uses, then some pixels in the range image will be left empty.

Since we create the point cloud with a camera system, we would not have the described projection data loss problem. Another problem with range images       \cite{Unnikrishnan08}.

\subsection{Volume Pixel Representation}
\label{subsection:volume_pixel_representation}

A volume pixel, or a voxel, is a prototype of a single entry in a 3d grid structure consisting of voxels. A voxel indicates occupancy in space, and it has at least a volume, and a location, which usually is in 3-dimensional array with constant dimensional sampling. A perfect example of a prototype voxel is a wooden box, which occupies a constant volume in space, and has a location in terms of, for example, a living room space. In digital systems, a voxel is more of a conceptual item that has a location, size, and optional services, such as statistical information about the encompassed voxel subspace. 

A 3d scene captured with a stereo camera system can be represented with a voxel grid. A standard voxel grid will have a constant sampling grid, thus, the scene will be composed of same-sized cubes. Anoter option is to use a varying sampling grid structure, or an octree structure (described in...?).

limitations: grid resolution, high-resolution grid consumes a lot of memory

\subsection{Octree Structure}
\label{subsection:octree_structure}
% multi-resolution octree representations

Originally, the idea to use octrees for mapping was introduced by Meagher in 1982 \cite{Meagher82].

Octomap is a new open-source framework for depicting 3D maps that can represent the world in multiple volumetric resolutions. The Octomap framework is based on a probabilistic octree structure using probabilistic occupancy estimation \cite{Hornung10}. 

We can assume that in all cases of range measurements a lot of noise and uncertainty will be introduced in the measured values. Thus, it is a good idea to represent the range measurements in a map that is probabilistic in nature.

Octomap differs from a traditional point cloud in some ways. In the original work Hornung et al. claim that a point cloud uses up a large amount of memory and are only suitable for modeling static environments with high precision sensory equipment.

+ Octomap can model occupied space, free space, and unknown space all at the same time.


+ probabilistic sensor fusion

Octomap
    + memory-efficiency principle (memory and disk space handling)
    + differentiates unmapped and obstacle-free areas
    + supports probabilistic information fusing
    + can represent arbitrary shapes in space (elevation map cannot)

\section{Geometric Environment Acquisition Techniques}
\label{section:environment_acquisition_techniques}

In this work the focus on environmental acquisition is on systems capable of 3d modeling the geometry of surrounding environments. Other environmental variables, such as temperature, wind conditions, humidity or other qualities of the environment are not considered in the context of 3d acquisition, although they could possibly have an effect on the perception sensor parameters in a dexterous real-world acquisition system. 

In general, an unordered real-world scene is difficult to model using perception sensors. A lot of study in real-world modeling has been done using multiple sensors including laser range scanners, multiple viewpoint cameras, color cameras, inertial measurement units (IMU) and global positioning system (GPS). Multiple perception sensors and large image quantities from a single environment are needed in order to achieve a realistic virtual environment model that displays a correct geometry of the environment, and presents textured object models with realistic lighting. \ref{ElHakim98}

Since our HIMMELI platform based solution that is used in this work enables only a single viewpoint stereo camera produced depth map at a single timestep, the model of the acquired environment is limited to a sparse point cloud presentation. Thus, the output 3d geometry will not resemble any realistic scene, but it does contain keypoints whose location and distance can be measured to produce Euclidean space measurements. In addition, a single-channel black and white high definition range image is available for e.g. object classification purposes. The details of the HIMMELI platform are presented in chapter \ref{section:himmeli_sensor_platform} in more depth.

Environment acquisition techniques either use an active perception sensor that emits energy in the environment, or passive perception sensor that only records energy reflected or radiated from the environment. 

%omnidirectional sensors

%multi-view registration
% application: city-scale 3d reconstruction from community-sourced online photos | ref Agarval09

\subsection{Triangulation}
\label{subsection:triangulation}

+ lidar cannot measure the depth of highly reflective materials, or very low-reflectance materials such as glass or paint-black steel.
+ Mechanical coordinate measuring machine (CMM) (3D modeling)
    + DCC CMM

+ Structured Light Triangulation
    +laser plane range finder

\subsection{Structure From Motion}
\label{subsection:structure_from_motion}

Structure from X techniques
+ Shape from shading
+ Structure from motion

\subsection{Multiple Perspective Interactive Video}

Multiple perspective interactive video (MPI) method is used to enhance live video with interactivity. The user can adjust camera views, or add digital content in the video. 

Basically, a detection of motion is achieved when each video frame is compared to a background image and the differences are logged. 

The method starts with knowing a priori metric map of the scene, and it requires multiple cameras to work with. The number of cameras connected to the system is in range from 2 to 1000 cameras.

Visible surface model
+ 2.5D Marr paradigm
+ Structure recovery method: multibaseline stereo
+ First make a triangle mesh model 

The real-world coordinates of each backprojected pixel can be computed using the depth and image coordinates given a camera calibration \cite{Rander97}. The resulting set of [x,y,z] coordinates is mainly referred to as a point cloud, depicting a point structure of the computed object.

In MPI method a triangle mesh presentation of the depicted scene is computed with the resulting point cloud. Many software exist that can visualise a point cloud, such as rviz in RoS system, but the suggested textured mesh presentation needs further processing and texture mapping. 

After digital processing we get a digital model of the scene from viewpoint A. When the camera is located exactly in viewpoint A, the presentation is flawless in comparison to the original scene. The report from Rander et al. \cite{Rander97} states that if a virtual camera is moved not far away, the scene can be transformed to another viewpoint B. In viewpoint B the image contains holes where occlusion was present in the original scene.

If the point cloud contains N x N points in the depth map, the computational complexity for triangles is O(N^2) \cite{Rander97}. Furthermore, the modeling of planar surfaces introduce shortcomings in surface tessellation. The surface may be tessellated with 1000 triangles in favor of 2 triangles, which means the computation overhead increases and causes extra calculations in texturing and modeling phases. The number of triangles in an area is described by a measure called mesh resolution that can be controlled in favor of smarter modeling \cite{Johnson96}.






\subsection{Stereo Techniques}
Synthetic random dot images can be used for algorithm testing \cite{Zitnick00}.
Pollard et al introduces a PMF algorithm that is based on three stereo matching constraints. The third constraint, disparity gradient limit, is unique to the PMF algorithm \cite{Sonka07}.
Photometric Stereo \cite{Woodham80}.
    + Recovers surface orientation assuming a known reflection function
    + a Lambertian surface model
    + requires at least 3 images of the object, one fixed camera view, changing direction of illumination
    + point illumination sources with known intensity and direction


% another snippet

A histogram is a two-dimensional graphical representation of an image grey-level distribution. The grey-levels correspond to the image intensity. As depicted in figure X, the X axis represents the number of independent grey levels that the image contains. The grey-levels range from 0 to N levels. For example, in an 8-bit single channel image there can be up to 255 independent different tones of gray. The Y axis represents per centage information about a single grey-level ranging from 0\% to 100\%. Thus, the histogram essentially displays a distribution of all grey-levels.

Photometrically, grey-levels of an image are quantized estimates of image irradiance \cite{Sonka07}.
%histogram equalisation
Histogram equalization maps the current image intensity response over the full available intensity range. In figure N, the first image depicts image intensities distributed over a limited band of intensities. The second image displays the mapping result of the equalization process. The resulting histogram is calculated with the cumulative distribution function H'(i).

todo: corresponding histograms of two test images, and their cumulative distribution functions

%end of chapter 3
\chapter{Stereoscopic Machine Vision System}
\label{chapter:stereographic_machine_vision_system}
% stereoscopic means to perceive and/or see in 3d | stereographic means projecting a sphere onto a plane
Stereoscopic perception is a natural phenomenon first discovered in human and animal vision systems. Stereoscopy in human vision  is based on stereopsis effect, which means perceiving depth in a scene using binocular vision. Similarly, if two monocular cameras are used in a binocular setup it is possible to not perceive, but quantifiably measure scene depth using a calibrated camera system. %reference?

Such a calibrated two-camera system is most often called a \emph{stereo vision system}, or \emph{3d vision system}. In general, 3d vision systems used in engineering applications solve problems of 3d scene reconstruction, understanding of object properties, and minimization problems depending on the scientific application at hand. Often a stereo camera setup is used to solve the problem of 3d information recovery from 2d images using knowledge about the camera setup geometry and the image content.\cite{Sonka07}

Different 3d information recovery techniques are introduced in section \ref{section:environment_acquisition_techniques} 

\section{From 2d to 3d}
\label{section:from_2d_to_3d}

According to \cite{Sonka07}, an object-centered approach is required in favor of perspective approach in order to fully realise the power of descriptor classes used for reconstruction of the scene. The requirement then, is to move step-by-step from a pixel-based intensity image representation to a full three-dimensional representation.

1. From pixels to surface representation
2. From surface representation to surface orientation model
3. From surface orientation model to a full three-dimensional description

2D => primal sketch => 2.5D sketch => full 3D

The primal sketch is an edge image where physical edges are not incurred. The idea is to extract information about physical features of different scales using filtering and second-order zero crossings. First, the original image is filtered with different radii Gaussian filters and after that a Laplacian operator is used to locate second-order zero crossings \cite{Sonka07}. 

The next step constructs the 2.5D image, or a depth map, with bottom-up machine vision techniques. Since bottom-up techniques are used, this step of depth map computing is applicable for all applications and no additional domain-speficic information is needed. 

The final transformation from the depth map to a full three-dimensional object representation is not presented in the thesis. It is also the most difficult transformation as opposed to well-known mechanisms of primal sketch generation and depth map generation. It is 


No additional domain-specific information is needed since the 

Interesting problems
+ Feature observability in an image
+ Marr's theory
+ Perspective projection challenges

Bottom-up reconstruction
    + from multiple image construct range images

Top-down recognition
    + CAD model recognition especially

Space Carving
    + volumetric representation


+ object surface parameters
    + reflectivity e.g.
    
    
+different approaches: top-down (model-based) or bottom-up (reconstruction)



\subsection{Marr's Theory}
\label{subsection:marrs_theory}

Marr's theory especially points out the great challenge in machine vision of how to achieve a better understanding of the visual system in general, and not just create another vision application suited for a single specific target.

\subsection{Stereo Imaging Techniques}
\label{subsection:stereo_imaging_techniques}

One of the fundamental issues in computer vision is how to display a solid shape \cite{Sonka07}. 

Stereo imaging is divided into two different categories by the used algorithm. The first category is dense matching algorithms where stereo matching algorithms run on all pixels. The second category is sparse matching algorithms where only distinctive features will be matched unambiguosly \cite{Terho10}.

\section{Perspective Projection Geometry}
\label{section:perspective_projection_geometry}

In general, a perspective projection, or central projection, is the de facto type of projection for human beings. We all have a pair of eyes that can be modeled with a pinhole camera model, and the pinhole camera depicts any scene with a perspective projection. In perspective projection objects that are far away appear smaller than objects that are near. This phenomenon is called a perspective. 

The stereo cameras that sense the world with CCD arrays give the user a perspective projection of the world on a 2D image plane. The basics of this projective geometry will be discussed in the case of non-parallel setup of the stereo cameras. That means the optical axes of the cameras are not in parallel, but have a specified angle with which the optical axes coincide. The cameras themselves will be modeled as pinhole cameras with certain lens system errors corrected for.

In computer vision, 

+ homogenous transformation => transforms lines into lines



The camera geometry model that we use is a single perspective camera model or a pinhole camera model.


%As depicted in the \ref{}

\subsection{Epipolar Geometry}
\label{subsection:epipolar_geometry}

Next we will introduce a two-camera epipolar geometry setup that is used for the depth mapping of the image features.

+ here put an image of the stereo geometry setup

+ reference to the optical centers C and C', and baseline of the camera setup

By definition, an epipolar plane is the planar surface that spans between vectors $\vec{CX}$ and $\vec{C'X}$. The epipolar plane intersects the image planes $\pi$ and $\pi'$ at epipolar lines. A feature of the epipolar line is that any projected point X must lie on this epipolar line on the image plane, a fact which greatly reduces the search space for matching image features.

Next, we introduce the epipole, which is the line intersection of an epipolar line and the baseline of the stereo camera configuration. By definition, epipoles always lie on the baseline.

A canonical configuration is a stereo camera setup where the camera optical axes are in parallel. In the canonical configuration the epipoles move to infinity, and the effect known as disparity can be seen on the two resulting images. Disparity means the shifting property of image features in a picture taken in another world coordinate. For a real disparity effect the camera must shift only along a single coordinate axis.

Any other stereo configuration with non-parallel optical axes can be transformed into canonical one by image rectification processing. 

The depth of a measured feature can be computed if the geometry of the stereo setup and the intrinsic camera parameters are known. The depth measurement value $Z_m$ is calculated using equation \ref{equation:disparity_eq},

\begin{equation}
\label{equation:disparity_eq}
\mathsf{Z_m} = \frac{\mathsf{bf}}{\mathsf{d_x}}
\end{equation}

where \emph{b} is the baseline width between the principal points of the camera image planes, f is the focal length for both cameras, and $d_x$ is the measured horizontal disparity value in a standard parallel stereo camera configuration. Now, if \ref{equation:disparity_eq} is differentiated with respect to horizontal disparity $d_x$, an equation for the effect of disparity change in the output measurement is achieved. 

\begin{equation}
\label{equation:disparity_derivative}
\frac{dZ_m}{dd_x} = \frac{-bf}{d_x^²}
\end{equation}

Furthermore, we may insert disparity value as $d_x = X_L - X_R$






Following the constraints that the epipolar plane induces to the stereo matching problem a fundamental matrix is introduced. The co-planar constraint 

\begin{equation}
\mathbf{X}_{L}^T (\mathbf{t} \times \mathbf{X'}_{L}) = 0
\end{equation}

is transformed into 

\begin{equation}
\nathbf{u}^T (K^{-1})^T S(t) R^{-1} (K')^{-1} \mathbf{u'} = 0
\end

An interested reader can find the full formulation of the fundamental matrix in Sonka et al. book \cite{Sonka07}. The fundamental matrix F is according to the bilinear relation introduced by Longuet and Higgins

\begin{equation}
\mathbf{u}^T F \mathbf{u'} = 0
\end{equation}

where the fundamental matrix F is 

\begin{equation}
F = (K^{-1})^T S(\mathbf{t}) R^{-1} (K')^{-1}
\end{equation}

The fundamental matrix captures all the available information from a pair of views between two different images.
The relative motion of the camera scheme is present in the used stereo camera setup, or namely two cameras with known caliberation in space. As we know the camera calibration matrices the normalized measurements can be described with a similar bilinear relation that includes the essential matrix $\mathbf{E}$. E can be estimated from the image measurements and it has a rank of 2 and it can be decomposed with a singular value decomposition (SVD). A normalization must be done in order to solve the epipolar geometry problem so that the solution is numerically stable \cite{Sonka07}.'
%Butterworth 97 also 

The fact that the essential matrix is a $3x3$ matrix with a rank of 2 means the projection of point X onto the image plane is a lossy transformation. 

Solving the overdetermined linear equation system in epipolar geometry requires a minimum of 6 image points. Noise affects the solution, and real image measurements use an algorithm called eight-point algorithm or a modification of it. The solution is minimised with least squares (Frobenius norm) effort. 
+ at least 8 points for a linear solution in 2 image system \cite{Sonka07}
+ at least 7 points for a linear solution in 3 image system (trilinear tensors used)

+ Mismatch errors 

+ 3D similarity reconstruction
    + X is found up to a scale and not Euclidean
    
+ case of unknown intrinsic and extrinsic parameters is not shown here
    + can do, depth from unknown video source

+ stereopsis => scene reconstruction in Euclidean reconstruction from 2 calibrated cameras

Using only two cameras instead of multiple cameras helps to avoid occlusion of objects and use existing powerful stereo processing tools.

+ 3 camera-setup can understand all available information in an orthographic projection => 4th camera does not add any more information
%page 473 Sonka07

\subsubsection{Intrinsic Camera Parameters}
\label{subsubsection:intrinsic_camera_parameters}

A camera model has a multitude of parameters whose amount depend on the selected camera and lens distortion models, and the number of calibration images used.

The camera model has internal parameters that are called the intrinsic camera parameters. The coefficients of the camera calibration matrix K \ref{equation:cameracalibrationmatrix} are the intrinsic camera parameters. If the coefficients of K are known, it is possible to extract metric values from the scene on the image plane $\pi$ \cite{Sonka07}.

If a more complex camera model is used, the matrix K may have larger dimensions, but the intrinsic parameters still are the coefficients of K.





A calibration technique that is unbiased and optimal is ideal. 

Originally, camera calibration algorithms were used in aerial imaging systems.


Many fast but inaccurate algortihms for parameter estimation have been introduced earlier. The increase in processing power of computing platforms helps to develop new non-linear camera calibration methods that are accurate. Heikkilä \cite{Heikkila00} claims that a subpixel accuracy of 1/50 of a pixel can be realistically achieved with a modern CCD imaging cell if such non-linear calibration is used.

+ Intrinsic camera parameters

+ Extrinsic camera parameters


\subsubsection{Extrinsic Camera Parameters}
\label{subsubsection:extrinsic_camera_parameters}

\subsection{Stereo Assumptions}
\label{subsection:stereo_assumptions}

Some basic assumptions about the measured crane working area must be made in order to use stereo correspondence algorithms. These assumptions operate on the collected images from the two stereo cameras. 

In general, the assumptions hold for any stereo camera setup that work on disparity principle unless otherwise stated. 

A single pixel must correspond to a single surface point in the world in order to construct a linear mapping function from the left stereo camera image to the right camera image. This restriction means that no opaque, or see-through, materials can be accurately measured with the used technology. Additionally, occluded and self-occluded surface pixels will be discarded in order to comply with the single pixel correspondence assumption.

+ A single pixel corresponds to a single surface point (restriction: no opaque materials | fish + fish bowl)
+ Disparity values are generally continuos (smooth within a local neighbourhood | discontinuities occur only at object boundaries)
+ Ordering constraint (if an object A is to the left of the object B in the left image, then the object A will be left of the object B also in the right image | sometimes violated by pole-like objects)
+ Lambertian surfaces => surfaces do not change appearance when viewed from another angle.
    + opaque
    + ideal diffusion
    + reclefts light energy in all directions

Stereo vision produces a dense disparity map . (If we compare to laser scanner sensor, the denseness of the produced point cloud is much higher)
Some requirements for the disparity map are elaborated in Zitnick et al. report, namely the disparity map should be smooth and detailed . The desirable result would provide smooth continuos mapping that detects small surface elements as separable regions. It turns out that these requirements are opposing to each other: a smooth continuos disparit map tends to filter out small details, and a detail preserving mapping is affected by noise \cite{Zitnick00}.

\subsection{Stereo Errors}
\label{subsection:stereo_errors}

+Embedded geometric and radiometric difficulties

In this work the machine vision system is used as means of extracting coordinate data from a pictured scene. Full recovery of the spatial 3d scene is not possible, because the stereo cameras only see the surface parts that project onto the 2d image plane.
In our case, a full recovery of a 3d object model is not needed to solve the problem of load positioning and dimensional measurement. As a result of depth mapping, a point cloud will provide coordinate data for the visible surface, which we can mask three-dimensionally and process further.


It can be seen from \ref{equation:disparity_limit} that the accuracy of such a stereo system is linearly proportional to the disparity measurement error, and there are trade-offs for most factors, such as focal length, camera separation, optical axes convergence, and aperture selection.

The lenght of the camera separation in a stereo camera system, also called the baseline width, affects the triangulation accuracy of the depth measurement setup to some extent. A common width for comparison is the human eye baseline width of X
% camera separation or baseline
+ hyperstereo = baseline much higher then the human vision system baseline
%=> distorts perception of motion for humans
%=> distorts perception of glossiness of materials
%=> also crosstalk = image leaks to the area of the other eye => visible ghosting in stereo perception


According to equation \ref{}


In standard stereo processing the object measurement error increases with items that are located far away. If a single baseline and resolution is used the error grows quadratically with increasing depth \cite{Gallup08}.

A simple equation for the depth error is introduced in \cite{Gallup08}:

\begin{equation}
\epsilon_z = \frac{z^2}{bf}\dot \epsilon_d
\end{equation}

where \emph{z} is the depth value, \emph{b} is the baseline value, \emph{f} is the focal length, and \epsilon_d is the disparity matching error (in pixels). The resulting depth error $\epsilon_z$ 


\subsubsection{Occlusion}
\label{subsubsection:occlusion}


Occlusion means partial or full loss of line of sight of an object in an image. In stereo matching, an object may be partially occluded by another object thus resulting in the original object surface in a stereo image A that is not visible in the stereo image B. The object may be occluded in only left or right stereo image, or partially occluded in both. In both cases, the other image has visible object surface that cannot be matched in the other image.

Occlusion is a big problem in stereo matching process. Most solutions do not explicitly detect these occluded regions 
Some methods:
+ multiple cameras and camera masking
+ bidirectional matching
+ 

+ occluding boundary is the surface of object that slides in the horizon so that the tangent of the surface is towards the viewer.

+ merging and splitting situations \cite{tikkanen13}
+ object tracks, initiating and terminating
+ 

\section{Camera Modeling And Calibration According To A Pinhole Camera Model}
\label{section:camera_calibration_according_to_pinhole_camera_model}

TODO: different camera models
1. pinhole camera model
2. orthographic projection
3. scaled orthographic projection
4. paraperspective projection
5. perspective projection
%source: NUS CS4243 camera.pdf slide

The camera model maps the transformation between the real-world object and the projection of the camera image plane. The camera model is a set of parameter that define this transformation as realistically as possible. 

The real-world object projection on the camera image plane is a lossy transformation where a lot of information is lost due transformation. For example, when a cube is projected on the image plane, we cannot compute the texture of all 6 cube faces anymore because the data of all non-visible cube faces is lost in the image plane.

Typical camera model is usually based on ortographic projection or perspective projection models. In 3D imaging a suitable camera model would be the perspective projection model with lens distortion modeling included. The selection really depends on the applications and its accuracy requirements for model accuracy, but in simple computer vision applications the linear ortographic model would be appropriate. 


\subsection{Pinhole Camera Geometry}
\label{subsection:pinhole_camera_geometry}


A pinhole camera will be the basis of the theoretical camera modeling since it is the simplest approximation that is suitable for a computer vision applications \cite{Sonka07}.

Some basic geometry items. There is an image plane. An optical axis lies perpendicularly to the image plane pointing out from the image plane towards the scene. 
Optical axis.
Focal point (optical point)

The pinhole camera includes a thin lens system that can be approximated with ideal formulation. 




The image sensor has width \emph{w} and height \emph{h} which relate to each other with aspect ratio \emph{a} so that

\begin{equation}
a = \frac{w}{h}
\end{equation}

We can relate the width of the image sensor with the field of view angle by equation

\begin{equation}
w = 2f\tan{\frac{\theta_{fov}}{2}}
\label{equation:width}
\end{equation}

If we consider a fixed-baseline stereo setup where baseline \emph{b} is kept constant a report by Gallup et al. easily shows how the depth error is related to the depth measurement in quadratic sense by equation

If we consider the effect of resolution on the disparity computing, or depth value computing, we can start with the number of pixels in the image sensor by equation

\begin{equation}
number of pixels = wh = \frac{w^2}{a}
\end{equation}

Furthermore, we can substitute width \emph{w} from \ref{equation:width} getting the equation for the number of pixels as in \cite{Gallup08}:

\begin{equation}
nop = \frac{{z_{far}}^4}{{\epsilon_z}^2} \frac{4\tan^2{\frac{\theta_{fov}}{2}}}{b^2 a}
\label{equation:zdepth}
\end{equation}

As we can see from equation \ref{equation:zepth} the increase in pixel resolution to match a specified accuracy also must match the depth resolution proportionally to ${z_{far}}^4$. Consequently, the required increase in resolution may be impossible due to engineering limitations of new hardware or computations limits of increased overheard due to increase in number of pixels.

The increase in stereo processing computation overhead is tightly coupled with the selected image resolution and depth error. According to \cite{Gallup08} the number of pixel comparisons needed is in the growth range of $ \Omega({z_{far}}^6 {\epsilon_z}^-3)$. For example, if we wish to extend the depth range by a factor of 3 the required number of comparisons would be $3^6 = 729$ times more computationally expensive.

When faced with this kind of image computation problem that should ideally be run in real-time the design of the imaging system must be carefully planned. It should be noted that increase in image resolution seems not to solve a stereo correspondence problem, but it may severely increase the computation time. 

The basis for camera modeling begins with the pinhole camera model where image coordinates \emph{u} and \emph{v} are transformed as

\begin{equation}
\Biggl[ \begin{array}{c}
\emph{u} \\
\emph{v} \\
1 \end{array} \Biggl]
= \Biggl[ \begin{array}{c}
\lambda \emph{u} \\
\lambda \emph{v} \\
\lambda \end{array} \Biggl]
= \textbf{F}\Biggl[ \begin{array}{c}
\emph{X} \\
\emph{Y} \\
\emph{Z} \\
1 \end{array} \Biggl]
= \textbf{PM} \Biggl[ \begin{array}{c} 
\emph{X} \\
\emph{Y} \\
\emph{Z} \\
1 \end{array} \Biggl]
\end{equation}

where a perspective transformation matrix \emph{P} is introduced as

%\begin{equation}
%\textbf{P} = \Biggl[ \begin{array}{cccc}
%emph{sf} & 0 & \emph{u_0} & 0 \\
% & \emph{f} & \emph{v_0} & 0 \\
% & 0 & 1 & 0 \end{array} \Biggl]
%\end{equation}

%+ The camera matrix

\begin{equation} K =
\Biggl[ \begin{array}{ccc}
Focal length 1 & Aspect ratio * Focal length 1 & Principal point 1 \\
0 & Focal length 2 & Principal point 2 \\
0 & 0 & 1 \Biggl] \end{array}
\end{equation}

+ Aspect ratio

%\image{lossy_transformation}

Camera calibration can be weak or strong. If strong camera parameters are known it is possible to calculate image scene Euclidian metrics. If weak camera parameters are known, only a pixel to pixel transformation from one image to another is possible, for example, an epipolar projection transformation can be done with weak parameters \cite{Rander97}.

\subsection{Single Camera Calibration}
\label{subsection:single_camera_calibration}

+ Why do we have to calibrate the cameras?
    + correct distortions
    + rectify images
        + Why do have to we rectify images?
    + improve performance of stereo corresponsdence algorithms
    
+ How do we calibrate a camera?
    + Air-glass interfaces in the lens system

Calibration is the means of linking the three-dimensional object and its two-dimensional projected representation in a scientific way. By modeling the image capture and projection with a camera, a camera model is presented. A camera model consist of multiple unknown parameters that need to be solved for a complete representation of the imaging sequence.

\subsubsection{Calibration Targets}
\label{subsubsection:calibration_targets}

Planar calibration grids are the most common calibration method for a stereo camera setup. Mainly checkerboard patterns and rectangular grid patterns are used for calibration purposes.

\subsubsection{Calibration Errors}
\label{subsubsection:calibration_errors}

+ Focal length estimation errors
It was found out in study by Kytö that the focal length affects the depth threshold of a stereo setup more dominantly than the baseline separation of the cameras. %what is really depth threshold 

+ Tangential distortion
+ Radial distortion (barrel effect)
It is important to compensate for lens system radial distortions, because they cause vertical disparity on the edges of the image. \ref{Kyto14}

+ Chromatic aberration

+ Affine distortion (aspect ratio)

+ Camera electronics, light intensity changes affect the phase locked loop (PPL) in the electronics => line jitter with sync signal systematic or random noise change

+ Calibration target location

\subsection{Stereo Camera Calibration}
\label{subsection:stereo_camera_calibration}

+ toed-in or parallel camera setup

The stereo cameras can be physically rotated towards each other, or they can be fixed in parallel configuration so that the optical axes intersect at infinity.

A parallel optical axes alignment is preferred to avoid vertical disparity caused by keystone distortion correction. The toed-in camera setup also induces depth plane curvature, which is an unwanted warping phenomenon of the image.\ref{Kyto14}    

+ convergence distance of the stereo system optical axes: approx. 264cm

The convergence distance of the stereo system optical axes affects the perceived location of the disparity range. The convergence distance can be computer controlled \ref{Chen et al. 2010 from Kyto14}, but in this work the convergence distance is not approximated accurately since the collected stereo video is not meant to be recorded for human vision system viewing. Increasing the convergence depth will also increase possibility for frame violations for negative disparity values near the edges of the screen. A frame violation would break the 3d perception illusion for a viewer, but for a computer system a violation will result in only an erroneous depth match that is eliminated from the final disparity image.
%what is negative disparity? => in front of the screen => does this matter in a measurement system


\section{Stereo Correspondence Problem}
\label{section:stereo_correspondence_problem}

It is important to choose a proper window size for stereo matching. In low contrast regions of the image too small a window cannot guarantee a unique match because of too little intensity variation. 
In general, a small window is desirable to avoid unnecessary smoothing, but optimal window size depends on the region intensity variation, texture, and disparity \cite{Zitnick00}.
The problems of window size can be solved with iterative window size methods, but increase in computation overhead or problems at occlusion boundary are still left unsolved \cite{Zitnick00}..

+ Feature-based Stereo Vision Algorithms
    + SIFT (slow)
    + SURF
    + MSER
    
\image{disparity_space_from_zitnick_report}

+ Uniqueness and continuity assumptions

% correspondence problem
+ the ambiguity of the correspondence problem can be reduced using one or multiple constraints such as
disparity limit constraint (disparity must be < limit, e.g. finger in front of the face cannot be imagined in stereo)
disparity smoothness (disparity changes only a little in any direction)
epipolar constraint (search space for matching pixels 1D on the epipolar line)
figural disparity constraint (corresponding elements should lie on an edge element in both images)
feature compatibility (physical origin of the matched points should be the same)
geometric similarity constraint (geometric features differ only a little)
mutual correspondence constraint (occluded points are not found thus ruled out)
ordering constraints (typically points lie on the same epipolar line for similar depth items, exception narrow close-up sticks)
photometric compatibility (little intensity differences)
uniqueness constraint (1 pixel can only correspond to 1 pixel in the 2nd image, exception two or more points on 1 ray in 1 image)

+ specularity edges cannot be used for feature matching because specular lighting changes depending on the viewpoint \cite{Sonka07}

Now, the point $U_c$ in equation \ref{equation:projectedpoint} can be represented in homogenous coordinate system as 

\begin{equation}
\tilde{U_c} = \Biggl[\begin{array}{c}
U \\
V \\
W \end{array} \Biggl]
\end{equation}

\subsection{Block Matching Techniques}
\label{subsection:block_matching_techniques}

\section{Depth Map Generation}
\label{section:depth_map_generation}

After the image rectification is done for a stereo image pair captured by the calibrated stereo camera setup, then the 3d point cloud generation becomes a problem of extracting the estimate of the disparity map (d(x,y).

Q is the reprojection matrix that encodes information about the stereo camera setup. Q matrix will provide information about camera optical axis convergence (parallel or cross-eyed), image plane principal points, and the stereo setup baseline width b.

%end of chapter 4
%===============================================================================================================================
\chapter{Coordinate Systems And Transformations}
\label{chapter:coordinate_systems_and_transformations}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{process_crane_load_object.jpg}
    \caption{The lifted load object in the process crane use case.}
    \label{fig:process_crane_load_object.jpg}
  \end{center}
\end{figure}

\label{chapter:load_object_registration_and_tracking}

\section{Image Coordinate Systems}
\label{section:image_coordinate_systems}

A widely accepted right-handed coordinate system will be used for describing a three-dimensional coordinate system. The details will be further discussed in the following sections. In general, the positive Z axis points out towards the scene in a two-dimensional image plane.

\subsection{Coordinate Transformations}
\label{subsection:coordinate_transformations}

The coordinate transformation matrices depicted in this thesis are three-dimensional Cartesian coordinate transformations who use a base coordinate denoted (x,y,z).

A point is a metric location in space that is uniquely defined with a three-dimensional vector in a chosen coordinate system.
A spatial coordinate has three parameters, namely x, y, and z, that will belong to a coordinate system indicated by a subscript character.

\begin{equation*}
\Biggl[ \begin{array}{ccc}
a & b & c \\
d & e & f \\
g & h & i \end{array} \Biggl]
\end{equation*}

The subscript depicts the coordinate space where a point is located. 
Also the transformation matrix has a subscript: it depicts on which coordinate space point (or vector) the transformation acts upon.
The transformation matrix has a superscript: it depicts to which coordinate space the point will end up transformed in.

\subsubsection{Geometric projection}
\label{subsubsection:geometric_projection}

A world point $X_w$ is translated and rotated into the camera coordinate system so that

\begin{equation}
\textbf{X}_c = \Biggl[ \begin{array}{c}
x_c \\
y_c \\
z_c \end{array} \Biggl] = R(\textbf{X}_w - \textbf{t})
\end{equation}

The point in the world according to the camera coordinate system now transformed to point
$X_c$. Next, the point will be projected onto the image plane $\pi$ according to the pinhole camera model. 
Point $X_c$ projects onto the image plane as Euclidean point $U_c$ where the projected point can be computed from the similar triangles of image \ref{image:projectedpoint}. 

\begin{equation}
\label{equation:projectedpointUc}
\textbf{U}_c = \Biggl[ \begin{array}{c}
\frac{-fx_c}{z_c} \\
\frac{-fy_c}{z_c} \\
-f \end{array} \Biggl]
\end{equation}

The image point that the camera outputs is the affine transformed $\textbf{U}_c$. Next, we must formulate a homogenous coordinate representation that allows to compute the affine transformation with a $3x3$ matrix multiplication for \ref{equation:projectedpointUc}.

We can represent a homogenous coordinate point in the image plane $\pi$ as 

\begin{equation}
\label{equation:affinedefinition}
\tilde{u} = \Biggl[ \begin{array}{c}
U \\
V \\
W \end{array} \Biggl]
\end{equation}

so that a Euclidean distance representation of point $\tilde{u}$ is 

\begin{equation}
\textbf{u} = \Biggl[ \begin{array}{c}
u \\
v \end{array} \Biggl] = \Biggl[ \begin{array}{c}
\frac{U}{W} \\
\frac{V}{W} \end{array} \Biggl]
\end{equation}

Now we have calculated the projected point \ref{equation:projectedpointUc} and we have determined that the resulting affine transformation results in $\tilde{u}$ in \ref{equation:affinedefinition}. We still need to define a principal point of the image plane $\pi$, which is the point where the optical axis and the image plane $\pi$ intersect. This point defines the homogenous transformation according to the intrinsic camera parameters. The principal point is defined in the affine image coordinate system as

\begin{equation}
\textbf{U}_{0a} = \Biggl[ \begin{array}{c}
u_0 \\
v_0 \\
0 \end{array} \Biggl]
\end{equation}

The equation for the affine transformation of projected point $U_c$ is

\begin{equation}
\tilde{u} = \Biggl[ \begin{array}{c}
U \\
V \\
W \end{array} \Biggl] = \Biggl[ \begin{array}{ccc}
a & b & -u_0 \\
0 & c & -v_0 \\
0 & 0 & 1 \end{array} \Biggl] \Biggl[ \begin{array}{c}
\frac{-fx_c}{z_c} \\
\frac{-fy_c}{z_c} \\
1 \end{array} \Biggl] 
\end{equation}

and with reordering the multiplication of $-f$ we obtain

\begin{equation}
\tilde{u} = \Biggl[ \begin{array}{ccc}
-fa & -fb & -u_0 \\
0 & -fc & -v_0 \\
0 & 0 & 1 \end{array} \Biggl] \Biggl[ \begin{array}{c}
\frac{x_c}{z_c} \\
\frac{y_c}{z_c} \\
1 \end{array} \Biggl]
\end{equation}

The obtained $3x3$ transformation matrix is called the camera calibration matrix K where

\begin{equation}
\label{equation:cameracalibrationmatrix}
K = \Biggl[ \begin{array}{ccc}
-fa & -fb & -u_0 \\
0 & -fc & -v_0 \\
0 & 0 & 1 \end{array} \Biggl]
\end{equation}.

We have followed the definition of Sonka et al. \cite{Sonka07} so that the resulting camera calibration matrix is an upper triangle containing three unknown parameters $a$, $b$, and $c$ for shearing and rescaling. If we study the coefficients of K carefully, we can see that the matrix actually contains the parameters focal length, the principal point, shear coefficient, and the affine distortion coefficients \cite{CaltechWeb10}.

Let us list the 5 intrinsic camera parameters in detail. The 5 intrinsic parameters and their associated coefficients of matrix K are 

\begin{itemize}
\label{list:intrinsiccameraparameters}
\setlength{\itemsep}{0pt}
\item -fa = represents scaling in the $u_a$ axis
\item -fb = shear coefficient that gives the alignment difference of camera coordinate system axis $x_c$ and affine image coordinate system axis $u_a$ at the length of focal length in pixels in the direction of affine image coordinate system axis $v_a$.
\item -fc = represents scaling in the $v_a$ axis
\item $u_0$ = u coordinate for the optical axis intersection on the image plane on the $u_a$ axis
\item $v_0$ = v coordinate for the optical axis intersection on the image plane on the $v_a$ axis
\end{itemize}

All the units are given in pixels.


\subsection{Prototype Transforms and Ordering}
\label{subsection:prototype_transforms_and_ordering}

Most of the matrix transformations are computed using three simple prototype transformations: translation, scaling, and rotation.

\textbf{Translation} of a point is given by a translation transformation matrix

\begin{equation}
\Biggl[ \begin{array}{c}
X^* \\
Y^* \\
Z^* \end{array} \Biggl]
= \Biggl[ \begin{array}{cccc}
1 & 0 & 0 & X_0 \\
0 & 1 & 0 & Y_0 \\
0 & 0 & 1 & Z_0 \\
0 & 0 & 0 & 1 \end{array} \Biggl]
\end{equation}

Using of square matrix is enforced here to enable multiple sequential transformations computation into a single transformation matrix. Additionally, a square matrix representation simplifies the notation a lot.

\textbf{Scaling} of a point is given by a scaling transformation matrix

+ scaling matrix definition here (2.5-8).

When is scaling used?

\textbf{Rotation} of a point is given by a rotation transformation matrix. A simple rotation matrix operates on a single axis at a time, and rotation about multiple axes is achieved with sequential single rotations about different axes of the three-dimensional image space. The outcome of multiple rotations depends on the order of selected axis rotations.

\begin{equation}
\textbf{R}_\alpha = \Biggl[ \begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & \cos \alpha & \sin \alpha & 0 \\
0 & -\sin \alpha & \cos \alpha & 0 \\
0 & 0 & 0 & 1 \end{array} \Biggl]
\end{equation}

Concatenating multiple transformations into a single $4 x 4$ matrix can achieve a simultaneous translation, rotation, and scaling. The ordering of the transformations depend on the matrix commutability, which in general is not a common property in transformation matrices, thus, the correct order of the transformations is crucial. 

It is obviously desirable to transform multiple coordinate points at a one time. Multiple coordinate point sets are called point clouds, and the transformation for a point cloud can be computed with the same square matrix used for single point transform.

If the point cloud $P$ contains $n$ points, we may structure a matrix P that contains the set of points as column vectors where

\begin{equation}
P = \Biggl[ \begin{array}{ccccc}
X_0 & X_1 & X_2 & \dots & X_n \\
Y_0 & Y_1 & X_2 & \dots & Y_n \\
Z_0 & Z_1 & X_2 & \dots & Z_n \end{array} \Biggl]
\end{equation}

\subsection{World Coordinate System}
\label{subsection:world_coordinate_system}

The world scene that the camera sensors are detecting is called \emph{the world} (or scene interchangeably), which maps to arbitrary world coordinate system with the coordinate convention $(X_w, Y_w, Z_w)$. Thus, the origin of the world coordinate system is the point $O_w$. The subscript $w$ is used with any point that belongs to the world.

The world is a three-dimensional system where a point lies in an Euclidian space $R^n$ and its span is $R^3$.
We acquire the world coordinate system orientation described in a report by Terho in 2010 \cite{Terho10}. Basically, the world x-y-surface can be fitted to the ground plane that is searched from coordinate point space. In this thesis, the absolute world coordinate mapping will be shown only if the information about the position and orientation of the camera platform will be available to use.

The world coordinate system axes may be referred to as world axes or scene axes. Interchangeably, they may be referred to as extrinsic axes of the camera.


The extrinsic parameters of the camera can be computed from the interaction between the camera coordinate system and the world coordinate system. Since the camera lies in some arbitrary world coordinates, and the detected scene changes according to the camera orientation, we can easily see that the extrinsic camera parameters change when the camera pose changes in the world coordinate system. The relation between the world coordinate system and the camera coordinate system can be computed with an Euclidean transformation consisting of a translation and rotation. 

+ Additionally, the world lighting conditions and weather affect the extrinsic parameters to some extent.?

\subsection{Camera Coordinate System}
\label{subsection:camera_coordinate_system}

The camera sensor array dimensions define the first two orthogonal dimensions of the camera coordinate system. The camera coordinate x-y-plane lies on the camera image array, and the z-axis points out from the image plane towards the camera lens system along the optical axis. The camera coordinate system z-axis coincides with the optical axis of the camera in an ideal situation.

The focal point of the camera defines the origin of the camera coordinate system, namely the focal point $O_c$. The subscript $c$ is used with any point that belongs to the camera coordinate system, mainly the physical points of the imaging array.

The camera coordinate system axes may be referred to as camera axes. Interchangeably, they may be referred to as intrinsic axes of the camera.

\subsubsection{Camera Image Plane}
\label{subsubsection:camera_image_plane}

The camera image plane is the projection plane where the three-dimensional point of the imaged scene is transformed with a non-linear transformation. The camera image plane is a two-dimensional plane that is orthogonally coincident with the optical axis of the lens. 

The camera image plane has its own coordinate system, namely $(x_i,y_i,z_i)$ coordinate system. We shall call the coordinate system a 

\subsection{Image Affine Coordinate System}
\label{subsection:image_affine_coordinate_system}

The affine coordinate system represents the shear (skew) and rescale aspects of the image Euclidean coordinate system \cite{Sonka07}. Shear and rescale options are usually called the aspect ratio of the image.  

The reason to introduce the image affine coordinate system is that pixels are not necessarily perfectly perpendicular. The correction for pixel dimension distortion is needed for applications that need high accuracy in measurements.

The image affine coordinate system introduces axes $u_i$, $v_i$, and $w_i$ that are not orthogonal. Axes v and w coincide with the camera coordinate system axes Y and Z, but axis u does not coincide with axis X, but is tilted. 




%end of chapter 5
%===============================================================================================================================
\chapter{Implementation}
\label{chapter:implementation}

The programming work done for the thesis was carried out during March 2013 to February 2014. External C++ libraries, namely Boost, OpenCV by Willow Garage, and PointCloudLibrary (PCL) were used in addition to the standard C and C++ libraries. The versions of the libraries are discussed later, but it is good to know that one of the biggest changes during the development of the thesis project was the moving from earlier versions of the PCL library to the PCL version 1.7 in August 2013. This on one hand helped the project onwards with simplified development effort, and on the other hand, made development more difficult with less IDE functionality and less ROS support than before.

Mainly OpenCV and PointCloudLibrary were used for tools programming, and the


\section{Overall System Design}
\label{section:overall_system_design}

\subsection{Use Cases}
\label{subsection:use_cases}

The software component design in this thesis was planned modular so that different parts of the measurement, e.g. stereo rectification and object segmentation, can be developed independently. The full system design will consist of many sequential components, some of which will be designed and implemented by other researchers in the FAMOUS research project.

\subsubsection{Use Case Configurations}
\label{subsubsection:use_case_configurations}


Four different use case configurations (UCC) are considered for the evaluation of the machine vision system implemented. Different use case configurations are titled with a paired numbering scheme, e.g. 1-1 or 2-1. The use case configurations will describe the overview of installed hardware, used parameters, and output data. Additionally, each UCC has very different advantages and drawbacks, which will be discussed in brief detail.

The first use case configuration UCC 1-1 had a process crane setup where the stereo camera rig is attached to the crane trolley with cameras facing down. The crane trolley can move freely with cameras attached, and the camera image will show the crane load approximately in the same location at all times in the image center. Hoisting the load up will not decenter the crane load showing in the output image. Instead, the crane load will be shown enlarged.

The second use case configuration UCC 1-2 had a process crane setup where the stereo camera rig is attached to the extreme end of the crane bridge beam on the left. The cameras are facing the load when the trolley is located near the left end of the crane bridge and the load is hoisted half way. Hoisting the load up will show the load moving towards the upper part of the output image. Hoisting movement does not enlarge the load,  instead driving the crane bridge trolley near the cameras will enlarge the load image in the output image.


The third use case configuration UCC 2-1 had a small telescopic crane installed on a forestry machine with a log lifting tool. The cameras are attached on the right side of the forestry machine chassis. The cameras are facing the load when the load is hoisted off of the ground behind the machine. The working area of the telescopic crane is occluded on the left side of the telescopic boom. Thus, the crane load will be occluded when the boom is actuated to the leftmost visible working area. 

In this use case the crane load shape is considered always cylindrical.

The fourth use case configuration UCC 2-2 had a small telescopic crane installed on a forestry machine with a log lifting tool. The cameras were attached on top of the operator cabin slightly to the right side of the telescopic crane boom . The cameras are facing the crane working area, and the crane boom is mostly visible in the output image. The log lifting tool at the end of the boom is mostly visible. The crane load is mostly visible, but partly occluded by the telescopic boom. 

In this use case the crane load shape is considered always cylindrical.

\subsection{Online Software}
\label{subsection:online_software}

\subsection{Offline Software}
\label{subsection:offline_software}

\subsubsection{End Effector Tracker Integration}
\label{subsubsection:end_effector_tracker_integration}

Haar training is an object detection system that can detect features in a digital image. The first suggestion of using Haar training for feature detection was by Viola et al in 


There are a number of different boosting algorithms available, 
+ Discrete Adaboost
+ Gentle Adaboost 
+ Real Adaboost

All the mentioned boosting algorithms are identical in classification sense, but they differ in their parameter learning algorithms \cite{Lienhart03}.

A cascade of classifiers is a decision tree structure which is constructed out of simple classifiers called stage classifiers. The cascade of classifiers results in a positive detection of a feature if the test image can pass all the stages of the cascade.


Accordingly, we may estimate the probability for a false alarm rate given the probability of a single weak classifier detection and the number of stages.

\formula{false_alarm_rate}

The cascade of classifiers can be constructed to enable feature scaling and scale-invariant detection. If the test image is re-scaled, the computation still works in constant time since a look-up table can be scaled accordingly and used in the computation. The downside of feature scaling is the loss of accuracy due integer rounding. The algorithm is sensitive to rounding errors and the result can vary severely.


+ 20 stages used in \cite{Lienhart03}

\section{HIMMELI Sensor Platform}
\label{section:himmeli_sensor_platform}


HIMMELI sensor platform is a modular sensor rig that was created in Aalto university department of automation and systems technology in a research project in 2012. The sensor platform was created for use in future automated machinery research and its purpose is to provide flexible sensing of environment with multiple sensors.

With installed stereo cameras, HIMMELI sensor platform provides means for creating a digital model of the environment, including the model of the crane load that we wish to measure. This is done using stereo imaging that outputs a range image of the crane working area. The area that the rig can model depends on the tilt of the cameras used, distance from the working area, and many other parameters. 

Two main reasons for choosing machine vision as the medium were the limitations of the crane environment instrumentation possibilities, and the cost-effectiveness of a camera system. With HIMMELI, we have the option to use laser scanning, too, so it was a natural choice as a camera platform for testing and evaluation. 

%The very first problem we had was that the boom in the forestry crane cannot be sensorised. Or so the John Deere representatives say. %The sensor life-time expectancy is low in the vibrating environment, and the space for installing any sensors is very limited.

The rig consists of three modules that can be used separately or together. Most cranes have little available space to install anything,  so the smaller the installed unit the better. The largest frame on the backside of the rig is the computation unit, including two PC computers and a control cabinet. In front, the upper module includes stereo cameras, HD camera, and other cameras. The lower module has an IR light source.

%\begin{figure}[ht]
%  \begin{center}
%    \includegraphics[width=\textwidth]{himmeli.JPG}
%    \caption{HIMMELI sensor platform prepared for measurement session.}
%    \label{fig:himmelifull}
%  \end{center}
%\end{figure}

The sensor rig can be powered with a 230 V mains power or alternatively with a 24 V DC source. We do not consider the details of powering up the HIMMELI rig since the power-up sequence and details are irrelevant for the end user.

\subsection{HIMMELI Platform Cameras}
\label{subsection:himmeli_platform_cameras}

The HIMMELI sensor platform carries two industrial-grade GigE uEye UI-5120 RE HDR cameras in its camera module. The uEye camera is a high dynamic range camera with a CMOS sensor array in PAL resolution 768 x 576 pixels. The model currently in use (UI-5120RE-M-GL Rev.2) carries a monochrome sensor that outputs the image at 50 frames per second maximum. 

The camera weights 175 grams, which means it is easy to install in small space, but also the image quality may suffer in vibrating environments due to non-existent passive damping properties of the camera hull. The camera is installed with a shielding IP65/67 lens tube so outdoor operation even in winter condition is not a problem. All the components of the camera comply to the IP65/65 class, so with splashproof and dustproof properties the camera is well-suited for industrial imaging purposes.

The data is transmitted through Gigabit Ethernet with Opto I/O interface.

\subsubsection{Image Array Details} 
%remove section

NIT CMOS Array

The CMOS sensor is manufactured by Northern Institute of Technology Management and has an optical size of 5.75 mm x 7.68 mm. It has a color depth of 12 bits 

The dynamic range of the camera is logarithmic with a 120dB sensitivity. \cite{IDSImagingWeb13}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=5cm]{camera-gige-ueye-se-cmos-1.jpg}
    \caption{The GigE uEye HDR camera with no protective casing. Image from \cite{IDSImagingWeb13}.}
    \label{fig:gige-camera}
  \end{center}
\end{figure}

\subsection{HIMMELI Control Cabinet}
\label{subsection:himmeli_control_cabinet}

The control cabinet includes 11 different components that are listed briefly in table \ref{table:controlcabinet}.

\begin{table}
\caption{Control cabinet components in the HIMMELI sensor rig.}
\label{table:controlcabinet}
\begin{tabular}{|p{6cm}|p{5cm}|}
\hline
\textbf{Component} & \textbf{Details} \\
\hline
ATX power supply x 2 & One for each embedded PC. \\
\hline
Embedded PC 1 & Operating system Windows 7 \\
\hline
Embedded PC 2 & Operating system GNU/Linux
\hline
Gigabit Ethernet switch & \\
\hline
Power supply switch & 230VAC/24VDC \\
\hline
Fuse box & \\
\hline
Capacitor & For power change-over usage. \\
\hline
Wireless module & \\
\hline
Power ground box & \\
\hline
Selector relay & For power change-over usage. \\ \hline
\end{tabular}
\end{table}

in order to provide a flexible sensory platform for research 

is currently readily available for recording stero image data, and is located in Tampere university of technology 



The HIMMELI sensor rig is used to collect data from the environment. It can sense environment from a distance with the installed cameras, and sense the orientation of the platform with its inertial measurement unit, IMU.

HIMMELI includes

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item HDR stereo camera pair
\item High resolution camera
\item 3D LIDAR laser scanner \url{http://velodynelidar.com/lidar/hdlproducts/hdl32e.aspx}
\item Infrared (IR) camera
\item Automotive radar
\end{itemize}

Additionally, an IR light source and LED flash are included.

\begin{table}
\caption{Device and its connector type in HIMMELI sensor rig.}
\label{table:connectortypes} %lable just after caption
\begin{tabular}{|p{6cm}|p{5cm}|}
\hline % The line on top of the table
\textbf{Device} & \textbf{Connector Type} \\
\hline
High dynamic range camera & Gigabit Ethernet \\
\hline
High resolution camera & IEEE 1394b (FireWire) \\
\hline
Automotive radar & CAN bus \\
\hline
Thermal camera & CVBS (PAL) or serial bus \\
\hline
LIDAR laser scanner & LIDAR \\
\hline
GNSS receiver & Serial bus 1 \\ \hline
\end{tabular} % for really simple tables, you can just use tabular
\end{table} % table makes a floating object with a title

The rig has computing capability with an embedded Linux PC that is mainly used for data routing and sensor fusion. The rig can be remotely operated via a WLAN network.

The 3D LIDAR sensor is a Velodyne HDL-32 model that scans the environment with 32 micromirror controlled lasers.

The stereo camera setup installed on the rig has two HDR cameras.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{himmeli.pdf}
    \caption{HIMMELI Sensor Platform Schematic.}
    \label{fig:himmeli}
  \end{center}
\end{figure}

\subsection{Camera Platform Installations}
\label{section:platform_installations}

The HIMMELI platform was installed in two different test environments as described in the use cases. 

\subsubsection{Forestry Crane Installation}
\label{subsubsection:forestry_crane_installation}

\subsubsection{Process Crane Installation}
\label{subsubsection:process_crane_installation}

The process crane had two options for a HIMMELI sensor platform installation. For both options, the different modules of the platform had to be installed in their own location because of size constraints. Thus, the sensor module and the support frame module were taken apart from the main PC frame for the duration of the process crane installation. 

The first installation option was to attach the sensor module to the side of the moving bridge using a custom made metal frame. In this installation option the camera units are facing towards the crane working area from side and above in a bird's eye view manner. The laser scanner rotation axis points towards the opposite end of the moving bridge, aligning the laser scan lines along the longer length of the warehouse space (x axis of the warehouse coordinate system).

%\begin{figure}[ht]
%  \begin{center}
%    \includegraphics[width=\textwidth]{himmeli_installation_11.png}
%   \caption{Sensor platform installation in the moving bridge of a process hall crane in online test session.}
%    \label{fig:himmeli_installation_11}
% \end{center}
%\end{figure}

The second installation option was to attach the sensor module directly to the moving crane trolley housing, see figure \ref{fig:himmeli_installation_12}. In this installation option the camera units are facing the crane working area in a top-down manner with a slight angle to the warehouse floor normal. Now, the laser scanner rotation axis points towards the floor, aligning the laser scan lines along the shorter length of the warehouse space (y axis of the warehouse coordinate system).

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{himmeli_installation_12.jpg}
   \caption{Sensor platform installation in the crane trolley of a process hall crane in online test session in March 2014.}
    \label{fig:himmeli_installation_12}
 \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{himmeli_modules.jpg}
   \caption{HIMMELI platform modules: a PC module, a support frame module, and the sensor module.}
    \label{fig:himmeli_modules}
 \end{center}
\end{figure}

\section{Point Cloud Library}
\label{section:point_cloud_library}

Point Cloud Library, or shortly PCL, is a fully templated modern C++ library for 3d point cloud processing purposes. 
PCL uses optimizations such as Intel SSE with Eigen library backend, GPU CUDA processing, and parallel programming in order to maximize the computation efficiency on specific computation platforms.

\subsection{Limitations of Point Cloud Computing}
\label{subsection:limitations_of_point_cloud_computing}

\section{Robot Operating System}
\label{section:robot_operating_system}

Robot Operating System, or shortly ROS, is an open source operating system-like framework targeted specifically for robotics application use. It was originally developed by Stanford AI laboratory in 2007 and the support is currently continued by Willow Garage Inc., who released the latest ROS Hydro release in September 2013. ROS is structured with a hierarchy of nodes, packages, stacks (legacy), and community-supported repositories, who provide the user with a large code base of readily available robotic applications. ROS system can also utilize a heterogenous computation network which is administered using a master-slave network setup.

ROS is mainly used in this thesis for its excellent capability of networked messaging through topics and subscribers. With a little effort we may use readily available data types for point cloud data networking, and interaction between popular operating system platforms, such as Windows and Unix environments. Data can be published via topics if we wish to disseminate it in the ROS network without receiving any feedback. In case we need feedback whether someone uses the data or not, ROS provides with a server-client scheme. 

ROS also includes a multitude of tools that can be used for data traffic verification.

\subsection{ROS I/O}
\label{subsection:ros_io}

\subsection{ROS Naming Conventions}


The current naming convention is meant to protect from colliding topic, node, and parameter names. 

\subsection{Parameter Server}
\label{subsection:parameter_server}

ROS parameter server is a cross-platform shared dictionary service that enables parameter retrieval at runtime in a ROS network. It is one of the advantages of using ROS since it can be used for reliably configuring a multi-device network. The parameter server is implemented usig XMLRPC libraries in the ROS master node.

The data types supported by the XMLRPC library are supported in the ROS parameter server.
\begin{table}
\caption{Supported data types of the ROS parameter server.}
\label{table:parameterserver}
\begin{tabular}{|p{7cm}|p{3cm}|}
\hline
\textbf{Java Type} & \textbf{XML Tag Name} & \textbf{Description} \\
\hline
Integer & int & 32-bit signed integer, non-null \\
\hline
Boolean & boolean & 0 or 1, non-null \\
\hline
String & string & A string, non-null\\
\hline
Double & double & A 64-bit signed floating point number, non-null \\
\hline
java.util.Date & dateTime.iso8601 & A ISO860 timestamp with milliseconds and time zone information missing \\
\hline
java.util.List & array & An array of objects \\
\hline
java.util.Map & struct & Key-value pairs \\
\hline
byte[] & base64 & Base64-encoded byte array \\ \hline
\end{tabular}
\end{table}

Setting parameters in the ROS parameter server is done using an XML launch file when launching a ROS node. All the variables are evaluated before launching any nodes, and all information is uploaded to the parameter server before launching the nodes. We can force certain data types for the parameters if the data types are not unambiguos. Supported types in the launch file are \emph{str}, \emph{int}, \emph{double}, and \emph{bool}. Additionally, we may set parameters in child namespaces or fully upload the contents of a parameter file to the parameter server as text or binary.

\subsection{Other Features}
\label{subsection:other_features}

+ dynamic_reconfigure
+ rxlogger

\section{Camera Calibration Toolbox}
\label{section:camera_calibration_toolbox}

\chapter{Computation}
\label{chapter:algorithms}

\section{Bounding Volume Computation}
\label{section:bounding_volume_computation}
% oriented bounding box tree => find minimum bounding box, cut with a plane against main axis, find more minimum bounding boxes

%Keywords: bounding volume, collision detection, urban simulation, AABB (axis-aligned bounding box)

Bounding volume is the main product we wish to generate out of all the computation done in the PCL library previously. The format of the information output from the underlying software depends on the user, but we can present some suggestions for standard solutions. 

Usually, a volume of an arbitrary object is presented as a linear combination of simple geometric objects. These prototypes of geometric  objects support some or all of the linear operations available, such as summation, subtraction, multiplication and division.

One of the simplest bounding volume outputs is an axis-aligned bounding box, or AABB, applied in 3 dimensional Euclidean space. AABB is the minimum perimeter along the directions of the axes that span the Euclidean space that fully contain the target object. AABB is straightforward to find by computing minimae and maximae along each axis and by spanning a convex polyhedron so that we select all minimum values in one corner of the polyhedral graph, and we select all maximum values for the opposing farthest possible corner of the polyhedron.

AABB computation can be easily optimized, but the biggest drawback of this technique is the large amount of empty space not occupied by the object that the AABB volume contains. In the special case of stick-like objects, or planar objects, whose orientation is maximally off-axis aligned, an AABB volume representation will show a volume that is occupied less than 10 \% by the actual object.

Another simple solution is an oriented bounding box. If we first specify a main longitudal axis for an object, and then calculate its orientation, we can iterate the smallest possible bounding volume that is oriented along the object axis. This will tremendously help to more accurately represent a volume of special shaped objects, such as cylinders and planar items.

We can find more advanced bounding volume techniques for even more accurate object volume representation, such as bounding volume shapes, bounding volume hierarchies (BVH), discrete oriented DOPs, k-DOPs and boxtrees. For example, BVH can be used to detect collisions, or object interference, and it is computed using raytracing and culling. 


\subsection{Bounding Volume From 3d Axis-aligned Bounding Box}
\label{subsection:bounding_volume_from_3d_axisaligned_bounding_box}

+ possibly previously unknown object who is suspended from the crane end effector tool. The form of the output information should be simple while still carefully thought in order to 


It is computationally expensive to update the AABB bounding box as the load orientation changes via rotation, and it is suggested that in order to fight excessive computation, the bounding box should be loosely defined so that with small angle changes there is no need for recomputation \cite{Ericson05}.

\subsection{Bounding Volume From 3d Oriented Bounding Box}
\label{subsection:bounding_volume_from_3d_oriented_bounding_box}

\section{Point Cloud Library Operations}
\label{section:point_cloud_library_operations}

\subsection{Object Segmentation}
\label{subsection:object_segmentation}
+ Euclidean Cluster Extraction

% the used scale of the point cloud point entries does have an effect on the computation time
% a computation processed using meters is much faster than computation on the same point cloud represented in micrometers, a million times larger values

\subsection{Point Cloud Filtering}
\label{subsubsection:point_cloud_filtering}

+ Voxel Grid Filtering
+ Outlier Removal

\subsection{Geometric Parameter Estimation}
\label{subsubsection:geometric_parameter_estimation}

Geometric parameter estimation, or geometric fitting, is a technique for estimating parameter models in unordered data sets. The idea in geometric parameter estimation is to fit a model of a geometric primitive shape to an experimental data set. The selected parameter model is usually a plane or a cylinder, or some other geometric primitive whose shape is easy to present in parameterized format. The existence of the shape in the data set is tested by finding inliers and outliers that determine whether the data set supports the model or not.

Classic parameter estimation techniques, such as least squares minimization, effectively optimize the fit of the selected parameter model using all points in the data set. In 1981, Fischler and Bolles presented a new parameterized model estimation technique called random sample consensus (RANSAC) that can improve upon classic techniques by detecting outlier points in the data, and take into account the gross errors they induce to the fitting algorithm. \ref{Fischler81}

While classic parameter estimation techniques incorrectly assume that most outliers are smoothed out in favor of good results, the RANSAC technique initializes the inlier point search with a small initial data set, and accumulates inliers when consistent data points are found. This way, not all data set points are used in the geometric fitting computation, and the solution is found easily even when gross errors are present in the data set.

Originally, the RANSAC algorithm was used to solve the location determination problem (LDP), which states that for a set of control points whose 3d location is known, a location of the camera is computed when a sufficient set of control points are visible in the projected image taken from the unknown location. The LDP problem is actually the camera calibration problem, which was presented in chapter \ref{subsection:single_camera_calibration}. RANSAC can solve the external location of the camera according to landmarks seen in the image, but in this thesis RANSAC is used to find the ground planes from the geometric data sets. If the ground plane data points are located correctly and removed from the point clouds, then the remaining points are parts of visible objects in the scene. 

Some variants of the RANSAC algorithm are 
\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item M-estimator Sample Consensus (MSAC)
\item Maximum Likelihood Estimator Sample Consensus (MLESAC) 
\item Progressive Sample Consensus (PROSAC)
\end{itemize}

but there are many more depending on the estimator technique used. When RANSAC algorithm produces poor results in parameter estimation application, other variants can be used if they are more suited for the data set. Many options can be found in the literature. \ref{Torr00} \ref{Huber81}

\section{Algorithms}
\label{section:algorithms}

\subsection{Load Selection Algorithm}
\label{subsection:load_selection_algorithm}

\subsection{Segmentation Center Selection Algorithm}
\label{subsection:segmentation_center_selection_algorithm}

\subsection{Cylinder Growing Algorithm}
\label{subsection:cylinder_growing_algorithm}


\chapter{Evaluation}
\label{chapter:evaluation}
%Pohdintaa ja future workia

Evaluation of the load object measurement system uses captured offline data. Online tests could not be run because the data from the HIMMELI platform at the time of online testing (March 2014) was not functional. The problems with online testing will be described in more detail in x

Since the work on load pose measurement system was started, ROS was considered as the number one framework choice in terms of cross-platform communication. It was also known that the learning curve for the ROS system is steep, so one risk in accomplishing a working online measurement system was actually missing knowledge on setting a ROS system up. Additionally, different versions of ROS distributions are not cross-compatible in the same ROS network. That means we need to decide upon a single ROS version which we are using. This is also a problem, because it is easiest to run PCL on a ROS Hydro distibution, where the other parts of the software (e.g. the TLD tracker) runs on older ROS Fuerte distribution. The old software component code base may be difficult to upgrade, so quite probably we will use ROS Fuerte and run older versions of PCL on it.   

\section{Data Quality in the Machine Vision Process}
\label{section:data_quality_in_the_machine_vision_process}

\subsection{Available Metrics}
\label{subsection:available_metrics}

\subsection{Point Cloud Segmentation Statistics}
\label{subsection:point_cloud_segmentation_statistics}


\section{Parameter Tuning Evaluation}
\label{section:parameter_tuning_evaluation}

\subsection{Disparity Tuning}
\label{subsection:disparity_tuning}

The disparity map generated with standard OpenCV functions needed parameter tuning to provide the best possible reprojection of geometric data from a 2d image to a 3d point cloud. The basic tuning of the block matching algorithm (BM) parameters was done first, and then a semi-global block matching algorithm (SGBM) was tuned with additional parameters. The tuning was done with a custom software tool that displays slider ranges for each parameter. The tool was made for what you see is what you get (WYSIWYG) operation, and the resulting disparity image is displayed instantaneously next to the sliders.

\subsubsection{Block Matching Algorithm Tuning}
\label{subsubsection:block_matching_algorithm_tuning}

The block matching algorithm is tuned with 10 parameters who reside in a BM state object. The parameters are listed in table \ref{table:block_matching_state_parameters}. Additionally, a pair of regions of interest (ROI) are set up in the BM state to mark off the new effective image regions after rectification process. To be able to understand the parameter effects on the block matching performance, a quick note about the inner works of the algorithm is introduced next.

The feature matching works in three steps:

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item Pre-filtering to normalize image brightness
\item Correspondence search with SAD window along the epipolar lines 
\item Post-filtering to remove bad matches
\end{itemize}

The tuning of the BM parameters was done against offline data collected from both selected use cases. The forestry crane data was preferred in the tuning effort since more objects can be seen in the images, thus, the saliency content of the captured video is higher compared to the process hall crane area. High saliency image regions produce good results in block matching, and a rule of thumb would be: more visible 2d objects result in more correctly matched points in the region for the 3d point cloud. For example, the texture rate of change for a matt painted warehouse floor is small, and in the process hall crane case the collected data had minimal floor feature matching response for some BM tuning sets.

\subsubsection{Block Matching Parameter Effects on Feature Matching Performance}
\label{subsubsection:block_matching_parameter_effects_on_feature_matching_performance}

First, the tuning was started with a coarse selection of parameters to get a visible disparity map. Especially, the SAD window size was kept higher than normal to produce a lot of visible matched points. The most important parameters in the final BM state tuning set are SAD window size, number of disparities, and speckle range parameters. These parameters should be tuned first, and other parameters can be used for fine tuning of the disparity map afterwards. 

The SAD window size, or sum of absolute differences window size, affects the size of the sliding kernel window used for correspondence search in the 1d epipolar search space of the rectified images. The smaller the value, the smaller the sliding smoothing kernel used is, and with a minimum value the search is called pixel-to-pixel correspondence search. For metric measurement purposes the disparity map should be as accurate as possible, thus, the minimum value of the SAD window size will be used at all times. If the rectified images are texture rich, then a match can be found for all pixels on an epipolar line. Then again, if the image pair is less rich in texture, for example, a matt painted warehouse floor, the process will find less matches, which usually is the case. A low texture environment will produce little found matches because the local neighbourhoods of the sliding window are too similar in order to confirmed a match. If the point cloud is used for 3d visualization purposes, a higher SAD window size value selection should produce more points for the delight of the viewer.

The number of disparities parameter sets the maximum disparity difference in pixels for a search space in a disparity map. The higher the value, the higher a pixel difference value is allowed in the disparity map. For example, if a very near-field object has a disparity difference of N pixels, and number of disparities property is set smaller than N, then the near-field object is not detected at all in the disparity map leaving a 'hole' in it. This property can be used to remove near-field occluded objects that are impossible to triangulate successfully. For example, in the process hall crane case, the hoisting wires are problematic since they are occluded and visible only from one side in each camera. With number of disparities parameter it is possible to remove the wires from the disparity search space, and altogether from the final point cloud depicting the crane working area. 

The wires are included in the disparity search from certain height onwards, but with a proper selection of number of disparities it is possible to minimize the erroneous reprojection of the hoisting mechanism in the final output point cloud. The user can use the number of disparities property in combination with the minimum disparity property for setting the horopter - the 3d volume that is covered by the stereo matching algorithm \ref{Bradski08}.

Horopter can be enlarged towards the camera image plane with changes in the stereo system parameters. Some changes that enlarge the horopter are decreasing the stereo camera baseline width, decreasing the focal length of the cameras, and increasing the disparity search space.

Speckles are high and low disparity patches generated by the block matching algorithm near object boundaries. The block matching sliding window will catch object foreground and background in the image pair, which is the problem that generates speckle in the disparity map. Thus, the third important parameter is the speckle range parameter. The speckle range controls the threshold for letting the local patches of speckle be matched to the resulting disparity map. If the threshold is met, then the local region is eliminated and no disparity matches will be available for that region. The speckle detector that uses the speckle range value also requires the speckle window size parameter. The value for the window size should be small enough to keep the computation to a minimum, but large enough to detect speckle regions properly.

For each use case, multiple tuning sets were designed, which can be visually inspected by viewing at the point cloud output quality. On one hand, some processes will not work with a small number of output points, such as the RANSAC iteration for a ground plane search. On the other hand, the metric accuracy of the point cloud points decrease as the number of points in the cloud increase. The resulting point cloud from stereo cameras is always a best possible trade-off between accuracy, quality, and size of the cloud. Since the amount of point cloud points heavily affects the time used for further processing of the cloud, accuracy is the number one priority decreasing the computation times simultaneously.

Over time, the quality of the bounding volume measurement depends on many things: primarily the amount of noise in the point cloud object over time, and secondarily on the orientation of the object over time. 

\begin{table}
\caption{List of block matching parameters in a BM state object.}
\label{table:block_matching_state_parameters} %label just after caption
\begin{tabular}{|p{4cm}|l|l|p{4cm}|}
\hline % The line on top of the table
\textbf{Parameter Name} & \textbf{Min.} & \textbf{Max.} & \textbf{Requirements} \\
\hline
Pre-Filter Size & 5 & 21 & Odd Number \\
\hline
Pre-Filter Cap & 1 & 63 & - \\
\hline
SAD Window Size & 5 & 255 & Odd Number \\
\hline
Min. Disparity & -100 & 100 & - \\
\hline
No. Of Disparities & 16 & 256 & Divisable By 16 \\
\hline
Texture Threshold & 0 & no upper limit & - \\ 
\hline
Uniqueness Ratio & 0 & 255 & - \\
\hline
Speckle Window Size & 0 & 100 & - \\
\hline
Speckle Range & 0 & 100 & - \\
\hline
Disp12MaxDiff & 0 & no upper limit & - \\ \hline
\end{tabular} % for really simple tables, you can just use tabular
\end{table} % table makes a floating object with a title

The tuning effort resulted in best possible BM parameters for each use case. The process hall crane parameters are listed in table \ref{table:process_hall_crane_bm_parameters}

\begin{table}
\caption{Best BM parameter tuning set in process hall crane case.}
\label{table:process_hall_crane_bm_parameters} %label just after caption
\begin{tabular}[c]{|p{4cm}|l|}
\hline % The line on top of the table
\textbf{Parameter Name} & \textbf{Value} \\
\hline
Pre-Filter Size & 9 \\
\hline
Pre-Filter Cap & 63 \\
\hline
SAD Window Size & 5 \\
\hline
Min. Disparity & 0 \\
\hline
No. Of Disparities & 96 \\
\hline
Texture Threshold & 270 \\ 
\hline
Uniqueness Ratio & 20 \\
\hline
Speckle Window Size & 42 \\
\hline
Speckle Range & 10 \\
\hline
Disp12MaxDiff & 10 \\ \hline
\end{tabular} % for really simple tables, you can just use tabular
\end{table} % table makes a floating object with a title

\begin{table}
\caption{Best BM parameter tuning set in forestry crane case.}
\label{table:forestry_crane_bm_parameters} %label just after caption
\begin{tabular}[c]{|p{4cm}|l|}
\hline % The line on top of the table
\textbf{Parameter Name} & \textbf{Value} \\
\hline
Pre-Filter Size & 9 \\
\hline
Pre-Filter Cap & 43 \\
\hline
SAD Window Size & 13 \\
\hline
Min. Disparity & 0 \\
\hline
No. Of Disparities & 160 \\
\hline
Texture Threshold & 0 \\ 
\hline
Uniqueness Ratio & 21 \\
\hline
Speckle Window Size & 44 \\
\hline
Speckle Range & 17 \\
\hline
Disp12MaxDiff & 1 \\ \hline
\end{tabular} % for really simple tables, you can just use tabular
\end{table} % table makes a floating object with a title

\subsubsection{Semi-global Block Matching Algorithm Tuning}
\label{subsubsection:semiglobal_block_matching_algorithm_tuning}




\subsection{Segmentation Tuning}
\label{subsection:segmentation_tuning}

\section{Offline Tests}
\label{section:offline_tests}

\subsection{Process Hall Case}
\label{subsection:offline_process_hall_case}
%note labeling

\section{Load Object Modeling Quality}
\label{section:load_object_modeling_quality}

%fix passive
We are not required to fully model the load object in order to find its position and a bounding volume. 

Currently the produced point cloud only depicts the visible part of the load object since a normal stereo camera setup cannot generate a full envelop view of the object.

If we wanted to fully model the load object then additional camera viewpoints would be needed that cover all the occluded and non-visible load object surfaces, such as the backside. With a full stereo camera envelop we are able to reconstruct the structure using any available reconstruction algorithm. 

An interesting idea would be to use an adaptive reconstruction scheme that statistically models the object surface with a self-organizing map as described in de Medeiro's study\cite{Medeiros07}. For such a statistical modeling approach a normalized load object measurement for each discrete timestep would be needed as an input for the SOM model.

It would be possible to do surface reconstruction if we can access additional viewpoints and envelop the load object fully 


using adaptive iteration techniques in order to model the object in greater detail as we receive more information from other viewpoints, including its backside.


statistically estimate and model the object in time using self-organizing maps and adaptive geometric meshing .


The greatest challenge in computing the load object measurements and position is the low quality of the point cloud data received from the stereo camera rig. Due to limitations in e.g. block matching algorithm, filtering kernels, and changing environmental variables (such as lighting conditions) we may expect white noise to be present in the data for all discrete timesteps.

Countering the varying geometric signal is difficult and a matter of discussion in itself: do we wish to add filtering that may possibly lower spatial resolution of the depicted scene in order to receive a filtered signal? 



Since we are interested in the load object AABB measurements we are not required to normalize coordinates for 


\subsection{Forest Case}
\label{subsection:forest_case}
%note labeling

\section{Online Usage Study}
\label{section:online_usage_study}

\subsection{Process Hall Case}
\label{subsection:online_process_hall_case}
%note labeling
Online tests for the FAMOUS project deliverables were conducted on March 5th, 2014 in a process hall crane test environment. 

\subsection{Forest Case}
\label{subsection:online_forest_case}
%note labelin

\section{Software Robustness In Tests}
\label{section:software_robustness_in_tests}

\chapter{Discussion}
\label{chapter:discussion}

\section{Machine Learning Extras}

Object recognition works on a classification principle: a classifier detects attributes in an object and works out into which object class it belongs to.

+ classification principle
    * statistical classifier
    
+ knowledge representations
    + syntactic pattern recognition
        + relations and description language
+ machine learning
    + neural nets
    + genetic algorithms
    
+ graph matching

+ fuzzy systems

%machine learning
A classifier is a concept of machine learning that is utilized in Haar training. A single classifier itself is a weak member of a more powerful committee of cascaded classifiers \cite{Freund96}. 

A number of simple, computationally light classifiers seem to outperform strong classifiers such as neural networks in principle \cite{Lienhart03}. 

In 2002, Lienhart and Maydt \cite{Lienhart02} proposed improvements upon the original work of the authors of Haar training classifiers. The Intel labs research team added a rotated feature classifier, which improves upon the performance of the original simple feature classifiers. One year later, they analysed different boosting algorithms in Haar training and concluded that Gentle Adaboost method performed the best in the training. 

The feature search space in Haar training is large in an image. The computational complexity depends on the image resolution, and on the chosen prototype of a feature.  For example, the number of features for a small window size of 24x24 is reported to total up to 117,941 features\cite{Lienhart03}. 

The computation of a single feature can be fast. The journal by... states that it can be done in constant time.

complexity for a single feature is constant. 

Currently, the 

+ 14 Haar-like features
+ 4 edge features, 8 line features, 2 center-surround features, 1 line diagonal feature
+ 

A lot of time was used for working on the ROS implementation of the load detection software. The software has a simple backbone implementation thanks to the well-documented readily available OpenCV and PCL libraries. Some of the code runs advanced algorithms underneath, such as RANSAC model iteration, but these are not visible for the user. It would have been possible to implement better algorithms for higher level functions, such as load detection computation and load geometric measurement computation, than what we are currently running on. Due to limited resources we decided to keep the algorithms simple and instead realised a concept software that will run with a supported camera setup. This way we got a working platform that can be used as a testbench for future upgrades and as a framework for implementing more advanced algorithms.  

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{taakka_tunnistettu1.png}
    \caption{Cylindrical load detection in the FAMOUS software using very low quality scenery input data.}
    \label{fig:load_detected1}
  \end{center}
\end{figure}

We lifted a tree trunk with a forestry machine crane and processed the video output with the FAMOUS software. A screenshot of the resulting output from the visualizer can be seen in Figure \ref{fig:load_detected1} where the parameters for stereo matching and 3D segmentation were not tuned optimally. The segmentation of the scene into objects turned out to be quite robust against missing data, fluctuating point clouds, and bad parameter values. The robustness is partly due to a good automatic stereo parameter functionality of OpenCV that was shamelessly used to obtain the stereo parameters. Other reasons for good robustness are the smart handling of uninitialized data in PCL library, and the software architecture that does not differentiate X, Y, and Z directions - no information about orientation of the scene is utilized in 3D segmentation. 
    Then again, we can generate more accurate information with optimal parameter sets, and the measurements from the load object will change for each timestep unless we use adaptive filtering to fight out the white noise and orientation differences in time. We have considered a Kalman filter that tracks the estimate of the actual bounding volume signal, but is was not implemented for simplicity.
    
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{taakka_tunnistettu2.png}
    \caption{Cylindrical load viewed from a virtual camera from across the crane working area.}
    \label{fig:load_detected2}
  \end{center}
\end{figure}

One problem present in Figure \ref{fig:load_detected2} was that the forestry crane boom belongs to the segmented load object after initial processing. We can see from the third viewport that the load object contains the crane boom, end effector tool, and the load itself. Normally we could assume that the load object is found fully under the end effector tool, but with use cases UCC2-1 and UCC2-2 this is not true. In Riihimäki tests, the tree trunks did easily reach above the end effector tool location when the operator picked them up so that they were tilted. Consequently, the bounding volume does reach above the end effector height, too. We decided to avoid accidental cutting of the load object after segmentation by not making any assumptions of the axis-wise load location other than it is located somewhere near the end effector tool for these 2 use cases. Instead, we opted to fit a cylinder object using RANSAC fitting, giving the approximate position and orientation for the cylindrical load as an output. 

Now, if the user wishes not to use the cylinder coefficients for pose measurements, and a crane-structure cutting AABB-based bounding volume is not a problem, then the AABB shown in Figure \ref{fig:load_detected2} can be used, too. The cylinder fitting implementation did pick up the cylindrical crane boom sometimes in testing, but we implemented a check for the condition that the load is located near the end effector tool location. With a little research, we found out that the cylinder fitting in a forestry crane machine vision system was not engineered previously in Aalto university. The advantage of the cylinder coefficients over the AABB bounding volume is that the oriented cylinder describes the actual volume and orientation of the tree trunk in much more detail compared to an AABB bounding volume, as discussed in chapter \ref{aabb_reference}. 

\section{OpenCV Extras}

The results achieved by image processing technologies have claimed a lot of interest in two major application areas.

First area is the enhancing of graphical content for human interpretation. In scientific image treatment it simply means that a noisy or corrupted original image can be treated in order to remove or diminish the corruption, resulting in useful content for human users to survey.

The second area is enabling autonomous machine perception. A machine can tirelessly inspect items on a product assembly line that would end up being dangerous, monotonous, or otherwise problematic for a human worker. Additionally, we may utilize the full electromagnetic spectral range in addition to visible light spectrum for quality control purposes.
 
Research effort in image processing technologies continues to result in better and faster algorithms for image enhancing.

\section{Other research}
+ Moving points in an image, and thus, objects, can be detected from photometric consistency in consecutive frames using discriminant analysis and image statistics (in Kanatani13)

%+ a great cylinder fitting scheme can be found in Pekka Forsman doctoral thesis

\chapter{Conclusions}
\label{chapter:conclusions}
%Write down the most important findings from your work.
%Like the introduction, this chapter is not very long.
%Two to four pages might be a good limit.

\chapter{References}
\label{chapter:references}

% Load the bibliographic references
% ------------------------------------------------------------------
% You can use several .bib files:
% \bibliography{thesis_sources,ietf_sources}
\bibliography{ref}


% Appendices go here
% ------------------------------------------------------------------
% If you do not have appendices, comment out the following lines
\appendix
% \input{appendices.tex}

\chapter{First appendix}
\label{chapter:first-appendix}

This is the first appendix. You could put some test images or verbose data in an
appendix, if there is too much data to fit in the actual text nicely.

For now, the Aalto logo variants are shown in Figure~\ref{fig:aaltologo}.

\begin{figure}
\begin{center}
\subfigure[In English]{\includegraphics[width=.8\textwidth]{aalto-logo-en}}
\subfigure[Suomeksi]{\includegraphics[width=.8\textwidth]{aalto-logo-fi}}
\subfigure[Pä svenska]{\includegraphics[width=.8\textwidth]{aalto-logo-se}}
\caption{Aalto logo variants}
\label{fig:aaltologo}
\end{center}
\end{figure}


% End of document!
% ------------------------------------------------------------------
% The LastPage package automatically places a label on the last page.
% That works better than placing a label here manually, because the
% label might not go to the actual last page, if LaTeX needs to place
% floats (that is, figures, tables, and such) to the end of the
% document.
\end{document}
