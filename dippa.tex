% Lines starting with a percent sign (%) are comments. LaTeX will
% not process those lines. Similarly, everything after a percent
% sign in a line is considered a comment. To produce a percent sign
% in the output, write \% (backslash followed by the percent sign).
% ==================================================================
% Usage instructions:
% ------------------------------------------------------------------
% The file is heavily commented so that you know what the various
% commands do. Feel free to remove any comments you don't need from
% your own copy. When redistributing the example thesis file, please
% retain all the comments for the benefit of other thesis writers!
% ==================================================================
% Compilation instructions:
% ------------------------------------------------------------------
% Use pdflatex to compile! Input images are expected as PDF files.
% Example compilation:
% ------------------------------------------------------------------
% > pdflatex thesis-example.tex
% > bibtex thesis-example
% > pdflatex thesis-example.tex
% > pdflatex thesis-example.tex
% ------------------------------------------------------------------
% You need to run pdflatex multiple times so that all the cross-references
% are fixed. pdflatex will tell you if you need to re-run it (a warning
% will be issued)
% ------------------------------------------------------------------
% Compilation has been tested to work in ukk.cs.hut.fi and kosh.hut.fi
% - if you have problems of missing .sty -files, then the local LaTeX
% environment does not have all the required packages installed.
% For example, when compiling in vipunen.hut.fi, you get an error that
% tikz.sty is missing - in this case you must either compile somewhere
% else, or you cannot use TikZ graphics in your thesis and must therefore
% remove or comment out the tikz package and all the tikz definitions.
% ------------------------------------------------------------------

% General information
% ==================================================================
% Package documentation:
%
% The comments often refer to package documentation. (Almost) all LaTeX
% packages have documentation accompanying them, so you can read the
% package documentation for further information. When a package 'xxx' is
% installed to your local LaTeX environment (the document compiles
% when you have \usepackage{xxx} and LaTeX does not complain), you can
% find the documentation somewhere in the local LaTeX texmf directory
% hierarchy. In ukk.cs.hut.fi, this is /usr/texlive/2008/texmf-dist,
% and the documentation for the titlesec package (for example) can be
% found at /usr/texlive/2008/texmf-dist/doc/latex/titlesec/titlesec.pdf.
% Most often the documentation is located as a PDF file in
% /usr/texlive/2008/texmf-dist/doc/latex/xxx, where xxx is the package name;
% however, documentation for TikZ is in
% /usr/texlive/2008/texmf-dist/doc/latex/generic/pgf/pgfmanual.pdf
% (this is because TikZ is a front-end for PGF, which is meant to be a
% generic portable graphics format for LaTeX).
% You can try to look for the package manual using the ``find'' shell
% command in Linux machines; the find databases are up-to-date at least
% in ukk.cs.hut.fi. Just type ``find xxx'', where xxx is the package
% name, and you should find a documentation file.
% Note that in some packages, the documentation is in the DVI file
% format. In this case, you can copy the DVI file to your home directory,
% and convert it to PDF with the dvipdfm command (or you can read the
% DVI file directly with a DVI viewer).
%
% If you can't find the documentation for a package, just try Googling
% for ``latex packagename''; most often you can get a direct link to the
% package manual in PDF format.
% ------------------------------------------------------------------


% Document class for the thesis is report
% ------------------------------------------------------------------
% You can change this but do so at your own risk - it may break other things.
% Note that the option pdftext is used for pdflatex; there is no
% pdflatex option.
% ------------------------------------------------------------------
\documentclass[12pt,a4paper,oneside,pdftex]{report}

% The input files (tex files) are encoded with the latin-1 encoding
% (ISO-8859-1 works). Change the latin1-option if you use UTF8
% (at some point LaTeX did not work with UTF8, but I'm not sure
% what the current situation is)
\usepackage[utf8]{inputenc}
% OT1 font encoding seems to work better than T1. Check the rendered
% PDF file to see if the fonts are encoded properly as vectors (instead
% of rendered bitmaps). You can do this by zooming very close to any letter
% - if the letter is shown pixelated, you should change this setting
% (try commenting out the entire line, for example).
\usepackage[OT1]{fontenc} %fontenc first, then inputenc if need be
% The babel package provides hyphenating instructions for LaTeX. Give
% the languages you wish to use in your thesis as options to the babel
% package (as shown below). You can remove any language you are not
% going to use.
% Examples of valid language codes: english (or USenglish), british,
% finnish, swedish; and so on.
\usepackage[british]{babel}


% Font selection
% ------------------------------------------------------------------
% The default LaTeX font is a very good font for rendering your
% thesis. It is a very professional font, which will always be
% accepted.
% If you, however, wish to spicen up your thesis, you can try out
% these font variants by uncommenting one of the following lines
% (or by finding another font package). The fonts shown here are
% all fonts that you could use in your thesis (not too silly).
% Changing the font causes the layouts to shift a bit; you many
% need to manually adjust some layouts. Check the warning messages
% LaTeX gives you.
% ------------------------------------------------------------------
% To find another font, check out the font catalogue from
% http://www.tug.dk/FontCatalogue/mathfonts.html
% This link points to the list of fonts that support maths, but
% that's a fairly important point for master's theses.
% ------------------------------------------------------------------
% <rant>
% Remember, there is no excuse to use Comic Sans, ever, in any
% situation! (Well, maybe in speech bubbles in comics, but there
% are better options for those too)
% </rant>

% \usepackage{palatino}
% \usepackage{tgpagella}



% Optional packages
% ------------------------------------------------------------------
% Select those packages that you need for your thesis. You may delete
% or comment the rest.

% Natbib allows you to select the format of the bibliography references.
% The first example uses numbered citations:
\usepackage[square,sort&compress,numbers]{natbib}
% The second example uses author-year citations.
% If you use author-year citations, change the bibliography style (below);
% acm style does not work with author-year citations.
% Also, you should use \citet (cite in text) when you wish to refer
% to the author directly (\citet{blaablaa} said blaa blaa), and
% \citep when you wish to refer similarly than with numbered citations
% (It has been said that blaa blaa~\citep{blaablaa}).
% \usepackage[square]{natbib}

% The alltt package provides an all-teletype environment that acts
% like verbatim but you can use LaTeX commands in it. Uncomment if
% you want to use this environment.
% \usepackage{alltt}

% The eurosym package provides a euro symbol. Use with \euro{}
\usepackage{eurosym}

% Verbatim provides a standard teletype environment that renderes
% the text exactly as written in the tex file. Useful for code
% snippets (although you can also use the listings package to get
% automatic code formatting).
\usepackage{verbatim}

% The listing package provides automatic code formatting utilities
% so that you can copy-paste code examples and have them rendered
% nicely. See the package documentation for details.
% \usepackage{listings}

% The fancuvrb package provides fancier verbatim environments
% (you can, for example, put borders around the verbatim text area
% and so on). See package for details.
% \usepackage{fancyvrb}

% Supertabular provides a tabular environment that can span multiple
% pages.
%\usepackage{supertabular}
% Longtable provides a tabular environment that can span multiple
% pages. This is used in the example acronyms file.
\usepackage{longtable}

% The fancyhdr package allows you to set your the page headers
% manually, and allows you to add separator lines and so on.
% Check the package documentation.
% \usepackage{fancyhdr}

% Subfigure package allows you to use subfigures (i.e. many subfigures
% within one figure environment). These can have different labels and
% they are numbered automatically. Check the package documentation.
\usepackage{subfigure}

% The titlesec package can be used to alter the look of the titles
% of sections, chapters, and so on. This example uses the ``medium''
% package option which sets the titles to a medium size, making them
% a bit smaller than what is the default. You can fine-tune the
% title fonts and sizes by using the package options. See the package
% documentation.
\usepackage[medium]{titlesec}

% The TikZ package allows you to create professional technical figures.
% The learning curve is quite steep, but it is definitely worth it if
% you wish to have really good-looking technical figures.
\usepackage{tikz}
% You also need to specify which TikZ libraries you use
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.pathmorphing,decorations.markings}
\usetikzlibrary{shapes}
\usetikzlibrary{patterns}


% The aalto-thesis package provides typesetting instructions for the
% standard master's thesis parts (abstracts, front page, and so on)
% Load this package second-to-last, just before the hyperref package.
% Options that you can use:
%   mydraft - renders the thesis in draft mode.
%             Do not use for the final version.
%   doublenumbering - [optional] number the first pages of the thesis
%                     with roman numerals (i, ii, iii, ...); and start
%                     arabic numbering (1, 2, 3, ...) only on the
%                     first page of the first chapter
%   twoinstructors  - changes the title of instructors to plural form
%   twosupervisors  - changes the title of supervisors to plural form
%\usepackage[mydraft,twosupervisors]{aalto-thesis}
%usepackage[mydraft,doublenumbering]{aalto-thesis}
\usepackage{aalto-thesis}


% Hyperref
% ------------------------------------------------------------------
% Hyperref creates links from URLs, for references, and creates a
% TOC in the PDF file.
% This package must be the last one you include, because it has
% compatibility issues with many other packages and it fixes
% those issues when it is loaded.
\RequirePackage[pdftex]{hyperref}
% Setup hyperref so that links are clickable but do not look
% different
\hypersetup{colorlinks=false,raiselinks=false,breaklinks=true}
\hypersetup{pdfborder={0 0 0}}
\hypersetup{bookmarksnumbered=true}
% The following line suggests the PDF reader that it should show the
% first level of bookmarks opened in the hierarchical bookmark view.
\hypersetup{bookmarksopen=true,bookmarksopenlevel=1}
% Hyperref can also set up the PDF metadata fields. These are
% set a bit later on, after the thesis setup.


% Thesis setup
% ==================================================================
% Change these to fit your own thesis.
% \COMMAND always refers to the English version;
% \FCOMMAND refers to the Finnish version; and
% \SCOMMAND refers to the Swedish version.
% You may comment/remove those language variants that you do not use
% (but then you must not include the abstracts for that language)
% ------------------------------------------------------------------
% If you do not find the command for a text that is shown in the cover page or
% in the abstract texts, check the aalto-thesis.sty file and locate the text
% from there.
% All the texts are configured in language-specific blocks (lots of commands
% that look like this: \renewcommand{\ATCITY}{Espoo}.
% You can just fix the texts there. Just remember to check all the language
% variants you use (they are all there in the same place).
% ------------------------------------------------------------------
\newcommand{\TITLE}{Crane Load Geometry Measurement using Machine Vision}
\newcommand{\FTITLE}{Taakan geometrian mittaus nosturista konenäköavusteisesti}
\newcommand{\DATE}{May 27th, 2014}
\newcommand{\FDATE}{27. toukokuuta 2014}
\newcommand{\SDATE}{Den 27 Maj 2014}

% Supervisors and instructors
% ------------------------------------------------------------------
% If you have two supervisors, write both names here, separate them with a
% double-backslash (see below for an example)
% Also remember to add the package option ``twosupervisors'' or
% ``twoinstructors'' to the aalto-thesis package so that the titles are in
% plural.
% Example of one supervisor:
\newcommand{\SUPERVISOR}{Professor Ville Kyrki}
\newcommand{\FSUPERVISOR}{Professori Ville Kyrki}
\newcommand{\SSUPERVISOR}{Professor Ville Kyrki}
% Example of twosupervisors:
%\newcommand{\SUPERVISOR}{Professor Ville Kyrki\\
%  D.Sc. Sami Terho}
%\newcommand{\FSUPERVISOR}{Professori Ville Kyrki\\
%  TkT Sami Terho}
%\newcommand{\SSUPERVISOR}{Professor Ville Kyrki\\
%  Dr. Sami Terho}

% If you have only one instructor, just write one name here
\newcommand{\INSTRUCTOR}{D.Sc. (Tech.) Sami Terho}
\newcommand{\FINSTRUCTOR}{Tekniikan tohtori Sami Terho}
\newcommand{\SINSTRUCTOR}{Teknologie doktor Sami Terho}
% If you have two instructors, separate them with \\ to create linefeeds
% \newcommand{\INSTRUCTOR}{Olli Ohjaaja M.Sc. (Tech.)\\
%  Elli Opas M.Sc. (Tech)}
%\newcommand{\FINSTRUCTOR}{Diplomi-insinööri Olli Ohjaaja\\
%  Diplomi-insinööri Elli Opas}
%\newcommand{\SINSTRUCTOR}{Diplomingenjör Olli Ohjaaja\\
%  Diplomingenjör Elli Opas}

% If you have two supervisors, it is common to write the schools
% of the supervisors in the cover page. If the following command is defined,
% then the supervisor names shown here are printed in the cover page. Otherwise,
% the supervisor names defined above are used.
\newcommand{\COVERSUPERVISOR}{Professor Ville Kyrki, Aalto University, School of Electrical Engineering}
% The same option is for the instructors, if you have multiple instructors.
% \newcommand{\COVERINSTRUCTOR}{Olli Ohjaaja M.Sc. (Tech.), Aalto University\\
%  Elli Opas M.Sc. (Tech), Aalto SCI}

% Other stuff
% ------------------------------------------------------------------
\newcommand{\PROFESSORSHIP}{Intelligent Robotics}
\newcommand{\FPROFESSORSHIP}{Älykäs Robotiikka}
\newcommand{\SPROFESSORSHIP}{Intelligent Robotteknik}
% Professorship code is the same in all languages
\newcommand{\PROFCODE}{T-110}
\newcommand{\KEYWORDS}{stereo vision, depth mapping, crane load, machine vision}
\newcommand{\FKEYWORDS}{stereonäkö, syvyyskartoitus, nosturin taakka, konenäkö}
\newcommand{\SKEYWORDS}{stereoseende, djuphet kartläggning, kran last, maskinseende}
\newcommand{\LANGUAGE}{English}
\newcommand{\FLANGUAGE}{Englanti}
\newcommand{\SLANGUAGE}{Engelska}
% Author is the same for all languages
\newcommand{\AUTHOR}{Erkka Mutanen}

% Currently the English versions are used for the PDF file metadata
% Set the PDF title
\hypersetup{pdftitle={\TITLE\ \AUTHOR}}
% Set the PDF author
\hypersetup{pdfauthor={\AUTHOR}}
% Set the PDF keywords
\hypersetup{pdfkeywords={\KEYWORDS}}
% Set the PDF subject
\hypersetup{pdfsubject={Master's Thesis}}

% Layout settings
% ------------------------------------------------------------------

% When you write in English, you should use the standard LaTeX
% paragraph formatting: paragraphs are indented, and there is no
% space between paragraphs.
% When writing in Finnish, we often use no indentation in the
% beginning of the paragraph, and there is some space between the
% paragraphs.

% If you write your thesis Finnish, uncomment these lines; if
% you write in English, leave these lines commented!
% \setlength{\parindent}{0pt}
% \setlength{\parskip}{1ex}

% Use this to control how much space there is between each line of text.
% 1 is normal (no extra space), 1.3 is about one-half more space, and
% 1.6 is about double line spacing.
\linespread{1.1} % This is the default
% \linespread{1.3}

% Bibliography style
% acm style gives you a basic reference style. It works only with numbered
% references.
\bibliographystyle{acm}
% Plainnat is a plain style that works with both numbered and name citations.
% \bibliographystyle{plainnat}


% Extra hyphenation settings
% ------------------------------------------------------------------
% You can list here all the files that are not hyphenated correctly.
% You can provide many \hyphenation commands and/or separate each word
% with a space inside a single command. Put hyphens in the places where
% a word can be hyphenated.
% Note that (by default) LaTeX will not hyphenate words that already
% have a hyphen in them (for example, if you write ``structure-modification
% operation'', the word structure-modification will never be hyphenated).
% You need a special package to hyphenate those words.
\hyphenation{di-gi-taa-li-sta yksi-suun-tai-sta}

% The preamble ends here, and the document begins.
% Place all formatting commands and such before this line.
% ------------------------------------------------------------------
\begin{document}
% This command adds a PDF bookmark to the cover page. You may leave
% it out if you don't like it...
\pdfbookmark[0]{Cover page}{bookmark.0.cover}
% This command is defined in aalto-thesis.sty. It controls the page
% numbering based on whether the doublenumbering option is specified
\startcoverpage

% Cover page
% ------------------------------------------------------------------
% Options: finnish, english, and swedish
% These control in which language the cover-page information is shown
\coverpage{english}
% Abstracts
% ------------------------------------------------------------------
% Include an abstract in the language that the thesis is written in,
% and if your native language is Finnish or Swedish, one in that language.

% Abstract in English
% ------------------------------------------------------------------
\thesisabstract{english}{

In the modern world, industrial goods are being moved more than ever, but only few industrial ventures rely on automatic movement of a product on the factory floor using overhead traveling cranes. The approach to measure moving goods in crane environments can enable new smarter automated features to overhead crane environments, such as route planning, collision detection, and collision avoidance features. The incentives for developing such features are increased goods handling safety, and the need towards more optimal energy consumption and longer crane life cycles.
    The 3d vision system selected for producing the measurement used passive stereographic triangulation to acquire the environment model as a disparity map. Known camera calibrations enabled the disparity data reprojection into a 3d point cloud model, which represented the scene seen by the cameras in metric coordinates. Finally, the point cloud was processed into a metric AABB measurement using Point Cloud Library (PCL).    
    An offline geometry measurement system was implemented using C++ libraries and Robot Operating System (ROS). A perception platform with two high dynamic range cameras was used. Different use cases were designed and analysed for electric overhead cranes and telescopic boom cranes in forestry applications. 
    The perception platform data from real crane environment produced a deterministic response with some noise in the load object measurement. 
    
    The measurement accuracy highly depended on the camera calibration and software parameters, and overall accuracy could not be verified.
    
    
    
    
    



The measurement describes an axis-aligned bounding box (AABB) of an arbitrary load object lifted with an electric overhead crane (EOT) or a small telescopic crane in outdoor environments. A load geometry measurement is used as the basis information in design of smart electrical overhead cranes that can detect collisions in their working environments. Furthermore, the 3d AABB measurement can be used in routing computation and in collision avoidance techniques.



The machine vision platform used was an earlier department project titled HIMMELI and it carries two HDR cameras and PC hardware. 
    The measurement is computed from a 3d reprojection of a disparity map that contains the depth information of the environment visible to the cameras. 
    A number of different factors affect the quality of the depth map computed
    
}

%BADThis thesis will primarily study a machine vision approach that uses stereo vision for the work environment
%epth mapping. Other optional sensors, such as a laser scanner, are considered.

%BADExisting machine vision approaches will be applied to the crane load problem. 
%The location measurement of the crane load could enable additional improvements in obstable avoidance while
%operating the crane.}

%\fixme{Abstract text goes here (and this is an example how to use fixme).}
%Fixme is a command that helps you identify parts of your thesis that still
%require some work. When compiled in the custom \texttt{mydraft} mode, text
%parts tagged with fixmes are shown in bold and with fixme tags around them. When
%compiled in normal mode, the fixme-tagged text is shown normally (without
%special formatting). The draft mode also causes the ``Draft'' text to appear on
%the front page, alongside with the document compilation date. The custom
%\texttt{mydraft} mode is selected by the \texttt{mydraft} option given for the
%package \texttt{aalto-thesis}, near the top of the \texttt{thesis-example.tex}
%file.

%The thesis example file (\texttt{thesis-example.tex}), all the chapter content
%files (\texttt{1introduction.tex} and so on), and the Aalto style file
%(\texttt{aalto-thesis.sty}) are commented with explanations on how the Aalto
%thesis works. The files also contain some examples on how to customize various
%details of the thesis layout, and of course the example text works as an
%example in itself. Please read the comments and the example text; that should
%get you well on your way!}

% Abstract in Finnish
% ------------------------------------------------------------------
\thesisabstract{finnish}{

Kivi on materiaali, joka muodostuu mineraaleista ja luokitellaan
mineraalisisältönsä mukaan. Kivet luokitellaan yleensä ne muodostaneiden
prosessien mukaan magmakiviin, sedimenttikiviin ja metamorfisiin kiviin.
Magmakivet ovat muodostuneet kiteytyneestä magmasta, sedimenttikivet vanhempien
kivilajien rapautuessa ja muodostaessa iskostuneita yhdisteitä, metamorfiset
kivet taas kun magma- ja sedimenttikivet joutuvat syvällä maan kuoressa
lämpötilan ja kovan paineen alaiseksi.

Kivi on epäorgaaninen eli elottoman luonnon aine, mikä tarkoittaa ettei se
sisällä hiiltä tai muita elollisen orgaanisen luonnon aineita. Niinpä kivestä
tehdyt esineet säilyvät maaperässä tuhansien vuosien ajan mätänemättä. Kun
orgaaninen materiaali jättää jälkensä kiveen, tulos tunnetaan nimellä fossiili.

Suomen peruskallio on suurimmaksi osaksi graniittia, gneissiä ja
Kaakkois-Suomessa rapakiveä.

Kiveä käytetään teollisuudessa moniin eri tarkoituksiin, kuten keittiötasoihin.
Kivi on materiaalina kalliimpaa mutta kestävämpää kuin esimerkiksi puu.
}
% Abstract in Swedish
% ------------------------------------------------------------------
%\thesisabstract{swedish}{
%illa Vargens universum är det tredje fiktiva universumet inom huvudfäran av de
%ecknade disneyserierna - de övriga två är Kalle Ankas och Musse Piggs
%niversum. Figurerna runt Lilla Vargen kommer huvudsakligen frän tre källor ---
%els persongalleriet i kortfilmen Tre smä grisar frän 1933 och dess uppföljare,
%dels längfilmen Sängen om Södern frän 1946, och dels frän episoden ``Bongo'' i
%ängfilmen Pank och fägelfri frän 1947. Framför allt de två första har
%sedermera även kommit att leva vidare, utvidgas och införlivas i varandra genom
%tecknade serier, främst sädana producerade av Western Publishing för
%amerikanska Disneytidningar under ären 1945--1984.

%Världen runt Lilla Vargen är, i jämförelse med den runt Kalle Anka eller Musse
%Pigg, inte helt enhetlig, vilket bland annat märks i Bror Björns skiftande
%personlighet. Den har även varit betydligt mer öppen för influenser frän andra
%Disneyvärldar, inte minst de tecknade längfilmerna. Ytterligare en skillnad är
%tt varelserna i vargserierna förefaller stä närmare sina förebilder inom den
%verkliga djurvärlden. Att vargen Zeke vill äta upp grisen Bror Duktig är till
%exempel ett ständigt äterkommande tema, men om katten Svarte Petter skulle fä
%för sig att äta upp musen Musse Pigg skulle detta antagligen höja ett och annat
%ögonbryn bland läsarna.}


% Acknowledgements
% ------------------------------------------------------------------
% Select the language you use in your acknowledgements
\selectlanguage{english}

% Uncomment this line if you wish acknoledgements to appear in the
% table of contents
%\addcontentsline{toc}{chapter}{Acknowledgements}

% The star means that the chapter isn't numbered and does not
% show up in the TOC
\chapter*{Acknowledgements}

I wish to thank all students who use \LaTeX\ for formatting their theses,
because theses formatted with \LaTeX\ are just so nice.

Thank you, and keep up the good work!
\vskip 10mm

\noindent Espoo, \DATE
\vskip 5mm
\noindent\AUTHOR

% Table of contents
% ------------------------------------------------------------------
\cleardoublepage
% This command adds a PDF bookmark that links to the contents.
% You can use \addcontentsline{} as well, but that also adds contents
% entry to the table of contents, which is kind of redundant.
% The text ``Contents'' is shown in the PDF bookmark.
\pdfbookmark[0]{Contents}{bookmark.0.contents}
\tableofcontents

% Abbreviations & Acronyms
% ------------------------------------------------------------------
% Use \cleardoublepage so that IF two-sided printing is used
% (which is not often for masters theses), then the pages will still
% start correctly on the right-hand side.
\cleardoublepage
% Example acronyms are placed in a separate file, acronyms.tex
% \input{acronyms}

\addcontentsline{toc}{chapter}{Abbreviations and Acronyms}
\chapter*{Abbreviations and Acronyms}

% The longtable environment should break the table properly to multiple pages,
% if needed

\noindent
\begin{longtable}{@{}p{0.25\textwidth}p{0.7\textwidth}@{}}
AABB & Axis-aligned bounding-box \\
AI & Artificial Intelligence \\
BM & Block Matching \\
BVH & Bounding Volume Hierarchies \\
CCIR & Comité Consultatif International pour la Radio \\
CMOS & Complementary Metal Oxide Semiconductor \\
CUDA & Compute Unified Device Architecture \\
FLIR & Forward Looking Infra Red \\
GPU & Graphics Processing Unit \\
HDR & High Dynamic Range \\
I/O & Input Output\\
LIDAR & Light Detecting And Ranging \\
OBB & Oriented Bounding Box \\
PC & Personal Computer \\
PCL & Point Cloud Library \\
PLC & Programmable Logic Controller \\
RADAR & Radio Detecting And Ranging \\
ROI & Region Of Interest \\
ROS & Robot Operating System \\
SAD & Sum of Absolute Differences \\
SGBM & Semi-Global Block Matching \\
SIMD & Single Instruction, Multiple Data \\
SOM & Self-organizing Map \\
SONAR & Sound Navigation And Ranging \\
SSD & Sum of Squared Differences \\
SSE & Streaming SIMD Extensions \\
TLD & Tracking, Detection, And Learning \\
TOF & Time Of Flight \\
UCC & Use Case Configuration \\
UDP & User Datagram Protocol \\
WYSIWYG & What You See Is What You Get \\
XML & Extensive Markup Language \\
XML-RPC & XML Remote Procedure Call \\ 
\end{longtable}

% List of tables
% ------------------------------------------------------------------
% You only need a list of tables for your thesis if you have very
% many tables. If you do, uncomment the following two lines.
% \cleardoublepage
% \listoftables

% Table of figures
% ------------------------------------------------------------------
% You only need a list of figures for your thesis if you have very
% many figures. If you do, uncomment the following two lines.
% \cleardoublepage
% \listoffigures

% The following label is used for counting the prelude pages
\label{pages-prelude}
\cleardoublepage

%%%%%%%%%%%%%%%%% The main content starts here %%%%%%%%%%%%%%%%%%%%%
% ------------------------------------------------------------------
% This command is defined in aalto-thesis.sty. It controls the page
% numbering based on whether the doublenumbering option is specified
\startfirstchapter

% Add headings to pages (the chapter title is shown)
\pagestyle{headings}

% The contents of the thesis are separated to their own files.
% Edit the content in these files, rename them as necessary.
% ------------------------------------------------------------------

% \input{1introduction.tex}


%===========================================================================================================
\chapter*{Notation and Symbols}

%\footnote{http://owl.english.purdue.edu/owl/resource/574/02/} of
%Purdue University or Strunk's Elements of
%Style\footnote{http://www.bartleby.com/141/}. Remember that footnotes

%\emph{cite}.

% An example of a traditional LaTeX table
% ------------------------------------------------------------------
% A note on underfull/overfull table cells and tables:
% ------------------------------------------------------------------
% In professional typography, the width of the text in a page is always a lot
% less than the width of the page. If you are accustomed to the (too wide) text
% areas used in Microsoft Word's standard documents, the width of the text in
% this thesis layout may suprise you. However, text in a book needs wide
% margins. Narrow text is easier to read and looks nicer. Longer lines are
% hard to read, because the start of the next line is harder to locate when
% moving from line to the next.
% However, tables that are in the middle of the text often would require a wider
% area. By default, LaTeX will complain if you create too wide tables with
% ``overfull'' error messages, and the table will not be positioned properly
% (not centered). If at all possible, try to make the table narrow enough so
% that it fits to the same space as the text (total width = \textwidth).
% If you do need more space, you can either
% 1) ignore the LaTeX warnings
% 2) use the textpos-package to manually position the table (read the package
%    documentation)
% 3) if you have the table as a PDF document (of correct size, A4), you can use
%    the pdfpages package to include the page. This overrides the margin
%    settings for this page and LaTeX will not complain.
% ------------------------------------------------------------------
% Another note:
% ------------------------------------------------------------------
% If your table fits to \textwidth, but the cells are so narrow that the text
% in p{..}-formatted cells does not flow nicely (you get underfull warnings
% because LaTeX tries to justify the text in the cells) you can manually set
% the text to unjustified by using the \raggedright command for each cell
% that you do not want to be justified (see the example below). \raggedleft
% is also possible, of course...
% ------------------------------------------------------------------
% If you need to have linefeeds (\\) inside a cell, you must create a new
% paragraph-formatting environment inside the cell. Most common ones are
% the minipage-environment and the \parbox command (see LaTeX documentation
% for details; or just google for ``LaTeX minipage'' and ``LaTeX parbox'').
% Alignment of sells: l=left, c=center, r=right.
% If you want wrapping lines, use p{width} exact cell widths.
% If you want vertical lines between columns, write | above between the letters
% Horizontal lines are generated with the \hline command:

% Place a & between the columns
% In the end of the line, use two backslashes \\ to break the line,
% then place a \hline to make a horizontal line below the row

%\begin{table}
%\begin{tabular}{|p{2cm}|p{3.8cm}|p{4.5cm}|p{1.1cm}|}
%\hline % The line on top of the table
%\textbf{Code} & \textbf{Name} & \textbf{Methods} & \textbf{Area} \\
%\hline
%T-110.6130 & Systems Engineering for Data Communications
%    Software & \raggedright Computer simulations, mathematical modeling,
%  experimental research, data analysis, and network service business
%  research methods, (agile method) & T-110 \\
%\hline
%\multicolumn{2}{|p{6.25cm}|}{Mat-2.3170 Simulation (here is an example of
% multicolumn for tables)}& Details of how to build simulations & T-110 \\
%\hline
%S-38.3184 & Network Traffic Measurements and Analysis
%& \raggedright How to measure and analyse network
%  traffic & T-110 \\ \hline
% \end{tabular} % for really simple tables, you can just use tabular
% You can place the caption either below (like here) or above the table
%\caption{Research methodology courses}
% Place the label just after the caption to make the link work
%\label{table:courses}
%\end{table} % table makes a floating object with a title


%The multicolumn command takes the following 3 arguments:
% the number of cells to merge, the cell formatting for the new cell, and the
% contents of the cell

\chapter{Introduction}
\label{chapter:introduction}
%Ongelman esittely, johdanto. Pituus noin 5 sivua.

In the modern world, products are being manufactured more than ever before, and at the same time the factory workflows should be optimised to guarantee profitable continuity for a business. These new requirements challenge all business owners, including industrial product makers who use overhead cranes in their manufacturing processes. Overhead cranes are found in most warehouses and factories, and they are often used in storage solutions, as part of the manufacturing workflow, and in the initial installation of machinery on the factory floor.

Since industrial revolution many different types of cranes were designed to help in loading and unloading of ship cargo in container crate terminals at seaports. Similar development happened to the overhead crane systems at factory sites, and the electric overhead crane (EOC) was introduced after the steam engine cranes. While the modern sea cargo handling process is highly automated, the de facto electric overhead crane installations are still mostly manually operated. Before the ideation of the modern container cargo handling, the lifting of barrels, pallets, and sacks was inefficient, and the ships were stalling longer times at docks than at sea, which reduced profitability\ref{Zrni04}. In sites where manual overhead cranes are used to move products, the crane operation could become more efficient with added automation systems.

The business of lifting solutions does get more automated as new crane systems are being developed to meet the ever increasing needs for performance. Speed and safety affect the productivity of lifting and moving goods with indoor cranes, and with ever lowering computing prices a lot of research is going on to find ways how added programming and sensors could improve productivity, or increase the life cycle of the crane. While speed is not important in all lifting operations, safety is important in all of them. 

 % measuring goods
Measuring products while they are being moved and processed at the industrial site is the very basis for all quality control in other manufacturing industries: for example, radio frequency identification tags are used to identify and control a batch of products in bulk manufacturing, video systems measure process quality in a paper factory, and a spectral analyzer measures the concentration of foreign substances in a soda bottle washing process. In overhead crane environment, there is no information on what is being lifted other than a mass, and the end effector tool location. A machine vision perception system can add quality control in the form of providing a feedback link: e.g. information on lifted object size, shape, color, and past locations.

%In the modern world, goods are being moved more than ever before, which brings challenges to keep the goods handling easy and safe. Products are mostly transported from one location to another using a modern automatic container handling process, but for example loading and unloading events remain laborous when single industrial items, such as engines or other machinery, are moved to a new location. The electric overhead crane (EOC) is the de facto standard for heavy lifting work in factory and warehouse environments.

%The business of lifting solutions gets more automated as new crane systems are being developed to meet the ever increasing needs for performance. Speed and safety affect the productivity of the event of lifting and moving goods with cranes, and with ever lowering computing prices a lot of research is going on to find ways how added programming and sensors could benefit the crane operator for increased performance. While speed is not important in all cargo  operations, safety is important in all of them. Then again, same automation technologies could be potentially used to speed up operations where speed is important, such as log picking operation in forestry solutions.

%If an overhead traveling crane is used to move products inside a warehouse e.g. from one manufacturing station to another, then a measurement of the lifted load object can potentially provide useful information for automation systems that use the size of the load object as a parameter in smart applications. Currently, overhead cranes do not measure any geometry of the load objects that they lift. Thus, all the decision-making whether the load object is physically in correct spatial coordinates is left for the operator to decide.

%Ideas of the modern container cargo handling process only started after the second half of the 20th century, and before that the only known way of transport was sea cargo packed in barrels, sacks and pallets. It was cumbersome to load and unload a ship filled with such small units, and it was not unusual that the ships spent more time in docks than at sea because of inefficient handling of small cargo.\ref{Zrni04}\par

It is possible to find examples of fast, automated, optimized, profitable, and safe factory manufacturing workflows, but many industrial ventures still rely on manual movement of a product on a factory floor. Such manual operation can suffer from non-optimized route selection, operator fatigue, damage to goods in collisions, safety issues for other workers, and other problems due human error. Some industries are not even profitable unless a certain speed of moving goods safely is reached, which applies for e.g. forestry log picking and tree felling work. To improve things, a real-time description of the load object and its environment could be available, so that it would be possible to prevent unwanted events using the installed crane systems with some perception sensors and software.

\section{Collision Control In Crane Environment}
\label{section:collision_control_in_crane_environment}

% käyttökokemus
Industrial goods e.g. beams, engines, large coils, drums of dangerous goods, and pallets of stacked items are expensive, often unique one-time orders that are being lifted and transferred inside a warehouse every day. If in collision, they may damage or break other products, which can be prevented with collision avoidance systems. Such an environment that can interact with the operator in productive ways could be called a smart crane environment. A smart crane environment can help a crane operator move products in a warehouse in automatic manner, and provide added information about products and their properties.

% törmäyksenesto
One of the most important features in a smart crane environment is the collision avoidance technology that can automatically prevent collision based damage to the environment, and warn the operator of safety hazards and engage safety measures if needed. This kind of technology can detect people who walk on the factory floor, and prevent injury to other factory workers. Such a collision avoidance system is based on automatic route planner that understands the load object model and the environment model. If the load object is carefully measured and its geometry is known at all times, a collision computation with a known environment model can be used to detect future collisions before they happen. The added value is potentially great, which is why the measurement of the load object is worth researching.

% tällä hetkellä overhead cranes
Traditionally, overhead crane operation has been manual and not computer aided. An overhead crane is operated by a technical worker who is making all the decisions whether the lifted product can be moved in one direction or not without a risk of collision or injury to other workers in the area. Supposedly, other workers are going about their daily work in the same factory floor where the overhead crane operates. With current generation manual overhead cranes it is possible to collide a load object with other objects that lie on the factory floor or on the shelves of a warehouse. Overhead cranes have automation systems that improve some qualities, such as load anti-sway systems, single motor hoisting mechanisms, and wireless remote controllers. While they add value for the crane operator and improve safety and speed of the overhead crane operation, it is still far from automatic. Moreover, the computer control system cannot warn the operator of any hazards.

% technical difficulties
From a technical point of view, a perfect tracking of a load object in space and time is difficult, but if an understanding of the object model and its qualities is formed, then an estimate of the current state of the model can be used for decision making at any time. Tracked objects should continuosly provide information about their location, dimensions, and moving speeds at least. With such model information it is possible to implement collision avoidance functionality to a crane working environment.

% how to implement collision detection
The first step in implementing a collision avoidance technology is to provide some metric model of the environment, and of all the objects in it. A machine vision system can be used for environment modeling in general, and this thesis  later introduces load object modeling with machine vision techniques. Further design of such a system should be done  after an understanding of how the environment can be modeled to support a collision detection technique. New overhead crane systems do not currently have any collision detection features, but optimized route planning could become a standard feature in the future.
 
\section{Applications}

Industrial applications for machine vision have become more ubiquitous as the price for machine vision systems have decreased. Applications range from space exploration to agriculture, and from mobile robotics to quality control at a manufacturing line. The use of machine vision brings the speed of the computer to many errands that are impossible or dangerous for a human to do, for example 


quality control in high speed manufacturing processes e.g. quality control of a multi-layer web forming in a paper factory. Machine vision is extensively used in packaging industry


Machine vision has been used in the overhead crane environment in some earlier research. For example, crane control was researched by Peng and Singhose using top-down cameras and a wand controller. They used the cameras to locate the wand controller, and the crane hook with help of reflective materials, and reported that the system greatly increased the operating efficiency of the crane\ref{Peng09}. They also reported machine vision reliability issues because of limited onboard camera processing abilities, and changing lighting in the environment. An upgrade to operation in the infrared spectrum of light was suggested to counteract changing visible light conditions.





Previously, machine vision has been  in the overhead crane environment for improved trajectory control using wand controller. 

\ref{}

A machine vision perception platform includes a stereo camera setup that is used to measure object dimensions in the crane working area. The installed cameras can be used for other purposes, too. For example, security camera surveillance can detect trespassers when the factory is shut down. 

recordings of states of the environment, provide video feed of accidents for insurance companies, and enable teaching material for new users as records of events. 



Size and weight statistics can help track the crane usage and lead to improvements in warehouse layout, or help a factory manager decide upon investing in a larger manufacturing floor if the capacity is at a limit.

% operointi
A human operator is better able to move load objects through narrow passages and execute difficult spatial tracks than a computer, but he or she can not minimise the used energy to perform a certain movement or minimise daily wear of moving parts. A computer control can positively improve the product life cycle and minimise maintenance breaks. 
Some automatic overhead cranes have been built, but usually they move bulk items, such as sand, steel, or other process-like material that is moved in environments where people are not working.




The biggest non-safety related features are the optimized route planning (using knowledge of geometric dimensions) and automatic moving of loads using a computer planner. During the life cycle of the crane installation such features can reduce vast amounts of used energy, possibly increase movement speeds, and sometimes increase service intervals.
 
 
 
 Some other properties can be e.g. warehouse surveillance, operations tracking, quality control etc.
 
% person lifter


%+ Environment sensing is needed for robotization of spaces
%+ 3d data of the environment is important in factory design, facility management and urban regional planning. \ref{Surmann03}

% stereoscopic teleoperation

%Teoriaa ja teknologioita. Pituus noin 20 sivua.
%Applications that can benefit from accurate positioning and tracking of a load object

%\section{Existing Positioning And Tracking Applications}
%\label{section:existing_positioning_and_tracking_applications}}

%\section{Safety Applications}
%\label{section:safety_applications}

%Traditional and new safety applications can utilize load positioning information for added functionality.

%A traditional safety application stops the crane process if a human is detected in a dangerous work zone \cite{Raula11}. Alternatively, an adaptive safety scheme can be utilised to restrict the operation range of the crane, but not halt it. If we are able to reliably detect a location for the moving load

%The work area of the crane can be modelled with state-of-the-art sensing of the environment enabling for example pre-collision avoidance.

%Adaptive safety techniques can be realised with the improved

%\section{Mapping Applications}
%\label{section:mapping_applications}
% application: city-scale 3d reconstruction from community-sourced online photos | ref Agarval09

%\section{Selected Use Cases}
%\label{section:selected_use_cases}
 
\section{Overhead Cranes}
\label{section:overhead_cranes}
% big picture
The focus of this work is on measuring the dimensions of industrial goods being lifted and moved in an industrial site using an overhead crane. A glimpse in the not so distant history reveals a development of the overhead crane, a hoisting system more commonly known as the bridge crane or the process crane. First mass manufactured overhead crane systems were commissioned in 1876 in England, and they were powered by steam engines in the beginning of the industrial revolution. Today, 138 years later the overhead crane is widely used in different industries for efficient process lifting and service duties using electricity. The development of the electric overhead traveling crane (EOT) has introduced the modern overhead crane as we know it. It is electrically powered and often used in heavy process industry, and in custom industrial manufacturing.\par
%structure of a single overhead crane
    Overhead cranes have a beam-like bridge installed between two rails. The bridge spans the gap between the rails and travels from one end of the rail to the other. Effectively, a polyhedron shaped (cuboid) crane working area (portal of the crane) is formed underneath the rails encompassing most space between a factory floor and the height of the installed crane. Usually, a control pendant, or a wireless hand-held controller is used to move the load object to a desired location in the 3d crane working space. \par
% medium picture, applications
    Overhead cranes are used for medium to heavy lifting in industrial manufacturing and process industries. For example, in steel mills, heavy lifting is needed when cylinders of molten steel or other metal products are carried to the next process station. In heavy industry, the capacity of the crane and standard compliance is important, and the crane must operate reliably in hazardous environments. Efficient and safe usage patterns depend on the application at hand. In all applications, most important factors usually are time-wise efficient movement of the crane, safe operation of the crane, and easiness of operation.\par
% thesis focus on overhead cranes 
    Objects that are considered as load objects in this thesis cover a range of custom manufactured products whose size is unknown prior to the lifting operation. Since each item moved with the crane is unique, non-standard size object, an understanding of the load object size would be valuable to a software system that operates autonomously.
    In heavy process industry, it is not often meaningful to measure load objects or containers whose size is standard and known prior to the lifting operation. For practical reasons, all testing on overhead cranes have been done using a company test site installation that was capable of lifting medium to heavy objects up to 15 tonnes. All the objects lifted with the overhead crane were smaller than 2 meters for all dimensions, which is quite a normal size for a pallet. Only the volume of the object affects its geometry measurement.

\section{Forestry Machine Load Object Measurement}
\label{section:forestry_machines_cranes}

A forestry machine load object measurement is a secondary objective that is researched in this thesis. Forestry machines that include a rotary telescopic crane are included as research targets, mainly log picker trucks. In tree felling work, little automation is used currently, and to increase the level of automation, a load object measurement system has been suggested. A lot of research has been done in the area of stereographic 3d teleoperation of log picker trucks, but without a better spatial understanding of the truck environment and computer aided end effector tool control it has been deemed not feasible with current teleoperation technologies. A machine vision system can provide a geometry measurement of a log picker load object, which in turn could be further used to provide a better spatial description of the forestry crane working area for the teleoperator. 

Log picking work differs from lifting work in a warehouse environment mostly due its productivity constraints. A certain speed of picking logs must be maintained to keep the operation of the log picker truck profitable in business sense. While teleoperating a rotary log picking crane is possible, the speed of a teleoperated crane is much lower than a human operated crane. Teleoperation techniques do enable a single driver operate multiple log picker trucks simultaneously, but currently the work is only productive with drivers on site. 

Research in automatic forestry machines is an ongoing effort that tries to increase productivity in the field with help of teleoperation and automatic machinery. A fully automatic log picker truck that is as fast as a human operated truck would provide the biggest productivity increase, but currently the research is focused more on aiding a single driver increase his or her productivity with help of computers. For example, new augmented reality displays are being introduced in the forestry machine cockpits to help the driver get information that helps with the work. 

\section{Goals And Objectives}
\label{section:goals_and_objectives}

The secondary objective of this thesis is to be able to compute the dimensions of a picked up log with the help of a machine vision system. Further processing of the measurement information and the details of teleoperation applications are outside of the scope of this thesis.

Many research efforts report using laser range finders for 3d environment modeling \ref{Hähnel03}. This is natural since laser range finders and LADAR scanners output a high resolution point cloud that is accurate up to sub-centimeter lengths in range of hundreds of meters. In this thesis, a stereo camera based environment modeling approach is used since measured lengths are limited to maximum of 15 meters depending on the application. While the accuracy of the stereo camera depth map output is suitable for the bounding volume computation of a load object, the price of the system is much smaller when compared to a laser based solution. A camera based solution price is up to 2000 USD depending on the setup, while a laser based system cost may exceed a whopping 50000 USD.

% goal with the stereo camera acquisition is that the overall system would be cheap to purchase
% objective: point cloud processing runs in almost real-time, more often than 5 times per second

An energy-saving, semi-automatic, safe process crane working area is the desired final outcome for upgrading a traditional manually operated process crane installation. It is under review whether a detailed, almost real-time description of the environment could provide enough information to realize the envisioned system. Currently, to the knowledge of the author, there are no commercially available systems that can provide a real-time description of an environment that integrates to a crane programmable logic controller (PLC), but it is a grand vision to start out with. The first step in realising such a system is to provide a measurement or a simple model of the load object and start aggregating other descriptions of the environment on top of it. To start with, this thesis uses a perception sensor platform solution that provides geometry knowledge of the process crane working area, and an external computer that generates the best possible geometry description of the load object for the use of the crane PLC computer. 

\chapter{Perception Sensors And Model Acquisition}
\label{chapter:perception_sensors_and_model_acquisition}

Perception sensors in a machine vision system include sensors such as digital cameras, time-of-flight cameras, structured light cameras, and other equipment that may recover information about the surrounding environment by capturing emitted electromagnetic radiation. Machine vision cameras may use the visible spectrum of the light, or sometimes other spectrum of light e.g. ultraviolet light spectrum. A lot of different sensor options are commercially available for machine vision works, and the design of the machine vision system is highly affected by the selection of the sensor, and the digital format of the information that is output from the sensor.

The problem of the load object geometry measurement inherently requires a sensor that can recover 3d geometry of the environment. Extensive research has been done in the field of 3d geometry recovery, and by utilising these vast sources of research many different sensors can be selected that can produce a good 3d geometry model of the environment.

Besides machine vision, also other perception sensors exist that can be used to recover 3d geometry, such as laser scanners and radars. These are not considered strictly machine vision sensors, but for example the laser scanner technology is accurate enough to be used to verify a 3d geometry that any of the camera based solutions have recovered. Thus, a Velodyne laser scanner was used in capturing an accurate model of the 3d environment in this work.

Next, different machine vision sensor options will be presented that could provide data input to the envisioned load object geometry measurement system.

\section{Machine Vision Perception Sensors}
\label{section:perception_sensors}

Camera based vision sensing can be active or passive depending on the technology used. An active sensor emits energy into the environment and measures the energy radiated back to a sensor array. A passive sensor does not emit energy on its own, but registers and measures energy radiated to the sensor array from the environment. In general, the energy that is registered and digitally sampled by a camera is visible light, which will be the technology used in this work.



+ active sensor: a sensor that controls and uses its own images. An active sensor controls its image's illumination with an electromagentic energy source, such as laser or white light.
+ time-of-flight cameras (TOF)
According to other research, the Kinect sensor cannot observe items cast in direct sunlight, preventing extensive outdoor usage besides its range limitations \cite{tikkanen13}.  

\subsection{Stereo Cameras}
\label{subsection:stereo_cameras}

\subsubsection{Photometrics}
\label{subsubsection:photometrics}

\ref{teledyne_whitepaper}


The aperture of the camera lens system controls the depth of field (DOF) perceived in a digital or analog image of a scene. The depth of field describes the range of depth that can be viewed in sharp detail through the lens system. If the DOF is large, then only a small depth range is in sharp detail, and the rest of the image is blurry for near-field objects and faraway objects. If the DOF is small, then near-field objects and faraway objects are sharply in focus alike. For a stereo camera depth measurement system, small DOFs, or extended DOFs are used to fully visualize a deep range of a scene in sharp detail. Diffraction limits the size of aperture, and small DOF images may suffer from high variance noise effects and low light conditions.

Radiance is a measure of power of light radiated from a unit surface area of an object to some spatial angle. 
Irradiance is a measure of power of light cast on a unit surface are of an object.
The relation between an object's surface that radiates onto the camera sensor array can be formulated as

\begin{equation}
\label{equation:radiance}
E = L \frac{\pi}{4} (\frac{d}{f})^2 \cos^4{\alpha}
\end{equation}

where L is object's radiance of unit surface area, and $\frac{d}{f}$ is the f-number of the lens in use. The $cos^4{\alpha}$ is the vignetting term. Not all radiating energy from the object is captured by the CMOS sensor array, but only the fraction that enters the camera lens and falls on the effective sensor array surface.

%weather conditions

Quality of measurements degrade in bad weather, such as fog, mist, rain, or snow \cite{Kawai12}. 
Especially camera systems are affected with possibly inhomogenous degradation in image quality.

Different types of degradation:
    + lens flare
    + condensation in humidity changes
    + changes in illumination
    + shadowing
    + occlusion
    + noise from moving droplets of water e.g.

If a stereo camera is used, inhomogenous degradation in image quality


Colored markers are sensitive to natural light in outdoor environments \cite{Kawai12}.

    
\subsection{Lidars And Radars And Sonars}
\label{subsection:lidars_and_radars_and_sonars}

Automotive radar is a millimeter wavelength radar that can measure ground properties or obstacles in front of a robot or a vehicle. It can measure environmental properties in the range of 1 to 120 meters \cite{Ahtiainen12}.

The advantages that the automotive radar has are weather-independent operation, and direct acquisition of range and velocity measurements \cite{Wenger07}.

The challenges that automotive radar introduces are measurement accuracy in resolution, and electromagnetic emissions on other radio frequencies \cite{Wenger07}. 

The automotive radar operates on a wide band of 5GHz to achieve a resolution of 5 cm \cite{Wenger07}.

In Europe the bands that are reserved for automotive radar operation in traffic applications are approximately 21 GHz to 27 GHz and 77 GHz to 81 GHz \cite{Wenger07}.

A world-wide harmonized channel allocation system for radar widebands has been proposed, but also considered very difficult to achieve due to national regulations \cite{Wenger07}.

Radar is not very widely used for machine perception due to low resolution and high cost. Instead, a stereo camera is often used. Still, automotive radar is an interesting add-on because of its capability of operation in adverse weathe, for example fog. 


\section{3d Scene Representations}
\label{section:3d_scene_representations}
+ possible algorithms: clustering, water-filling, and convex hull computations

In the virtual reality (VR) research the modeling of real-world items has been a big problem for many years. 
It is not easy to recreate a real-world item in a digital system, but many approaches have been suggested.

One option is to record video from multiple cameras that look at the object from different points of view. In order to obtain a digital model we may utilise a multi-camera stereo process and compute range images for further processing. Obviously, this approach is very computationally difficult and results vary depending on the object convexity, lighting environment and the used algorithms.  

A study by the Carnegie Mellon university robotics institute call the model acquired from the intensity image and the corresponding range imagery a visible surface model (VSM) \cite{Rander97}. Basically a VSM contains a subset of real-world surface features and texture in the digital format, and the VSM can be further placed into a more comprehensive representation of the world, namely a complete surface model (CSM). 

\subsection{Range Image Representation}
\label{subsection:range_image_representation}

Range image is a type of 3d data point representation that uses a 2d image array whose pixels correspond to a known location in the imaged scene. A range image is conceptually equivalent to an intensity image where image intensities are replaced with range measurements acquired from a sensor.

Range images have limitations in the ability to reproduce a detailed original point cloud. The term detailed point cloud here is a reference to an object model that loosely describes the object shape and details using a convex hull of points in space, including points that may be not visible from a single viewpoint. Then, if the detailed original point cloud includes multiple points on a line that projects to a single pixel in the range image, some data will be lost in the conversion to a range image. 

Also, if the ranging sensor has a spatially different measurement lattice structure compared to the 2d-array structure of range image, some pixels may be left blank because of a mismatch in spacing of range measurements.

for range measurements than what the range image uses, then some pixels in the range image will be left empty.

Since we create the point cloud with a camera system, we would not have the described projection data loss problem. Another problem with range images       \cite{Unnikrishnan08}.

\subsection{Volume Pixel Representation}
\label{subsection:volume_pixel_representation}

A volume pixel, or a voxel, is a prototype of a single entry in a 3d grid structure consisting of voxels. A voxel indicates occupancy in space, and it has at least a volume, and a location, which usually is in 3-dimensional array with constant dimensional sampling. A perfect example of a prototype voxel is a wooden box, which occupies a constant volume in space, and has a location in terms of, for example, a living room space. In digital systems, a voxel is more of a conceptual item that has a location, size, and optional services, such as statistical information about the encompassed voxel subspace. 

A 3d scene captured with a stereo camera system can be represented with a voxel grid. A standard voxel grid will have a constant sampling grid, thus, the scene will be composed of same-sized cubes. Anoter option is to use a varying sampling grid structure, or an octree structure (described in...?).

limitations: grid resolution, high-resolution grid consumes a lot of memory

\subsection{Octree Structure}
\label{subsection:octree_structure}
% multi-resolution octree representations

Originally, the idea to use octrees for mapping was introduced by Meagher in 1982 \cite{Meagher82].

Octomap is a new open-source framework for depicting 3D maps that can represent the world in multiple volumetric resolutions. The Octomap framework is based on a probabilistic octree structure using probabilistic occupancy estimation \cite{Hornung10}. 

We can assume that in all cases of range measurements a lot of noise and uncertainty will be introduced in the measured values. Thus, it is a good idea to represent the range measurements in a map that is probabilistic in nature.

Octomap differs from a traditional point cloud in some ways. In the original work Hornung et al. claim that a point cloud uses up a large amount of memory and are only suitable for modeling static environments with high precision sensory equipment.

+ Octomap can model occupied space, free space, and unknown space all at the same time.


+ probabilistic sensor fusion

Octomap
    + memory-efficiency principle (memory and disk space handling)
    + differentiates unmapped and obstacle-free areas
    + supports probabilistic information fusing
    + can represent arbitrary shapes in space (elevation map cannot)

\section{Geometric Environment Acquisition Techniques}
\label{section:environment_acquisition_techniques}

In this work the focus on environmental acquisition is on systems capable of 3d modeling the geometry of surrounding environments. Other environmental variables, such as temperature, wind conditions, humidity or other qualities of the environment are not considered in the context of 3d acquisition, although they could possibly have an effect on the perception sensor parameters in a dexterous real-world acquisition system. 

\subsection{Object And Environment Modeling}

In general, an unordered real-world scene is difficult to model using perception sensors. A lot of study in real-world object modeling has been done using multiple sensors including laser range scanners, multiple viewpoint cameras, color cameras, inertial measurement units (IMU) and global positioning system (GPS). Multiple perception sensors and large image quantities from a single environment are needed in order to compute a realistic virtual environment model that displays a correct geometry of the environment, and presents textured object models with realistic lighting. \ref{ElHakim98}



Since the used HIMMELI platform based solution that is used in this work enables only a single viewpoint stereo camera produced depth map at a single timestep, the model of the acquired environment is limited to a sparse point cloud presentation. Thus, the output 3d geometry will not resemble any realistic scene, but it does contain keypoints coordinate information that enables Euclidean dimension measurements. Additionally, a single-channel black and white high definition range image is available for e.g. object classification purposes. More in depth specifications of the HIMMELI platform are presented in chapter \ref{section:himmeli_sensor_platform}.

If multiple depth maps are used to acquire a complete model of an object, then data acquisition, registration between different viewpoints, and integration of the different registered views are compulsory steps in modeling.  Multiple camera view registration is a classic technique that can estimate geometric surfaces of objects in more complete detail than a single camera view depth mapping. Multiple view registration is outside of the scope of this thesis, but an interested reader may find extensive research in literature on complete object modeling using multiple registered camera images.  \ref{Chen91}

All geometric acquisition techniques either use an active perception sensor that emits energy in the environment, or use a passive perception sensor that only detects energy reflected or radiated from the environment. Next, some techniques that are used to acquire geometric environment depth maps are introduced. Triangulation and structure from motion (SFM) are techniques that uses passive sensing of the environment with cameras. Laser scanning and structured light patterns use active sensing of the environment using light sources and cameras.  

\subsection{Triangulation}
\label{subsection:triangulation}



+ lidar cannot measure the depth of highly reflective materials, or very low-reflectance materials such as glass or paint-black steel.
+ Mechanical coordinate measuring machine (CMM) (3D modeling)
    + DCC CMM

+ Structured Light Triangulation
    +laser plane range finder

\subsection{Structure From Motion}
\label{subsection:structure_from_motion}

Structure from X techniques
+ Shape from shading
+ Structure from motion

\subsection{Multiple Perspective Interactive Video}

Multiple perspective interactive video (MPI) method is used to enhance live video with interactivity. The user can adjust camera views, or add digital content in the video. 

Basically, a detection of motion is achieved when each video frame is compared to a background image and the differences are logged. 

The method starts with knowing a priori metric map of the scene, and it requires multiple cameras to work with. The number of cameras connected to the system is in range from 2 to 1000 cameras.

Visible surface model
+ 2.5D Marr paradigm
+ Structure recovery method: multibaseline stereo
+ First make a triangle mesh model 

The real-world coordinates of each backprojected pixel can be computed using the depth and image coordinates given a camera calibration \cite{Rander97}. The resulting set of [x,y,z] coordinates is mainly referred to as a point cloud, depicting a point structure of the computed object.

In MPI method a triangle mesh presentation of the depicted scene is computed with the resulting point cloud. Many software exist that can visualise a point cloud, such as rviz in RoS system, but the suggested textured mesh presentation needs further processing and texture mapping. 

After digital processing we get a digital model of the scene from viewpoint A. When the camera is located exactly in viewpoint A, the presentation is flawless in comparison to the original scene. The report from Rander et al. \cite{Rander97} states that if a virtual camera is moved not far away, the scene can be transformed to another viewpoint B. In viewpoint B the image contains holes where occlusion was present in the original scene.

If the point cloud contains N x N points in the depth map, the computational complexity for triangles is O(N^2) \cite{Rander97}. Furthermore, the modeling of planar surfaces introduce shortcomings in surface tessellation. The surface may be tessellated with 1000 triangles in favor of 2 triangles, which means the computation overhead increases and causes extra calculations in texturing and modeling phases. The number of triangles in an area is described by a measure called mesh resolution that can be controlled in favor of smarter modeling \cite{Johnson96}.






\subsection{Stereo Techniques}
%omnidirectional cameras


Synthetic random dot images can be used for algorithm testing \cite{Zitnick00}.
Pollard et al introduces a PMF algorithm that is based on three stereo matching constraints. The third constraint, disparity gradient limit, is unique to the PMF algorithm \cite{Sonka07}.
Photometric Stereo \cite{Woodham80}.
    + Recovers surface orientation assuming a known reflection function
    + a Lambertian surface model
    + requires at least 3 images of the object, one fixed camera view, changing direction of illumination
    + point illumination sources with known intensity and direction


% another snippet

A histogram is a two-dimensional graphical representation of an image grey-level distribution. The grey-levels correspond to the image intensity. As depicted in figure X, the X axis represents the number of independent grey levels that the image contains. The grey-levels range from 0 to N levels. For example, in an 8-bit single channel image there can be up to 255 independent different tones of gray. The Y axis represents per centage information about a single grey-level ranging from 0\% to 100\%. Thus, the histogram essentially displays a distribution of all grey-levels.

Photometrically, grey-levels of an image are quantized estimates of image irradiance \cite{Sonka07}.
%histogram equalisation
Histogram equalization maps the current image intensity response over the full available intensity range. In figure N, the first image depicts image intensities distributed over a limited band of intensities. The second image displays the mapping result of the equalization process. The resulting histogram is calculated with the cumulative distribution function H'(i).

todo: corresponding histograms of two test images, and their cumulative distribution functions

%end of chapter 3
\chapter{Stereoscopic Machine Vision System}
\label{chapter:stereographic_machine_vision_system}
% stereoscopic means to perceive and/or see in 3d | stereographic means projecting a sphere onto a plane
Stereoscopic perception is a natural phenomenon first discovered in human and animal vision systems. Stereoscopy in human vision is based on stereopsis effect, which means perceiving depth in a scene using binocular vision. A binocular vision system in humans is constructed out of two monocular vision eyes that are used in a binocular setup. The same depth perception can be programmed to a computer, but instead of eyes, cameras are used. Moreover, instead of brains, algorithms are used to mathematically compute depth information from the image data.

While humans perceive and estimate object sizes with the help of their previous knowledge of the world and their implicit stereo vision calibration by the brains, a machine vision system utilising two cameras must be carefully calibrated with the help of a calibration target, after which it can quantifiably output metric measures of physical objects. After capturing a pair of images and processing them, it is possible to request a 3d coordinate value for almost any feature found in one of the images. In this way, by selecting two points from the 2d image that span the width of the wanted load object, it is possible to request a 3d vector whose length reports the actual length of the projected load object in the 2d image. Thus, moving from 2d feature projection to a partial 3d feature model is important since the measurement is done using a virtual 3d ruler.

\section{From 2d to 3d}
\label{section:from_2d_to_3d}

A calibrated two-camera system is most often called a \emph{stereo vision system}, or \emph{3d vision system}. In general, 3d vision systems used in engineering applications solve problems of 3d scene reconstruction, understanding of object properties, and minimization problems depending on the scientific application at hand. Often a stereo camera setup is used to solve the problem of 3d information recovery from 2d images using knowledge about the camera setup geometry and the image content.\cite{Sonka07} 


Different 3d information recovery techniques are introduced in section \ref{section:environment_acquisition_techniques}. 

A three-dimensional object-centered representation is suggested favourable to a pixel-based intensity image representation to improve description of objects. Sonka suggests that this can be achieved in three steps where

\begin{itemize}
\label{list:objectcenteredmodeling}
\setlength{\itemsep}{0pt}
\item 1. Pixel representation is processed into a surface representation (primal sketch)
\item 2. Surface representation is processed into an oriented surface model (2.5d - depth map of visible surfaces)
\item 3. Surface orientation model is processed into a 3d description (3d model)
\end{itemize}

\cite{Sonka07}


1. From pixels to surface representation
2. From surface representation to surface orientation model
3. From surface orientation model to a full three-dimensional description

2D => primal sketch => 2.5D sketch => full 3D

The primal sketch is an edge image where physical edges are not incurred. The idea is to extract information about physical features of different scales using filtering and second-order zero crossings. First, the original image is filtered with different radii Gaussian filters and after that a Laplacian operator is used to locate second-order zero crossings \cite{Sonka07}. 

The next step constructs the 2.5D image, or a depth map, with bottom-up machine vision techniques. Since bottom-up techniques are used, this step of depth map computing is applicable for all applications and no additional domain-speficic information is needed. 

The final transformation from the depth map to a full three-dimensional object representation is not presented in the thesis. It is also the most difficult transformation as opposed to well-known mechanisms of primal sketch generation and depth map generation. It is 


No additional domain-specific information is needed since the 

Interesting problems
+ Feature observability in an image
+ Marr's theory
+ Perspective projection challenges

Bottom-up reconstruction
    + from multiple image construct range images

Top-down recognition
    + CAD model recognition especially

Space Carving
    + volumetric representation


+ object surface parameters
    + reflectivity e.g.
    
    
+different approaches: top-down (model-based) or bottom-up (reconstruction)



\subsection{Marr's Theory}
\label{subsection:marrs_theory}

Marr's theory especially points out the great challenge in machine vision of how to achieve a better understanding of the visual system in general, and not just create another vision application suited for a single specific target.

\subsection{Stereo Imaging Techniques}
\label{subsection:stereo_imaging_techniques}

One of the fundamental issues in computer vision is how to display a solid shape \cite{Sonka07}. 

Stereo imaging is divided into two different categories by the used algorithm. The first category is dense matching algorithms where stereo matching algorithms run on all pixels. The second category is sparse matching algorithms where only distinctive features will be matched unambiguosly \cite{Terho10}.

\section{Perspective Projection Geometry}
\label{section:perspective_projection_geometry}

In general, a perspective projection, or central projection, is the de facto type of projection for human beings. We all have a pair of eyes that can be modeled with a pinhole camera model, and the pinhole camera depicts any scene with a perspective projection. In perspective projection objects that are far away appear smaller than objects that are near. This phenomenon is called a perspective. 

The stereo cameras that sense the world with CCD arrays give the user a perspective projection of the world on a 2D image plane. The basics of this projective geometry will be discussed in the case of non-parallel setup of the stereo cameras. That means the optical axes of the cameras are not in parallel, but have a specified angle with which the optical axes coincide. The cameras themselves will be modeled as pinhole cameras with certain lens system errors corrected for.

In computer vision, 

+ homogenous transformation => transforms lines into lines



The camera geometry model that we use is a single perspective camera model or a pinhole camera model.


%As depicted in the \ref{}

\subsection{Epipolar Geometry}
\label{subsection:epipolar_geometry}

Next we will introduce a two-camera epipolar geometry setup that is used for the depth mapping of the image features.

+ here put an image of the stereo geometry setup

+ reference to the optical centers C and C', and baseline of the camera setup

By definition, an epipolar plane is the planar surface that spans between vectors $\vec{CX}$ and $\vec{C'X}$. The epipolar plane intersects the image planes $\pi$ and $\pi'$ at epipolar lines. A feature of the epipolar line is that any projected point X must lie on this epipolar line on the image plane, a fact which greatly reduces the search space for matching image features.

Next, we introduce the epipole, which is the line intersection of an epipolar line and the baseline of the stereo camera configuration. By definition, epipoles always lie on the baseline.

A canonical configuration is a stereo camera setup where the camera optical axes are in parallel. In the canonical configuration the epipoles move to infinity, and the effect known as disparity can be seen on the two resulting images. Disparity means the shifting property of image features in a picture taken in another world coordinate. For a real disparity effect the camera must shift only along a single coordinate axis.

Any other stereo configuration with non-parallel optical axes can be transformed into canonical one by image rectification processing. 

The depth of a measured feature can be computed if the geometry of the stereo setup and the intrinsic camera parameters are known. The depth measurement value $Z_m$ is calculated using equation \ref{equation:disparity_eq},

\begin{equation}
\label{equation:disparity_eq}
\mathsf{Z_m} = \frac{\mathsf{bf}}{\mathsf{d_x}}
\end{equation}

where \emph{b} is the baseline width between the principal points of the camera image planes, f is the focal length for both cameras, and $d_x$ is the measured horizontal disparity value in a standard parallel stereo camera configuration. Now, if \ref{equation:disparity_eq} is differentiated with respect to horizontal disparity $d_x$, an equation for the effect of disparity change in the output measurement is achieved. 

\begin{equation}
\label{equation:disparity_derivative}
\frac{dZ_m}{dd_x} = \frac{-bf}{d_x^²}
\end{equation}

Furthermore, we may insert disparity value as $d_x = X_L - X_R$






Following the constraints that the epipolar plane induces to the stereo matching problem a fundamental matrix is introduced. The co-planar constraint 

\begin{equation}
\mathbf{X}_{L}^T (\mathbf{t} \times \mathbf{X'}_{L}) = 0
\end{equation}

is transformed into 

\begin{equation}
\nathbf{u}^T (K^{-1})^T S(t) R^{-1} (K')^{-1} \mathbf{u'} = 0
\end

An interested reader can find the full formulation of the fundamental matrix in Sonka et al. book \cite{Sonka07}. The fundamental matrix F is according to the bilinear relation introduced by Longuet and Higgins

\begin{equation}
\mathbf{u}^T F \mathbf{u'} = 0
\end{equation}

where the fundamental matrix F is 

\begin{equation}
F = (K^{-1})^T S(\mathbf{t}) R^{-1} (K')^{-1}
\end{equation}

The fundamental matrix captures all the available information from a pair of views between two different images.
The relative motion of the camera scheme is present in the used stereo camera setup, or namely two cameras with known caliberation in space. As we know the camera calibration matrices the normalized measurements can be described with a similar bilinear relation that includes the essential matrix $\mathbf{E}$. E can be estimated from the image measurements and it has a rank of 2 and it can be decomposed with a singular value decomposition (SVD). A normalization must be done in order to solve the epipolar geometry problem so that the solution is numerically stable \cite{Sonka07}.'
%Butterworth 97 also 

The fact that the essential matrix is a $3x3$ matrix with a rank of 2 means the projection of point X onto the image plane is a lossy transformation. 

Solving the overdetermined linear equation system in epipolar geometry requires a minimum of 6 image points. Noise affects the solution, and real image measurements use an algorithm called eight-point algorithm or a modification of it. The solution is minimised with least squares (Frobenius norm) effort. 
+ at least 8 points for a linear solution in 2 image system \cite{Sonka07}
+ at least 7 points for a linear solution in 3 image system (trilinear tensors used)

+ Mismatch errors 

+ 3D similarity reconstruction
    + X is found up to a scale and not Euclidean
    
+ case of unknown intrinsic and extrinsic parameters is not shown here
    + can do, depth from unknown video source

+ stereopsis => scene reconstruction in Euclidean reconstruction from 2 calibrated cameras

Using only two cameras instead of multiple cameras helps to avoid occlusion of objects and use existing powerful stereo processing tools.

+ 3 camera-setup can understand all available information in an orthographic projection => 4th camera does not add any more information
%page 473 Sonka07

\subsubsection{Intrinsic Camera Parameters}
\label{subsubsection:intrinsic_camera_parameters}

A camera model has a multitude of parameters whose amount depend on the selected camera and lens distortion models, and the number of calibration images used.

The camera model has internal parameters that are called the intrinsic camera parameters. The coefficients of the camera calibration matrix K \ref{equation:cameracalibrationmatrix} are the intrinsic camera parameters. If the coefficients of K are known, it is possible to extract metric values from the scene on the image plane $\pi$ \cite{Sonka07}.

If a more complex camera model is used, the matrix K may have larger dimensions, but the intrinsic parameters still are the coefficients of K.





A calibration technique that is unbiased and optimal is ideal. 

Originally, camera calibration algorithms were used in aerial imaging systems.


Many fast but inaccurate algortihms for parameter estimation have been introduced earlier. The increase in processing power of computing platforms helps to develop new non-linear camera calibration methods that are accurate. Heikkilä \cite{Heikkila00} claims that a subpixel accuracy of 1/50 of a pixel can be realistically achieved with a modern CCD imaging cell if such non-linear calibration is used.

+ Intrinsic camera parameters

+ Extrinsic camera parameters


\subsubsection{Extrinsic Camera Parameters}
\label{subsubsection:extrinsic_camera_parameters}

\subsection{Stereo Assumptions}
\label{subsection:stereo_assumptions}

Some basic assumptions about the measured crane working area must be made in order to use stereo correspondence algorithms. These assumptions operate on the collected images from the two stereo cameras. 

In general, the assumptions hold for any stereo camera setup that work on disparity principle unless otherwise stated. 

A single pixel must correspond to a single surface point in the world in order to construct a linear mapping function from the left stereo camera image to the right camera image. This restriction means that no opaque, or see-through, materials can be accurately measured with the used technology. Additionally, occluded and self-occluded surface pixels will be discarded in order to comply with the single pixel correspondence assumption.

+ A single pixel corresponds to a single surface point (restriction: no opaque materials | fish + fish bowl)
+ Disparity values are generally continuos (smooth within a local neighbourhood | discontinuities occur only at object boundaries)
+ Ordering constraint (if an object A is to the left of the object B in the left image, then the object A will be left of the object B also in the right image | sometimes violated by pole-like objects)
+ Lambertian surfaces => surfaces do not change appearance when viewed from another angle.
    + opaque
    + ideal diffusion
    + reclefts light energy in all directions

Stereo vision produces a dense disparity map . (If we compare to laser scanner sensor, the denseness of the produced point cloud is much higher)
Some requirements for the disparity map are elaborated in Zitnick et al. report, namely the disparity map should be smooth and detailed . The desirable result would provide smooth continuos mapping that detects small surface elements as separable regions. It turns out that these requirements are opposing to each other: a smooth continuos disparit map tends to filter out small details, and a detail preserving mapping is affected by noise \cite{Zitnick00}.

\subsection{Stereo Errors}
\label{subsection:stereo_errors}

+Embedded geometric and radiometric difficulties

In this work the machine vision system is used as means of extracting coordinate data from a pictured scene. Full recovery of the spatial 3d scene is not possible, because the stereo cameras only see the surface parts that project onto the 2d image plane.
In our case, a full recovery of a 3d object model is not needed to solve the problem of load positioning and dimensional measurement. As a result of depth mapping, a point cloud will provide coordinate data for the visible surface, which we can mask three-dimensionally and process further.


It can be seen from \ref{equation:disparity_limit} that the accuracy of such a stereo system is linearly proportional to the disparity measurement error, and there are trade-offs for most factors, such as focal length, camera separation, optical axes convergence, and aperture selection.

The lenght of the camera separation in a stereo camera system, also called the baseline width, affects the triangulation accuracy of the depth measurement setup to some extent. A common width for comparison is the human eye baseline width of X
% camera separation or baseline
+ hyperstereo = baseline much higher then the human vision system baseline
%=> distorts perception of motion for humans
%=> distorts perception of glossiness of materials
%=> also crosstalk = image leaks to the area of the other eye => visible ghosting in stereo perception


According to equation \ref{}


In standard stereo processing the object measurement error increases with items that are located far away. If a single baseline and resolution is used the error grows quadratically with increasing depth \cite{Gallup08}.

A simple equation for the depth error is introduced in \cite{Gallup08}:

\begin{equation}
\epsilon_z = \frac{z^2}{bf}\dot \epsilon_d
\end{equation}

where \emph{z} is the depth value, \emph{b} is the baseline value, \emph{f} is the focal length, and \epsilon_d is the disparity matching error (in pixels). The resulting depth error $\epsilon_z$ 


\subsubsection{Occlusion}
\label{subsubsection:occlusion}


Occlusion means partial or full loss of line of sight of an object in an image. In stereo matching, an object may be partially occluded by another object thus resulting in the original object surface in a stereo image A that is not visible in the stereo image B. The object may be occluded in only left or right stereo image, or partially occluded in both. In both cases, the other image has visible object surface that cannot be matched in the other image.

Occlusion is a big problem in stereo matching process. Most solutions do not explicitly detect these occluded regions 
Some methods:
+ multiple cameras and camera masking
+ bidirectional matching
+ 

+ occluding boundary is the surface of object that slides in the horizon so that the tangent of the surface is towards the viewer.

+ merging and splitting situations \cite{tikkanen13}
+ object tracks, initiating and terminating
+ 

\section{Camera Modeling And Calibration According To A Pinhole Camera Model}
\label{section:camera_calibration_according_to_pinhole_camera_model}

TODO: different camera models
1. pinhole camera model
2. orthographic projection
3. scaled orthographic projection
4. paraperspective projection
5. perspective projection
%source: NUS CS4243 camera.pdf slide

The camera model maps the transformation between the real-world object and the projection of the camera image plane. The camera model is a set of parameter that define this transformation as realistically as possible. 

The real-world object projection on the camera image plane is a lossy transformation where a lot of information is lost due transformation. For example, when a cube is projected on the image plane, we cannot compute the texture of all 6 cube faces anymore because the data of all non-visible cube faces is lost in the image plane.

Typical camera model is usually based on ortographic projection or perspective projection models. In 3D imaging a suitable camera model would be the perspective projection model with lens distortion modeling included. The selection really depends on the applications and its accuracy requirements for model accuracy, but in simple computer vision applications the linear ortographic model would be appropriate. 


\subsection{Pinhole Camera Geometry}
\label{subsection:pinhole_camera_geometry}


A pinhole camera will be the basis of the theoretical camera modeling since it is the simplest approximation that is suitable for a computer vision applications \cite{Sonka07}.

Some basic geometry items. There is an image plane. An optical axis lies perpendicularly to the image plane pointing out from the image plane towards the scene. 
Optical axis.
Focal point (optical point)

The pinhole camera includes a thin lens system that can be approximated with ideal formulation. 




The image sensor has width \emph{w} and height \emph{h} which relate to each other with aspect ratio \emph{a} so that

\begin{equation}
a = \frac{w}{h}
\end{equation}

We can relate the width of the image sensor with the field of view angle by equation

\begin{equation}
w = 2f\tan{\frac{\theta_{fov}}{2}}
\label{equation:width}
\end{equation}

If we consider a fixed-baseline stereo setup where baseline \emph{b} is kept constant a report by Gallup et al. easily shows how the depth error is related to the depth measurement in quadratic sense by equation

If we consider the effect of resolution on the disparity computing, or depth value computing, we can start with the number of pixels in the image sensor by equation

\begin{equation}
number of pixels = wh = \frac{w^2}{a}
\end{equation}

Furthermore, we can substitute width \emph{w} from \ref{equation:width} getting the equation for the number of pixels as in \cite{Gallup08}:

\begin{equation}
nop = \frac{{z_{far}}^4}{{\epsilon_z}^2} \frac{4\tan^2{\frac{\theta_{fov}}{2}}}{b^2 a}
\label{equation:zdepth}
\end{equation}

As we can see from equation \ref{equation:zepth} the increase in pixel resolution to match a specified accuracy also must match the depth resolution proportionally to ${z_{far}}^4$. Consequently, the required increase in resolution may be impossible due to engineering limitations of new hardware or computations limits of increased overheard due to increase in number of pixels.

The increase in stereo processing computation overhead is tightly coupled with the selected image resolution and depth error. According to \cite{Gallup08} the number of pixel comparisons needed is in the growth range of $ \Omega({z_{far}}^6 {\epsilon_z}^-3)$. For example, if we wish to extend the depth range by a factor of 3 the required number of comparisons would be $3^6 = 729$ times more computationally expensive.

When faced with this kind of image computation problem that should ideally be run in real-time the design of the imaging system must be carefully planned. It should be noted that increase in image resolution seems not to solve a stereo correspondence problem, but it may severely increase the computation time. 

The basis for camera modeling begins with the pinhole camera model where image coordinates \emph{u} and \emph{v} are transformed as

\begin{equation}
\Biggl[ \begin{array}{c}
\emph{u} \\
\emph{v} \\
1 \end{array} \Biggl]
= \Biggl[ \begin{array}{c}
\lambda \emph{u} \\
\lambda \emph{v} \\
\lambda \end{array} \Biggl]
= \textbf{F}\Biggl[ \begin{array}{c}
\emph{X} \\
\emph{Y} \\
\emph{Z} \\
1 \end{array} \Biggl]
= \textbf{PM} \Biggl[ \begin{array}{c} 
\emph{X} \\
\emph{Y} \\
\emph{Z} \\
1 \end{array} \Biggl]
\end{equation}

where a perspective transformation matrix \emph{P} is introduced as

%\begin{equation}
%\textbf{P} = \Biggl[ \begin{array}{cccc}
%emph{sf} & 0 & \emph{u_0} & 0 \\
% & \emph{f} & \emph{v_0} & 0 \\
% & 0 & 1 & 0 \end{array} \Biggl]
%\end{equation}

%+ The camera matrix

\begin{equation} K =
\Biggl[ \begin{array}{ccc}
Focal length 1 & Aspect ratio * Focal length 1 & Principal point 1 \\
0 & Focal length 2 & Principal point 2 \\
0 & 0 & 1 \Biggl] \end{array}
\end{equation}

+ Aspect ratio

%\image{lossy_transformation}

Camera calibration can be weak or strong. If strong camera parameters are known it is possible to calculate image scene Euclidian metrics. If weak camera parameters are known, only a pixel to pixel transformation from one image to another is possible, for example, an epipolar projection transformation can be done with weak parameters \cite{Rander97}.

\subsection{Single Camera Calibration}
\label{subsection:single_camera_calibration}

% original reference Zhang99

+ Why do we have to calibrate the cameras?
    + correct distortions
    + rectify images
        + Why do have to we rectify images?
    + improve performance of stereo corresponsdence algorithms
    
+ How do we calibrate a camera?
    + Air-glass interfaces in the lens system

Calibration is the means of linking the three-dimensional object and its two-dimensional projected representation in a scientific way. By modeling the image capture and projection with a camera, a camera model is presented. A camera model consist of multiple unknown parameters that need to be solved for a complete representation of the imaging sequence.

\subsubsection{Calibration Targets}
\label{subsubsection:calibration_targets}

Planar calibration grids are the most common calibration method for a stereo camera setup. Mainly checkerboard patterns and rectangular grid patterns are used for calibration purposes.

\subsubsection{Calibration Errors}
\label{subsubsection:calibration_errors}

+ Focal length estimation errors
It was found out in study by Kytö that the focal length affects the depth threshold of a stereo setup more dominantly than the baseline separation of the cameras. %what is really depth threshold 

+ Tangential distortion
+ Radial distortion (barrel effect)
It is important to compensate for lens system radial distortions, because they cause vertical disparity on the edges of the image. \ref{Kyto14}

+ Chromatic aberration

+ Affine distortion (aspect ratio)

+ Camera electronics, light intensity changes affect the phase locked loop (PPL) in the electronics => line jitter with sync signal systematic or random noise change

+ Calibration target location

\subsection{Stereo Camera Pair Calibration}
\label{subsection:stereo_camera_calibration}

The stereo camera pair calibration computes the rotation and translation between a stereo camera pair. This is also known as the extrinsic calibration of the stereo pair cameras. The calibration results in a trans-rotational matrix that describes the origin and direction of the optical axis of the camera in the other camera's sensor coordinate system.\par
A known calibration for each single camera is required if metricity data needs to be produced using the stereo pair calibration. This section will focus on stereo calibration where the intrinsic parameters of each camera are known. The stereo calibration with these parameters can be done most simply using a stereo image pair and the 8-point algorithm that solves the correspondence problem between two different viewpoints. 8-point algorithm computation from two images is simple and straightforward, but in order to obtain better results, many more linear correspondence equations must be added to the problem.\par

The OpenCV stereo calibration uses the same implementation used by the MATLAB Camera Calibration Toolbox by Bouquet. Bouquet's implementation mixes multiple techniques from other camera calibration original research including papers from Zhang, Heikkilä, Silven, and Tsai \ref{BouquetRef}. Basically, the technique used estimates the fundamental and essential matrices between two stereo images who contain calibration chessboard patterns. The calibration is based on the knowledge of chessboard crossing locations in high accuracy.\par



The convergence distance of the stereo system optical axes affects the perceived location of the disparity range. The convergence distance can be computer controlled \ref{Chen et al. 2010 from Kyto14}, but in this work the convergence distance is not approximated accurately since the collected stereo video is not meant to be recorded for human vision system viewing. Increasing the convergence depth will also increase possibility for frame violations for negative disparity values near the edges of the screen. A frame violation would break the 3d perception illusion for a viewer, but for a computer system a violation will result in only an erroneous depth match that is eliminated from the final disparity image.
%what is negative disparity? => in front of the screen => does this matter in a measurement system

+ toed-in or parallel camera setup

The stereo cameras can be physically rotated towards each other, or they can be fixed in parallel configuration so that the optical axes intersect at infinity.

A parallel optical axes alignment is preferred to avoid vertical disparity caused by keystone distortion correction. The toed-in camera setup also induces depth plane curvature, which is an unwanted warping phenomenon of the image.\ref{Kyto14}    

+ convergence distance of the stereo system optical axes: approx. 264cm




\section{Stereo Correspondence Problem}
\label{section:stereo_correspondence_problem}

It is important to choose a proper window size for stereo matching. In low contrast regions of the image too small a window cannot guarantee a unique match because of too little intensity variation. 
In general, a small window is desirable to avoid unnecessary smoothing, but optimal window size depends on the region intensity variation, texture, and disparity \cite{Zitnick00}.
The problems of window size can be solved with iterative window size methods, but increase in computation overhead or problems at occlusion boundary are still left unsolved \cite{Zitnick00}..

+ Feature-based Stereo Vision Algorithms
    + SIFT (slow)
    + SURF
    + MSER
    
\image{disparity_space_from_zitnick_report}

+ Uniqueness and continuity assumptions

% correspondence problem
+ the ambiguity of the correspondence problem can be reduced using one or multiple constraints such as
disparity limit constraint (disparity must be < limit, e.g. finger in front of the face cannot be imagined in stereo)
disparity smoothness (disparity changes only a little in any direction)
epipolar constraint (search space for matching pixels 1D on the epipolar line)
figural disparity constraint (corresponding elements should lie on an edge element in both images)
feature compatibility (physical origin of the matched points should be the same)
geometric similarity constraint (geometric features differ only a little)
mutual correspondence constraint (occluded points are not found thus ruled out)
ordering constraints (typically points lie on the same epipolar line for similar depth items, exception narrow close-up sticks)
photometric compatibility (little intensity differences)
uniqueness constraint (1 pixel can only correspond to 1 pixel in the 2nd image, exception two or more points on 1 ray in 1 image)

+ specularity edges cannot be used for feature matching because specular lighting changes depending on the viewpoint \cite{Sonka07}

Now, the point $U_c$ in equation \ref{equation:projectedpoint} can be represented in homogenous coordinate system as 

\begin{equation}
\tilde{U_c} = \Biggl[\begin{array}{c}
U \\
V \\
W \end{array} \Biggl]
\end{equation}

\subsection{Block Matching Techniques}
\label{subsection:block_matching_techniques}

\section{Depth Map Generation}
\label{section:depth_map_generation}

After the image rectification is done for a stereo image pair captured by the calibrated stereo camera setup, then the 3d point cloud generation becomes a problem of extracting the estimate of the disparity map (d(x,y).

Q is the reprojection matrix that encodes information about the stereo camera setup. Q matrix will provide information about camera optical axis convergence (parallel or cross-eyed), image plane principal points, and the stereo setup baseline width b.


\section{Load Object Modeling Quality}
\label{section:load_object_modeling_quality}

%fix passive
We are not required to fully model the load object in order to find its position and a bounding volume. 

Currently the produced point cloud only depicts the visible part of the load object since a normal stereo camera setup cannot generate a full envelop view of the object.

If we wanted to fully model the load object then additional camera viewpoints would be needed that cover all the occluded and non-visible load object surfaces, such as the backside. With a full stereo camera envelop we are able to reconstruct the structure using any available reconstruction algorithm. 

An interesting idea would be to use an adaptive reconstruction scheme that statistically models the object surface with a self-organizing map as described in de Medeiro's study\cite{Medeiros07}. For such a statistical modeling approach a normalized load object measurement for each discrete timestep would be needed as an input for the SOM model.

It would be possible to do surface reconstruction if we can access additional viewpoints and envelop the load object fully 


using adaptive iteration techniques in order to model the object in greater detail as we receive more information from other viewpoints, including its backside.


statistically estimate and model the object in time using self-organizing maps and adaptive geometric meshing .


The greatest challenge in computing the load object measurements and position is the low quality of the point cloud data received from the stereo camera rig. Due to limitations in e.g. block matching algorithm, filtering kernels, and changing environmental variables (such as lighting conditions) we may expect white noise to be present in the data for all discrete timesteps.

Countering the varying geometric signal is difficult and a matter of discussion in itself: do we wish to add filtering that may possibly lower spatial resolution of the depicted scene in order to receive a filtered signal? 



Since we are interested in the load object AABB measurements we are not required to normalize coordinates for 


%end of chapter 4
%===============================================================================================================================
\chapter{Coordinate Systems And Transformations}
\label{chapter:coordinate_systems_and_transformations}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{process_crane_load_object.jpg}
    \caption{The lifted load object in the process crane use case.}
    \label{fig:process_crane_load_object.jpg}
  \end{center}
\end{figure}

\label{chapter:load_object_registration_and_tracking}

\section{Image Coordinate Systems}
\label{section:image_coordinate_systems}

A widely accepted right-handed coordinate system will be used for describing a three-dimensional coordinate system. The details will be further discussed in the following sections. In general, the positive Z axis points out towards the scene in a two-dimensional image plane.

\subsection{Coordinate Transformations}
\label{subsection:coordinate_transformations}

The coordinate transformation matrices depicted in this thesis are three-dimensional Cartesian coordinate transformations who use a base coordinate denoted (x,y,z).

A point is a metric location in space that is uniquely defined with a three-dimensional vector in a chosen coordinate system.
A spatial coordinate has three parameters, namely x, y, and z, that will belong to a coordinate system indicated by a subscript character.

\begin{equation*}
\Biggl[ \begin{array}{ccc}
a & b & c \\
d & e & f \\
g & h & i \end{array} \Biggl]
\end{equation*}

The subscript depicts the coordinate space where a point is located. 
Also the transformation matrix has a subscript: it depicts on which coordinate space point (or vector) the transformation acts upon.
The transformation matrix has a superscript: it depicts to which coordinate space the point will end up transformed in.

\subsubsection{Geometric projection}
\label{subsubsection:geometric_projection}

A world point $X_w$ is translated and rotated into the camera coordinate system so that

\begin{equation}
\textbf{X}_c = \Biggl[ \begin{array}{c}
x_c \\
y_c \\
z_c \end{array} \Biggl] = R(\textbf{X}_w - \textbf{t})
\end{equation}

The point in the world according to the camera coordinate system now transformed to point
$X_c$. Next, the point will be projected onto the image plane $\pi$ according to the pinhole camera model. 
Point $X_c$ projects onto the image plane as Euclidean point $U_c$ where the projected point can be computed from the similar triangles of image \ref{image:projectedpoint}. 

\begin{equation}
\label{equation:projectedpointUc}
\textbf{U}_c = \Biggl[ \begin{array}{c}
\frac{-fx_c}{z_c} \\
\frac{-fy_c}{z_c} \\
-f \end{array} \Biggl]
\end{equation}

The image point that the camera outputs is the affine transformed $\textbf{U}_c$. Next, we must formulate a homogenous coordinate representation that allows to compute the affine transformation with a $3x3$ matrix multiplication for \ref{equation:projectedpointUc}.

We can represent a homogenous coordinate point in the image plane $\pi$ as 

\begin{equation}
\label{equation:affinedefinition}
\tilde{u} = \Biggl[ \begin{array}{c}
U \\
V \\
W \end{array} \Biggl]
\end{equation}

so that a Euclidean distance representation of point $\tilde{u}$ is 

\begin{equation}
\textbf{u} = \Biggl[ \begin{array}{c}
u \\
v \end{array} \Biggl] = \Biggl[ \begin{array}{c}
\frac{U}{W} \\
\frac{V}{W} \end{array} \Biggl]
\end{equation}

Now we have calculated the projected point \ref{equation:projectedpointUc} and we have determined that the resulting affine transformation results in $\tilde{u}$ in \ref{equation:affinedefinition}. We still need to define a principal point of the image plane $\pi$, which is the point where the optical axis and the image plane $\pi$ intersect. This point defines the homogenous transformation according to the intrinsic camera parameters. The principal point is defined in the affine image coordinate system as

\begin{equation}
\textbf{U}_{0a} = \Biggl[ \begin{array}{c}
u_0 \\
v_0 \\
0 \end{array} \Biggl]
\end{equation}

The equation for the affine transformation of projected point $U_c$ is

\begin{equation}
\tilde{u} = \Biggl[ \begin{array}{c}
U \\
V \\
W \end{array} \Biggl] = \Biggl[ \begin{array}{ccc}
a & b & -u_0 \\
0 & c & -v_0 \\
0 & 0 & 1 \end{array} \Biggl] \Biggl[ \begin{array}{c}
\frac{-fx_c}{z_c} \\
\frac{-fy_c}{z_c} \\
1 \end{array} \Biggl] 
\end{equation}

and with reordering the multiplication of $-f$ we obtain

\begin{equation}
\tilde{u} = \Biggl[ \begin{array}{ccc}
-fa & -fb & -u_0 \\
0 & -fc & -v_0 \\
0 & 0 & 1 \end{array} \Biggl] \Biggl[ \begin{array}{c}
\frac{x_c}{z_c} \\
\frac{y_c}{z_c} \\
1 \end{array} \Biggl]
\end{equation}

The obtained $3x3$ transformation matrix is called the camera calibration matrix K where

\begin{equation}
\label{equation:cameracalibrationmatrix}
K = \Biggl[ \begin{array}{ccc}
-fa & -fb & -u_0 \\
0 & -fc & -v_0 \\
0 & 0 & 1 \end{array} \Biggl]
\end{equation}.

We have followed the definition of Sonka et al. \cite{Sonka07} so that the resulting camera calibration matrix is an upper triangle containing three unknown parameters $a$, $b$, and $c$ for shearing and rescaling. If we study the coefficients of K carefully, we can see that the matrix actually contains the parameters focal length, the principal point, shear coefficient, and the affine distortion coefficients \cite{CaltechWeb10}.

Let us list the 5 intrinsic camera parameters in detail. The 5 intrinsic parameters and their associated coefficients of matrix K are 

\begin{itemize}
\label{list:intrinsiccameraparameters}
\setlength{\itemsep}{0pt}
\item -fa = represents scaling in the $u_a$ axis
\item -fb = shear coefficient that gives the alignment difference of camera coordinate system axis $x_c$ and affine image coordinate system axis $u_a$ at the length of focal length in pixels in the direction of affine image coordinate system axis $v_a$.
\item -fc = represents scaling in the $v_a$ axis
\item $u_0$ = u coordinate for the optical axis intersection on the image plane on the $u_a$ axis
\item $v_0$ = v coordinate for the optical axis intersection on the image plane on the $v_a$ axis
\end{itemize}

All the units are given in pixels.


\subsection{Prototype Transforms and Ordering}
\label{subsection:prototype_transforms_and_ordering}

Most of the matrix transformations are computed using three simple prototype transformations: translation, scaling, and rotation.

\textbf{Translation} of a point is given by a translation transformation matrix

\begin{equation}
\Biggl[ \begin{array}{c}
X^* \\
Y^* \\
Z^* \end{array} \Biggl]
= \Biggl[ \begin{array}{cccc}
1 & 0 & 0 & X_0 \\
0 & 1 & 0 & Y_0 \\
0 & 0 & 1 & Z_0 \\
0 & 0 & 0 & 1 \end{array} \Biggl]
\end{equation}

Using of square matrix is enforced here to enable multiple sequential transformations computation into a single transformation matrix. Additionally, a square matrix representation simplifies the notation a lot.

\textbf{Scaling} of a point is given by a scaling transformation matrix

+ scaling matrix definition here (2.5-8).

When is scaling used?

\textbf{Rotation} of a point is given by a rotation transformation matrix. A simple rotation matrix operates on a single axis at a time, and rotation about multiple axes is achieved with sequential single rotations about different axes of the three-dimensional image space. The outcome of multiple rotations depends on the order of selected axis rotations.

\begin{equation}
\textbf{R}_\alpha = \Biggl[ \begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & \cos \alpha & \sin \alpha & 0 \\
0 & -\sin \alpha & \cos \alpha & 0 \\
0 & 0 & 0 & 1 \end{array} \Biggl]
\end{equation}

Concatenating multiple transformations into a single $4 x 4$ matrix can achieve a simultaneous translation, rotation, and scaling. The ordering of the transformations depend on the matrix commutability, which in general is not a common property in transformation matrices, thus, the correct order of the transformations is crucial. 

It is obviously desirable to transform multiple coordinate points at a one time. Multiple coordinate point sets are called point clouds, and the transformation for a point cloud can be computed with the same square matrix used for single point transform.

If the point cloud $P$ contains $n$ points, we may structure a matrix P that contains the set of points as column vectors where

\begin{equation}
P = \Biggl[ \begin{array}{ccccc}
X_0 & X_1 & X_2 & \dots & X_n \\
Y_0 & Y_1 & X_2 & \dots & Y_n \\
Z_0 & Z_1 & X_2 & \dots & Z_n \end{array} \Biggl]
\end{equation}

\subsection{World Coordinate System}
\label{subsection:world_coordinate_system}

The world scene that the camera sensors are detecting is called \emph{the world} (or scene interchangeably), which maps to arbitrary world coordinate system with the coordinate convention $(X_w, Y_w, Z_w)$. Thus, the origin of the world coordinate system is the point $O_w$. The subscript $w$ is used with any point that belongs to the world.

The world is a three-dimensional system where a point lies in an Euclidian space $R^n$ and its span is $R^3$.
We acquire the world coordinate system orientation described in a report by Terho in 2010 \cite{Terho10}. Basically, the world x-y-surface can be fitted to the ground plane that is searched from coordinate point space. In this thesis, the absolute world coordinate mapping will be shown only if the information about the position and orientation of the camera platform will be available to use.

The world coordinate system axes may be referred to as world axes or scene axes. Interchangeably, they may be referred to as extrinsic axes of the camera.


The extrinsic parameters of the camera can be computed from the interaction between the camera coordinate system and the world coordinate system. Since the camera lies in some arbitrary world coordinates, and the detected scene changes according to the camera orientation, we can easily see that the extrinsic camera parameters change when the camera pose changes in the world coordinate system. The relation between the world coordinate system and the camera coordinate system can be computed with an Euclidean transformation consisting of a translation and rotation. 

+ Additionally, the world lighting conditions and weather affect the extrinsic parameters to some extent.?

\subsection{Camera Coordinate System}
\label{subsection:camera_coordinate_system}

The camera sensor array dimensions define the first two orthogonal dimensions of the camera coordinate system. The camera coordinate x-y-plane lies on the camera image array, and the z-axis points out from the image plane towards the camera lens system along the optical axis. The camera coordinate system z-axis coincides with the optical axis of the camera in an ideal situation.

The focal point of the camera defines the origin of the camera coordinate system, namely the focal point $O_c$. The subscript $c$ is used with any point that belongs to the camera coordinate system, mainly the physical points of the imaging array.

The camera coordinate system axes may be referred to as camera axes. Interchangeably, they may be referred to as intrinsic axes of the camera.

\subsubsection{Camera Image Plane}
\label{subsubsection:camera_image_plane}

The camera image plane is the projection plane where the three-dimensional point of the imaged scene is transformed with a non-linear transformation. The camera image plane is a two-dimensional plane that is orthogonally coincident with the optical axis of the lens. 

The camera image plane has its own coordinate system, namely $(x_i,y_i,z_i)$ coordinate system. We shall call the coordinate system a 

\subsection{Image Affine Coordinate System}
\label{subsection:image_affine_coordinate_system}

The affine coordinate system represents the shear (skew) and rescale aspects of the image Euclidean coordinate system \cite{Sonka07}. Shear and rescale options are usually called the aspect ratio of the image.  

The reason to introduce the image affine coordinate system is that pixels are not necessarily perfectly perpendicular. The correction for pixel dimension distortion is needed for applications that need high accuracy in measurements.

The image affine coordinate system introduces axes $u_i$, $v_i$, and $w_i$ that are not orthogonal. Axes v and w coincide with the camera coordinate system axes Y and Z, but axis u does not coincide with axis X, but is tilted. 




%end of chapter 5
%===============================================================================================================================
\chapter{Implementation}
\label{chapter:implementation}

The software implementation for the thesis work was prepared with the FAMOUS project team in Generic Intelligent Machines (GIM) research group. The implemented software includes many software nodes, mainly the stand-alone load object measurement tool that processes point cloud data into load object bounding volume information. Additionally, the point cloud data publisher node was implemented that integrates the HIMMELI camera platform hardware as the sensor input platform for the load object measurement tool.\par
The programming work done for the thesis was carried out from March 2013 to February 2014. In addition to standard C and C++ libraries, external libraries specialized in image processing and point cloud processing were used. OpenCV library by WillowGarage Inc. was used for image processing and stereo rectification purposes, Point Cloud Library (PCL) was used for all data intensive point cloud processing, and Boost library was used for all file system operations. All the data networking, runtime configuration, and other online networking solutions were implemented using the Robot Operating System (ROS).  \par
The design of the software was finished in December 2013 after successful compatibility testing between library functionalities and proofing of concepts with offline data sets. The final version of the load object measurement system that supports online perception sensors in the ROS network was finished in May 2014.\par
During the implementation period, new versions of most of the libraries became available, but the implementation uses latest currently stable versions: ROS Groovy and PCL 1.6 (compatible with ROS Groovy). If the software would be upgraded, then a newer version of ROS (Hydro) can only be used with the next version of PCL (1.7). PCL 1.7 does simplify some things, for example PCL visualization camera controls, and brings some extended features that could benefit the load object measurement system (for example, the new moment of inertia and eccentricity based descriptors functionality). Other than that, the old versions have all the functionality that the system needs, and they are successfully used to run the software at the time of writing this.

\section{Overall System Design}
\label{section:overall_system_design}

\subsection{Use Cases}
\label{subsection:use_cases}

The software component design in this thesis was planned modular so that different parts of the measurement, e.g. stereo rectification and object segmentation, can be developed independently. The full system design will consist of many sequential components, some of which will be designed and implemented by other researchers in the FAMOUS research project.

\subsubsection{Use Case Configurations}
\label{subsubsection:use_case_configurations}


Four different use case configurations (UCC) are considered for the evaluation of the machine vision system implemented. Different use case configurations are titled with a paired numbering scheme, e.g. 1-1 or 2-1. The use case configurations will describe the overview of installed hardware, used parameters, and output data. Additionally, each UCC has very different advantages and drawbacks, which will be discussed in brief detail.

The first use case configuration UCC 1-1 had a process crane setup where the stereo camera rig is attached to the crane trolley with cameras facing down. The crane trolley can move freely with cameras attached, and the camera image will show the crane load approximately in the same location at all times in the image center. Hoisting the load up will not decenter the crane load showing in the output image. Instead, the crane load will be shown enlarged.

The second use case configuration UCC 1-2 had a process crane setup where the stereo camera rig is attached to the extreme end of the crane bridge beam on the left. The cameras are facing the load when the trolley is located near the left end of the crane bridge and the load is hoisted half way. Hoisting the load up will show the load moving towards the upper part of the output image. Hoisting movement does not enlarge the load,  instead driving the crane bridge trolley near the cameras will enlarge the load image in the output image.


The third use case configuration UCC 2-1 had a small telescopic crane installed on a forestry machine with a log lifting tool. The cameras are attached on the right side of the forestry machine chassis. The cameras are facing the load when the load is hoisted off of the ground behind the machine. The working area of the telescopic crane is occluded on the left side of the telescopic boom. Thus, the crane load will be occluded when the boom is actuated to the leftmost visible working area. 

In this use case the crane load shape is considered always cylindrical.

The fourth use case configuration UCC 2-2 had a small telescopic crane installed on a forestry machine with a log lifting tool. The cameras were attached on top of the operator cabin slightly to the right side of the telescopic crane boom . The cameras are facing the crane working area, and the crane boom is mostly visible in the output image. The log lifting tool at the end of the boom is mostly visible. The crane load is mostly visible, but partly occluded by the telescopic boom. 

In this use case the crane load shape is considered always cylindrical.

\subsection{Online Software}
\label{subsection:online_software}

\subsection{Offline Software}
\label{subsection:offline_software}

\subsubsection{End Effector Tracker Integration}
\label{subsubsection:end_effector_tracker_integration}

Haar training is an object detection system that can detect features in a digital image. The first suggestion of using Haar training for feature detection was by Viola et al in 


There are a number of different boosting algorithms available, 
+ Discrete Adaboost
+ Gentle Adaboost 
+ Real Adaboost

All the mentioned boosting algorithms are identical in classification sense, but they differ in their parameter learning algorithms \cite{Lienhart03}.

A cascade of classifiers is a decision tree structure which is constructed out of simple classifiers called stage classifiers. The cascade of classifiers results in a positive detection of a feature if the test image can pass all the stages of the cascade.


Accordingly, we may estimate the probability for a false alarm rate given the probability of a single weak classifier detection and the number of stages.

\formula{false_alarm_rate}

The cascade of classifiers can be constructed to enable feature scaling and scale-invariant detection. If the test image is re-scaled, the computation still works in constant time since a look-up table can be scaled accordingly and used in the computation. The downside of feature scaling is the loss of accuracy due integer rounding. The algorithm is sensitive to rounding errors and the result can vary severely.


+ 20 stages used in \cite{Lienhart03}

\section{HIMMELI Sensor Platform}
\label{section:himmeli_sensor_platform}


HIMMELI sensor platform is a modular sensor rig that was created in Aalto university department of automation and systems technology in a research project in 2012. The sensor platform was created for use in future automated machinery research and its purpose is to provide flexible sensing of environment with multiple sensors.

With installed stereo cameras, HIMMELI sensor platform provides means for creating a digital model of the environment, including the model of the crane load that we wish to measure. This is done using stereo imaging that outputs a range image of the crane working area. The area that the rig can model depends on the tilt of the cameras used, distance from the working area, and many other parameters. 

Two main reasons for choosing machine vision as the medium were the limitations of the crane environment instrumentation possibilities, and the cost-effectiveness of a camera system. With HIMMELI, we have the option to use laser scanning, too, so it was a natural choice as a camera platform for testing and evaluation. 

%The very first problem we had was that the boom in the forestry crane cannot be sensorised. Or so the John Deere representatives say. %The sensor life-time expectancy is low in the vibrating environment, and the space for installing any sensors is very limited.

The rig consists of three modules that can be used separately or together. Most cranes have little available space to install anything,  so the smaller the installed unit the better. The largest frame on the backside of the rig is the computation unit, including two PC computers and a control cabinet. In front, the upper module includes stereo cameras, HD camera, and other cameras. The lower module has an IR light source.

%\begin{figure}[ht]
%  \begin{center}
%    \includegraphics[width=\textwidth]{himmeli.JPG}
%    \caption{HIMMELI sensor platform prepared for measurement session.}
%    \label{fig:himmelifull}
%  \end{center}
%\end{figure}

The sensor rig can be powered with a 230 V mains power or alternatively with a 24 V DC source. We do not consider the details of powering up the HIMMELI rig since the power-up sequence and details are irrelevant for the end user.

\subsection{HIMMELI Platform Cameras}
\label{subsection:himmeli_platform_cameras}

The HIMMELI sensor platform carries two industrial-grade GigE uEye UI-5120 RE HDR cameras in its camera module. The uEye camera is a high dynamic range camera with a CMOS sensor array in PAL resolution 768 x 576 pixels. The model currently in use (UI-5120RE-M-GL Rev.2) carries a monochrome sensor that outputs the image at 50 frames per second maximum. 

The camera weights 175 grams, which means it is easy to install in small space, but also the image quality may suffer in vibrating environments due to non-existent passive damping properties of the camera hull. The camera is installed with a shielding IP65/67 lens tube so outdoor operation even in winter condition is not a problem. All the components of the camera comply to the IP65/65 class, so with splashproof and dustproof properties the camera is well-suited for industrial imaging purposes.

The data is transmitted through Gigabit Ethernet with Opto I/O interface.

\subsubsection{Image Array Details} 
%remove section

NIT CMOS Array

The CMOS sensor is manufactured by Northern Institute of Technology Management and has an optical size of 5.75 mm x 7.68 mm. It has a color depth of 12 bits 

The dynamic range of the camera is logarithmic with a 120dB sensitivity. \cite{IDSImagingWeb13}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=5cm]{camera-gige-ueye-se-cmos-1.jpg}
    \caption{The GigE uEye HDR camera with no protective casing. Image from \cite{IDSImagingWeb13}.}
    \label{fig:gige-camera}
  \end{center}
\end{figure}

\subsection{HIMMELI Control Cabinet}
\label{subsection:himmeli_control_cabinet}

The control cabinet includes 11 different components that are listed briefly in table \ref{table:controlcabinet}.

\begin{table}
\caption{Control cabinet components in the HIMMELI sensor rig.}
\label{table:controlcabinet}
\begin{tabular}{|p{6cm}|p{5cm}|}
\hline
\textbf{Component} & \textbf{Details} \\
\hline
ATX power supply x 2 & One for each embedded PC. \\
\hline
Embedded PC 1 & Operating system Windows 7 \\
\hline
Embedded PC 2 & Operating system GNU/Linux
\hline
Gigabit Ethernet switch & \\
\hline
Power supply switch & 230VAC/24VDC \\
\hline
Fuse box & \\
\hline
Capacitor & For power change-over usage. \\
\hline
Wireless module & \\
\hline
Power ground box & \\
\hline
Selector relay & For power change-over usage. \\ \hline
\end{tabular}
\end{table}

in order to provide a flexible sensory platform for research 

is currently readily available for recording stero image data, and is located in Tampere university of technology 



The HIMMELI sensor rig is used to collect data from the environment. It can sense environment from a distance with the installed cameras, and sense the orientation of the platform with its inertial measurement unit, IMU.

HIMMELI includes

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item HDR stereo camera pair
\item High resolution camera
\item 3D LIDAR laser scanner \url{http://velodynelidar.com/lidar/hdlproducts/hdl32e.aspx}
\item Infrared (IR) camera
\item Automotive radar
\end{itemize}

Additionally, an IR light source and LED flash are included.

\begin{table}
\caption{Device and its connector type in HIMMELI sensor rig.}
\label{table:connectortypes} %lable just after caption
\begin{tabular}{|p{6cm}|p{5cm}|}
\hline % The line on top of the table
\textbf{Device} & \textbf{Connector Type} \\
\hline
High dynamic range camera & Gigabit Ethernet \\
\hline
High resolution camera & IEEE 1394b (FireWire) \\
\hline
Automotive radar & CAN bus \\
\hline
Thermal camera & CVBS (PAL) or serial bus \\
\hline
LIDAR laser scanner & LIDAR \\
\hline
GNSS receiver & Serial bus 1 \\ \hline
\end{tabular} % for really simple tables, you can just use tabular
\end{table} % table makes a floating object with a title

The rig has computing capability with an embedded Linux PC that is mainly used for data routing and sensor fusion. The rig can be remotely operated via a WLAN network.

The 3D LIDAR sensor is a Velodyne HDL-32 model that scans the environment with 32 micromirror controlled lasers.

The stereo camera setup installed on the rig has two HDR cameras.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{himmeli.pdf}
    \caption{HIMMELI Sensor Platform Schematic.}
    \label{fig:himmeli}
  \end{center}
\end{figure}

\subsection{Camera Platform Installations}
\label{section:platform_installations}

The HIMMELI platform was installed in two different test environments as described in the use cases. 

\subsubsection{Forestry Crane Installation}
\label{subsubsection:forestry_crane_installation}

\subsubsection{Process Crane Installation}
\label{subsubsection:process_crane_installation}

The process crane had two options for a HIMMELI sensor platform installation. For both options, the different modules of the platform had to be installed in their own location because of size constraints. Thus, the sensor module and the support frame module were taken apart from the main PC frame for the duration of the process crane installation. 

The first installation option was to attach the sensor module to the side of the moving bridge using a custom made metal frame. In this installation option the camera units are facing towards the crane working area from side and above in a bird's eye view manner. The laser scanner rotation axis points towards the opposite end of the moving bridge, aligning the laser scan lines along the longer length of the warehouse space (x axis of the warehouse coordinate system).

%\begin{figure}[ht]
%  \begin{center}
%    \includegraphics[width=\textwidth]{himmeli_installation_11.png}
%   \caption{Sensor platform installation in the moving bridge of a process hall crane in online test session.}
%    \label{fig:himmeli_installation_11}
% \end{center}
%\end{figure}

The second installation option was to attach the sensor module directly to the moving crane trolley housing, see figure \ref{fig:himmeli_installation_12}. In this installation option the camera units are facing the crane working area in a top-down manner with a slight angle to the warehouse floor normal. Now, the laser scanner rotation axis points towards the floor, aligning the laser scan lines along the shorter length of the warehouse space (y axis of the warehouse coordinate system).

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{himmeli_installation_12.jpg}
   \caption{Sensor platform installation in the crane trolley of a process hall crane in online test session in March 2014.}
    \label{fig:himmeli_installation_12}
 \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{himmeli_modules.jpg}
   \caption{HIMMELI platform modules: a PC module, a support frame module, and the sensor module.}
    \label{fig:himmeli_modules}
 \end{center}
\end{figure}

\section{Point Cloud Library}
\label{section:point_cloud_library}

Point Cloud Library, or shortly PCL, is a fully templated modern C++ library for 3d point cloud processing purposes. 
PCL uses optimizations such as Intel SSE with Eigen library backend, GPU CUDA processing, and parallel programming in order to maximize the computation efficiency on specific computation platforms.

\subsection{Limitations of Point Cloud Computing}
\label{subsection:limitations_of_point_cloud_computing}

Although PCL library is optimized for point cloud computing, the biggest limitation in point cloud processing currently is the slow processing speeds of some operations. The number of data entries per point cloud has a big effect on some operations, such as the K-nearest neighbour search.


One estimate for the processing speed of a single cloud into a bounding volume output can be done by looking into other implementations in other research teams' efforts. As all the computation for PCL processing is done using an Intel Core i5 3.2GHz CPU on a 64-bit system with 



\section{Robot Operating System}
\label{section:robot_operating_system}


%Additionally, different versions of ROS distributions are not cross-compatible in the same ROS network. That means we need to decide upon a single ROS version which we are using. This is also a problem, because it is easiest to run PCL on a ROS Hydro distibution, where the other parts of the software (e.g. the TLD tracker) runs on older ROS Fuerte distribution. The old software component code base may be difficult to upgrade, so quite probably we will use ROS Fuerte and run older versions of PCL on it.   

Robot Operating System, or shortly ROS, is an open source operating system-like framework targeted specifically for robotics application use. It was originally developed by Stanford AI laboratory in 2007 and the support is currently continued by Willow Garage Inc., who released the latest ROS Hydro release in September 2013. ROS is structured with a hierarchy of nodes, packages, stacks (legacy), and community-supported repositories, who provide the user with a large code base of readily available robotic applications. ROS system can also utilize a heterogenous computation network which is administered using a master-slave network setup.

ROS is mainly used in this thesis for its excellent capability of networked messaging through topics and subscribers. With a little effort we may use readily available data types for point cloud data networking, and interaction between popular operating system platforms, such as Windows and Unix environments. Data can be published via topics if we wish to disseminate it in the ROS network without receiving any feedback. In case we need feedback whether someone uses the data or not, ROS provides with a server-client scheme. 

ROS also includes a multitude of tools that can be used for data traffic verification.

For the networked data approach, ROS was considered as the number one framework choice in terms of cross-platform communications for the load object measurement system. Other options considered were an own implementation of the UDP communication protocol, some other real-time operating system, such as FreeRTOS, or an implementation with no networking capabilities at all. The other options were not as readily available for a full-fledged PC network in an industrial LAN setting as ROS was, thus, ROS was selected for use in the software development effort.

\subsection{ROS I/O}
\label{subsection:ros_io}

\subsection{ROS Naming Conventions}


The current naming convention is meant to protect from colliding topic, node, and parameter names. 

\subsection{Parameter Server}
\label{subsection:parameter_server}

ROS parameter server is a cross-platform shared dictionary service that enables parameter retrieval at runtime in a ROS network. It is one of the advantages of using ROS since it can be used for reliably configuring a multi-device network. The parameter server is implemented usig XMLRPC libraries in the ROS master node.

The data types supported by the XMLRPC library are supported in the ROS parameter server.
\begin{table}
\caption{Supported data types of the ROS parameter server.}
\label{table:parameterserver}
\begin{tabular}{|p{7cm}|p{3cm}|}
\hline
\textbf{Java Type} & \textbf{XML Tag Name} & \textbf{Description} \\
\hline
Integer & int & 32-bit signed integer, non-null \\
\hline
Boolean & boolean & 0 or 1, non-null \\
\hline
String & string & A string, non-null\\
\hline
Double & double & A 64-bit signed floating point number, non-null \\
\hline
java.util.Date & dateTime.iso8601 & A ISO860 timestamp with milliseconds and time zone information missing \\
\hline
java.util.List & array & An array of objects \\
\hline
java.util.Map & struct & Key-value pairs \\
\hline
byte[] & base64 & Base64-encoded byte array \\ \hline
\end{tabular}
\end{table}

Setting parameters in the ROS parameter server is done using an XML launch file when launching a ROS node. All the variables are evaluated before launching any nodes, and all information is uploaded to the parameter server before launching the nodes. We can force certain data types for the parameters if the data types are not unambiguos. Supported types in the launch file are \emph{str}, \emph{int}, \emph{double}, and \emph{bool}. Additionally, we may set parameters in child namespaces or fully upload the contents of a parameter file to the parameter server as text or binary.

\subsection{Other Features}
\label{subsection:other_features}

+ dynamic_reconfigure
+ rxlogger

\section{Camera Calibration Toolbox}
\label{section:camera_calibration_toolbox}

\chapter{Computation}
\label{chapter:algorithms}

\section{Bounding Volume Computation}
\label{section:bounding_volume_computation}

+Results from the software:
+ Big standard box dimensions
+ Mean X: 1.67 meters
+ mean Y: 1.47 meters
+ mean Z: 1.06 meters
+ Real values:

% oriented bounding box tree => find minimum bounding box, cut with a plane against main axis, find more minimum bounding boxes

%Keywords: bounding volume, collision detection, urban simulation, AABB (axis-aligned bounding box)

Bounding volume is the main product we wish to generate out of all the computation done in the PCL library previously. The format of the information output from the underlying software depends on the user, but we can present some suggestions for standard solutions. 

Usually, a volume of an arbitrary object is presented as a linear combination of simple geometric objects. These prototypes of geometric  objects support some or all of the linear operations available, such as summation, subtraction, multiplication and division.

One of the simplest bounding volume outputs is an axis-aligned bounding box, or AABB, applied in 3 dimensional Euclidean space. AABB is the minimum perimeter along the directions of the axes that span the Euclidean space that fully contain the target object. AABB is straightforward to find by computing minimae and maximae along each axis and by spanning a convex polyhedron so that we select all minimum values in one corner of the polyhedral graph, and we select all maximum values for the opposing farthest possible corner of the polyhedron.

AABB computation can be easily optimized, but the biggest drawback of this technique is the large amount of empty space not occupied by the object that the AABB volume contains. In the special case of stick-like objects, or planar objects, whose orientation is maximally off-axis aligned, an AABB volume representation will show a volume that is occupied less than 10 \% by the actual object.

Another simple solution is an oriented bounding box. If we first specify a main longitudal axis for an object, and then calculate its orientation, we can iterate the smallest possible bounding volume that is oriented along the object axis. This will tremendously help to more accurately represent a volume of special shaped objects, such as cylinders and planar items.

We can find more advanced bounding volume techniques for even more accurate object volume representation, such as bounding volume shapes, bounding volume hierarchies (BVH), discrete oriented DOPs, k-DOPs and boxtrees. For example, BVH can be used to detect collisions, or object interference, and it is computed using raytracing and culling. 


\subsection{Bounding Volume From 3d Axis-aligned Bounding Box}
\label{subsection:bounding_volume_from_3d_axisaligned_bounding_box}

+ possibly previously unknown object who is suspended from the crane end effector tool. The form of the output information should be simple while still carefully thought in order to 


It is computationally expensive to update the AABB bounding box as the load orientation changes via rotation, and it is suggested that in order to fight excessive computation, the bounding box should be loosely defined so that with small angle changes there is no need for recomputation \cite{Ericson05}.

\subsection{Bounding Volume From 3d Oriented Bounding Box}
\label{subsection:bounding_volume_from_3d_oriented_bounding_box}

The 3d oriented bounding box (OBB) is similar to the axis-aligned bounding box (AABB) with one difference: the bounding volume fills the cubic shape of the OBB optimally so that the bounding volume is minimized. This means that the OBB first computes a main axis for the load object and then the object frame is rotated along the axis. Now, the bounding box for the load object is seen as rotated in the world frame along the object, and the bounding volume becomes invariant to load rotation in the world frame.\par
The PCL library versions 1.7 and higher support a version of the oriented bounding box computation using moment of inertia and eccentricity based descriptors. With these descriptors the eigen vectors of the point cloud object can be computed, and the bounding box can be oriented along the largest eigen vector in a right-hand normalised coordinate system. The OBB along the major eigen vector axis does not guarantee minimality of the bounding volume, but the method does make the bounding volume computation more invariant to changes in the load object orientation in the world coordinate space.\par
Unfortunately, the current version of ROS Groovy does not include the moment of inertia and eccentricity descriptors required to compute the OBB. Consequently, future versions of the software running on ROS Hydro with PCL 1.7 and higher can improve the bounding volume accuracy by implementing the oriented bounding box computation. The AABB bounding volume computation currently used should be replaced with the OBB computation if the reported bounding volume accuracy is increased.

\section{Point Cloud Library Operations}
\label{section:point_cloud_library_operations}

\subsection{Object Segmentation}
\label{subsection:object_segmentation}
+ Euclidean Cluster Extraction

% the used scale of the point cloud point entries does have an effect on the computation time
% a computation processed using meters is much faster than computation on the same point cloud represented in micrometers, a million times larger values

\subsection{Point Cloud Filtering}
\label{subsubsection:point_cloud_filtering}

Point Cloud Library provides many readily available filtering products to be used for point cloud processing purposes. For example, filters that remove NaN values, voxel grid filtering, and outlier data point filtering are available online. Next, all the filters used in the pre-processing, and downsampling phases of the load area point cloud are introduced. The load area point cloud contains the depth data computed from the stereo cameras, which describes the nearby environment from a single viewpoint.

In pre-processing of the load area point cloud, a NaN removal filter, and a Inf value filter are used. The filters remove any values marked as infinity ($Inf$/$std::numeric_limits<float>::infinity()$), or not a number ($NaN$). The NaN removal is a PCL feature that was readily used, and the Inf removal was implemented in the work.

Next, the point cloud is downsampled using the voxel grid filtering scheme available in the PCL library. The voxel grid filter approximates the world with a voxel grid that is a grid of cubic 3d volumes representing whether a space is occupied or not. The accuracy of the voxel grid depends on the selected cube length. Consequently, the voxel grid filtering can be used to compute different fidelity levels for the environment representation (analogous to Gaussian image pyramids). For example, in the implemented software a value of 0.12 m was selected as the voxel cube side, which produces a point cloud presentation of the original point cloud downsampled to a 2.2\% size compared to the original point cloud size. The downsampled cloud that is significantly smaller than the original still contains the geometry data in decent detail. The subset of the downsampled point cloud that represents the load object is used to produce the actual load object measurement.

All data points that coincide in the same voxel grid voxel are downsampled into a single data point that is then output as the voxel grid cell result. Two different approaches are used in the approximation: the voxel cell can simply output its center coordinate as the result, or a more often used center of gravity (centroid) of the original point set can be used as the output result. In this work, the computationally more expensive centroid of the points approximation is used. The resulting downsampled point cloud will thereby consist of a set of centroids computed from the original point cloud that was divided in voxel grid cells.

% tähän voxel grid filtering kuva

+ Outlier Removal

\subsection{Ground Plane Estimation}
\label{subsubsection:ground_plane_estimation}

A parameter estimation technique was used to find the ground plane that is visible in the test data images. Parameter estimation is a technique for finding models in data sets. In this case, the parameter estimation tries to fit a model of a geometric primitive shape to a geometry data set. The geometric shape that is selected for the parameter model is in this thesis a plane or a cylinder model depending on the function. Shapes such as planes, cylinders, and spheres are usually used, but any model that can be presented implicitly using parameter model may be used. The existence of the shape in the data set is tested by finding inliers and outliers that determine whether the data set supports the model or not for some initial guess.

Classic parameter estimation techniques, such as least squares minimization, effectively optimize the fit of the selected parameter model using all points in the data set. In 1981, Fischler and Bolles presented a new parameterized model estimation technique called random sample consensus (RANSAC) that can improve upon classic techniques by detecting outlier points in the data, and by taking into account the gross errors they induce to the fitting algorithm \ref{Fischler81}.

While classic parameter estimation techniques incorrectly assume that most outliers are smoothed out in favor of good results, the RANSAC technique initializes the inlier point search with a small initial data set, and accumulates inliers when consistent data points are found. This way, not all data set points are used in the geometric fitting computation, and the solution is found easily even when gross errors are present in the data set. In the implemented test software, the RANSAC algorithm from PCL package is used, which finds the ground plane from point cloud data sets successfully for most cases. 

Originally, the RANSAC algorithm was used to solve the location determination problem (LDP), which states that for a set of control points whose 3d location is known, a location of the camera is computed when a sufficient set of control points are visible in the projected image taken from the unknown location. The LDP problem is actually the camera calibration problem, which was presented in chapter \ref{subsection:single_camera_calibration}. RANSAC can solve the external location of the camera according to landmarks seen in the image, but in this thesis RANSAC is used to find the ground planes from the geometric data sets. If the ground plane data points are located correctly and removed from the point clouds, then the remaining points are parts of visible objects in the scene. 

Some variants of the RANSAC algorithm are 
\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item M-estimator Sample Consensus (MSAC)
\item Maximum Likelihood Estimator Sample Consensus (MLESAC) 
\item Progressive Sample Consensus (PROSAC)
\end{itemize}

but there are many more depending on the estimator technique used. When RANSAC algorithm produces poor results in parameter estimation application, other variants can be used if they are more suited for the data set. Many options can be found in the literature. \ref{Torr00} \ref{Huber81}

While most of the time the RANSAC algorithm finds the ground plane correctly, there are some conditions that must be fulfilled in order to find a correct solution. First, the ground plane must contain the largest number of points in a planar fashion. Otherwise, some other plane will be found by the algorithm, such as a large warehouse wall.

\section{Algorithms}
\label{section:algorithms}

\subsection{Load Selection Algorithm}
\label{subsection:load_selection_algorithm}

\subsection{Segmentation Center Selection Algorithm}
\label{subsection:segmentation_center_selection_algorithm}

\subsection{Cylinder Growing Algorithm}
\label{subsection:cylinder_growing_algorithm}


\chapter{Evaluation}
\label{chapter:evaluation}
%Pohdintaa ja future workia

\section{Software Overall Design Evaluation}

The design of the load object measurement system evolved a lot during its implementation phase. The original design was a non-linear, component-based design running on a C++ code base. The early software schematic is presented in Figure \ref{figure:original_softwaredesign}. 

%figure:original_softwaredesign

Originally, a network of atomic processing components was designed where data would be sent to all the required processing stages using ROS topics and subscribers. Since the amount of data generated in the stereo cameras was quite large, the design had to be changed to a more linear design where the bandwidth of the ROS networked messages per stereo pair was minimised. The final design included more logical, larger processing components that had many positive effects on the overall performance of the software compared to the first designs: 

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item The overall workflow was more usable and easier to launch (less nodes were started)
\item Less bandwidth was used in the ROS network (less publishers and subscribers needed to be configured)
\item Data reduced and information increased per component principle was enforced by the new design   
\end{itemize}

This chapter evaluates the implementation of the final software schematic presented in Figure. The design proved to enable stable heterogenous PC network communications with the Himmeli platform on wired LAN architectures in tests. Tests were run in an actual overhead crane environment, and also in laboratory conditions with simple networks (no hardware routers). Many readily available ROS software packages for visualization, configuration and debugging tools were used to get the networks running as designed. These utilities proved to be of tremendous help in configuring and troubleshooting the network settings. No tests were run in a wireless network environment, but it is quite possible that the current system would be bottlenecked in a wireless network due high data bandwidth requirement. \par
For the selected stereo camera setup and tested networks, any amount of data could be relayed from any software component to the next one successfully by using networking with topics and subscribers. No issues were found, although some limitations posed by the architecture were discovered: 

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item The ROS network introduced varying length delays, as expected
\item Custom ROS message contents (LoadArea.msg) could not be viewed on subscriber machine that had no message headers built
\item The PCL visualization introduces large periodic delays of more than 1000 milliseconds 
\end{itemize}

Any compatibility issues were not discovered during multiple PC network testing. All the ROS installations used in the PC network were running on ROS Groovy, and no compatibility issues were found in the bounding volume measurement tool chain even with mixed build environments (catkin and rosmake). It is reported that different versions of ROS distributions are not cross-compatible, thus, ROS Groovy was selected as the ROS platform installation. \par
Currently, the ROS distribution functionality poses most issues for consideration in the overall design. The biggest risk in the project start was missing ROS knowledge in the project team, and it was known that the ROS programming learning curve is steep for beginners. The missing knowledge risk realised as longer development time of the system, but as such it was an acceptable slowdown in the process. Since ROS is a community-based effort to provide software nodes for robotic programming, a well-maintained index of all the available content did not exist, and it was sometimes difficult to find the right tools and learn how to use them. Tutorials and examples were used to implement all ROS node functionality, and some features, such as the dynamic reconfigure server, suffer from some unstable functionality.\par
On the other hand, the PCL library documentation proved to be excellent, easy to navigate, and very well documented. First, all point cloud processing features were implemented using PCL tutorials, which were excellent. Then, the code was further advanced with the help of the PCL API documentation. All point cloud processing functions fast, and the user can successfully interact with the visualization. The only hiccup in the PCL design was the visualization introducing periodic delays. This may be due large amounts of memory reserved and freed periodically, but a further analysis of the source of the delay was not possible in the scope of the thesis. A further development suggestion is to run memory management and process thread bug testing for the software to counteract errors in continuos operations. Another suggestion would be to branch out a version of the software where the visualization may be disabled after the machine vision system is properly configured. This would effectively enable the use of the client software node in near real-time continuos operations while eliminating the delay induced in the visualization.\par
In the end, the evaluation of the load object measurement system was done using offline data sets that were recorded from use cases in real crane environments. Unfortunately, the online implementation was finished but not tested during the thesis writing, and as such the online performance is not included in this work. The next section will analyse the data quality that was seen in the implemented software in more depth.

\section{Data Quality in the Machine Vision Process}
\label{section:data_quality_in_the_machine_vision_process}

The point cloud data quality was affected by the overall machine vision process including stereo calibration, disparity tuning, and the hardware setup. The hardware setup and the stereo calibration were used as monolithic initial calibration items. The calibration was done automatically for the hardware setup, and after a good basic point cloud output generation was achieved, no further engineering was done for the hardware or the stereo calibration. After an initial point cloud output was generated, the problem of understanding the data quality could be presented: how accurately did the point cloud data resemble the real world scene? \par
The machine vision process generated a point cloud that illustrated the 3d real-world scene viewed by the stereo cameras. The point cloud was constructed from the image areas that were visible for both cameras, and could be modeled according to section \ref{section:depth_map_generation}. For some areas the point cloud data resembled the real scene, and for some areas the point cloud did not look like what it was supposed to look like. The problem is how can this resemblance of the real scene be quantified by a machine? Is it possible to automatically get a value of how good the data describing the real scene is? This section tries to formulate a goodness measure for the output point cloud quality where most important factors of point cloud quality are 

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item Location accuracy of the surface 
\item Surface noisiness
\item Speckle regions, misinterpreted Z depth regions, and artefacts of any kind
\item Number of finite data point entries
\end{itemize}

Accuracy of the point cloud representation can be quantified using point cloud data quality indicators. Point cloud data quality indicators are used in modeling of urban constructed environments and in airborne LIDAR mapping etc. In these fields, the quality evaluation is done using two different goodness analysis mechanisms: spatial structural analysis and positioning analysis. The need for different quality indicators depends on the application at hand, but the constructed industrial environment encourages to try spatial structure analysis for indoor crane environment, too. Positioning analysis was not done in this thesis since multiple view registration was not used\ref{Feng08}.\par
In literature, also reverse engineering of shape primitives from point cloud response have been used for point cloud data quality verification in industrial settings. In industrial environments, a lot of box-shaped or cylinder-shaped objects can be found and be used for verification. Also, for any primitive geometric shapes it is fairly simple to quantify a goodness of fit value using geometric fitting schemes via least squares error minimization\ref{Fidera04}.
In spatial structure analysis, some real world feature qualities are measured from the environment, and the same qualities are then computed from the scene's point cloud response. Such qualities are for example: location, orientation, volume, smoothness of surface, or some other attribute of the item. Next, these measures are compared and it is possible to determine a goodness of fit -value that can be further processed into a goodness of data quality information. Of course, it is difficult to formulate a meaningful goodness indicator since point cloud data from stereo cameras always contains missing surfaces, measurement noise and errors increasing with more distant objects and other errors. Still, it is good to understand how much the reported measurement from the point cloud is off (in per cent) when compared to the actual object dimensions. \par
For position accuracy verification, landmark objects may be installed in the environment, and their correct locations can be searched for and verified in the point cloud data later. For orientation verification, eigenvectors may be calculated for a point cloud describing an object and the largest eigenvector direction can be compared to the real-world object main axis. For volume verification, an oriented bounding volume may be computed for fully visible objects, and compared to the real-world object volume.\par
In this work, installed landmarks or beacons would have been useful in the offline data sets since they could have been used to verify the correct scale of the point cloud, and correctness of local measured lengths. Since the data set environments did not contain any installed landmarks, a scale verification is not available. A further development suggestion would be to add landmarks objects (e.g. standard size boxes) in the environment and measure their locations prior to capturing the data sets to be able to do a better spatial structure analysis.\par
In this thesis, such objects whose qualities are known a priori are being called standard objects. For example, in the process hall data sets, a standard object titled \emph{large box} with dimensions of 1.52 x 0.94 x 0.92 meters was used. It can be seen in most process crane data sets as part of the setup. The large box object was constructed out of metal sheets, and it was a box-shaped, partially see-through mass weighing in 3.97 tonnes. In the forestry crane measurements, no standard objects were used, because the tree stumps were not measured nor tracked as individual standard objects. As such, the forestry crane data sets cannot be used for point cloud data quality control. In case a CAD model of the forestry crane boom was available, spatial structure analysis could be done for forestry data sets, too, but in this work no CAD model was available.\par
A single effective spatial structure analysis that was possible to do with the captured offline data sets was to check how big was the angle between standing cabinets and the floor in the process crane data sets. For example, a cabinet might be known to form a 90 degree angle with a floor, and such a scene would generate a point cloud response where the angle is not exactly 90 degrees (it could be e.g. 85 degrees). Obviously, this will give indication of goodness of structural integrity only for the local neighbourhood of the feature in the point cloud. For example, good structural integrity in the near-field feature does not guarantee similar accuracy for far-field features.\par
The effects of different stereo camera hardware setups, for example toed-in optical axes installation against parallel installation, were not researched and no definite results can be said of different stereo camera setups on the point cloud quality. A toed-in hardware installation was used in this work, and some vertical disparity definitely affected the point cloud quality, but it was a minor hindrance. By visual inspection, physical moving of the cameras and recalibration of the system generated no visible change in the point cloud quality. The toed-in setup does enable negative disparity values for z-depths that do not exceed the crossover depth for optical axes point of intersection, which was approximately at 1.6 meters. Toeing-in made the overlapping image areas smaller, but it also increases the stereo imaging volume to more near the camera than a parallel setup. Of course, unless disparity values less than zero are enabled, objects that are located in between the cameras near the image plane will still not generate points to the output point cloud. Using the disparity search space property in stereo processing, all the objects near the camera can be removed from the point cloud data response. This property was used to remove the hoisting wires from the output point cloud in the overhead crane data sets where the camera is installed in the trolley facing downwards. Hoisting wires that were more than 1.6 meters away from the camera still could make it into the generated point cloud, but their location was computed erroneously most of the time.\par
After the camera hardware setup was reconfigured, the toe-in angle became larger then previously. This resulted in a smaller region of interest in the rectified images, which means that the usable image area becomes smaller as the toe-in angle is made larger. OpenCV takes this change into account in the stereo rectification process, but as the rectified image becomes more and more distorted with increased toe-in angle, the region of interest becomes smaller, resulting in less accuracy in the reprojection phase.


It should be reported that one time after the right stereo pair camera was moved due a collision of the platform, the cameras were re-installed and a new stereo calibration was done for the HIMMELI platform. In the new hardware setup, the toe-in angle between the camera optical axes was larger then previously, resulting in a smaller overlap of the image regions because of very cross-eyed camera images. The point cloud data quality was affected so that the effective region of interest in the rectified stereo image pair was smaller than previously due to the toe-in angle increase. This was discovered when comparing the calibration files from the original camera setup to the calibration files computed from the new installation. According to the current knowledge by the author, the point cloud data quality was affected very little by the hardware setup change and a re-calibration. Still, more testing and a point cloud quality goodness measure needs to be formulated in order to verify the point cloud data quality indifference towards different hardware setups with calibration.

Measured height of the cameras installed from the floor was 545 cm.

\section{Parameter Tuning Evaluation}
\label{section:parameter_tuning_evaluation}

\subsection{Disparity Tuning}
\label{subsection:disparity_tuning}

The disparity map generated with standard OpenCV functions needed parameter tuning to provide the best possible reprojection of geometric data from a 2d image to a 3d point cloud. The basic tuning of the block matching algorithm (BM) parameters was done first, and then a semi-global block matching algorithm (SGBM) was tuned with additional parameters. The tuning was done with a custom software tool that displays slider ranges for each parameter. The tool was made for what you see is what you get (WYSIWYG) operation, and the resulting disparity image is displayed instantaneously next to the sliders.

\subsubsection{Block Matching Algorithm Tuning}
\label{subsubsection:block_matching_algorithm_tuning}

The block matching algorithm is tuned with 10 parameters who reside in a BM state object. The parameters are listed in table \ref{table:block_matching_state_parameters}. Additionally, a pair of regions of interest (ROI) are set up in the BM state to mark off the new effective image regions after rectification process. To be able to understand the parameter effects on the block matching performance, a quick note about the inner works of the algorithm is introduced next.

The feature matching works in three steps:

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item Pre-filtering to normalize image brightness
\item Correspondence search with SAD window along the epipolar lines 
\item Post-filtering to remove bad matches
\end{itemize}

The tuning of the BM parameters was done against offline data collected from both selected use cases. The forestry crane data was preferred in the tuning effort since more objects can be seen in the images, thus, the saliency content of the captured video is higher compared to the process hall crane area. High saliency image regions produce good results in block matching, and a rule of thumb would be: more visible 2d objects result in more correctly matched points in the region for the 3d point cloud. For example, the texture rate of change for a matt painted warehouse floor is small, and in the process hall crane case the collected data had minimal floor feature matching response for some BM tuning sets.

\subsubsection{Block Matching Parameter Effects on Feature Matching Performance}
\label{subsubsection:block_matching_parameter_effects_on_feature_matching_performance}

First, the tuning was started with a coarse selection of parameters to get a visible disparity map. Especially, the SAD window size was kept higher than normal to produce a lot of visible matched points. The most important parameters in the final BM state tuning set are SAD window size, number of disparities, and speckle range parameters. These parameters should be tuned first, and other parameters can be used for fine tuning of the disparity map afterwards. 

The SAD window size, or sum of absolute differences window size, affects the size of the sliding kernel window used for correspondence search in the 1d epipolar search space of the rectified images. The smaller the value, the smaller the sliding smoothing kernel used is, and with a minimum value the search is called pixel-to-pixel correspondence search. For metric measurement purposes the disparity map should be as accurate as possible, thus, the minimum value of the SAD window size will be used at all times. If the rectified images are texture rich, then a match can be found for all pixels on an epipolar line. Then again, if the image pair is less rich in texture, for example, a matt painted warehouse floor, the process will find less matches, which usually is the case. A low texture environment will produce little found matches because the local neighbourhoods of the sliding window are too similar in order to confirmed a match. If the point cloud is used for 3d visualization purposes, a higher SAD window size value selection should produce more points for the delight of the viewer.

The number of disparities parameter sets the maximum disparity difference in pixels for a search space in a disparity map. The higher the value, the higher a pixel difference value is allowed in the disparity map. For example, if a very near-field object has a disparity difference of N pixels, and number of disparities property is set smaller than N, then the near-field object is not detected at all in the disparity map leaving a 'hole' in it. This property can be used to remove near-field occluded objects that are impossible to triangulate successfully. For example, in the process hall crane case, the hoisting wires are problematic since they are occluded and visible only from one side in each camera. With number of disparities parameter it is possible to remove the wires from the disparity search space, and altogether from the final point cloud depicting the crane working area. 

The wires are included in the disparity search from certain height onwards, but with a proper selection of number of disparities it is possible to minimize the erroneous reprojection of the hoisting mechanism in the final output point cloud. The user can use the number of disparities property in combination with the minimum disparity property for setting the horopter - the 3d volume that is covered by the stereo matching algorithm \ref{Bradski08}.

Horopter can be enlarged towards the camera image plane with changes in the stereo system parameters. Some changes that enlarge the horopter are decreasing the stereo camera baseline width, decreasing the focal length of the cameras, and increasing the disparity search space.

Speckles are high and low disparity patches generated by the block matching algorithm near object boundaries. The block matching sliding window will catch object foreground and background in the image pair, which is the problem that generates speckle in the disparity map. Thus, the third important parameter is the speckle range parameter. The speckle range controls the threshold for letting the local patches of speckle be matched to the resulting disparity map. If the threshold is met, then the local region is eliminated and no disparity matches will be available for that region. The speckle detector that uses the speckle range value also requires the speckle window size parameter. The value for the window size should be small enough to keep the computation to a minimum, but large enough to detect speckle regions properly.

For each use case, multiple tuning sets were designed, which can be visually inspected by viewing at the point cloud output quality. On one hand, some processes will not work with a small number of output points, such as the RANSAC iteration for a ground plane search. On the other hand, the metric accuracy of the point cloud points decrease as the number of points in the cloud increase. The resulting point cloud from stereo cameras is always a best possible trade-off between accuracy, quality, and size of the cloud. Since the amount of point cloud points heavily affects the time used for further processing of the cloud, accuracy is the number one priority decreasing the computation times simultaneously.

Over time, the quality of the bounding volume measurement depends on many things: primarily the amount of noise in the point cloud object over time, and secondarily on the orientation of the object over time. 

\begin{table}
\caption{List of block matching parameters in a BM state object.}
\label{table:block_matching_state_parameters} %label just after caption
\begin{tabular}{|p{4cm}|l|l|p{4cm}|}
\hline % The line on top of the table
\textbf{Parameter Name} & \textbf{Min.} & \textbf{Max.} & \textbf{Requirements} \\
\hline
Pre-Filter Size & 5 & 21 & Odd Number \\
\hline
Pre-Filter Cap & 1 & 63 & - \\
\hline
SAD Window Size & 5 & 255 & Odd Number \\
\hline
Min. Disparity & -100 & 100 & - \\
\hline
No. Of Disparities & 16 & 256 & Divisable By 16 \\
\hline
Texture Threshold & 0 & no upper limit & - \\ 
\hline
Uniqueness Ratio & 0 & 255 & - \\
\hline
Speckle Window Size & 0 & 100 & - \\
\hline
Speckle Range & 0 & 100 & - \\
\hline
Disp12MaxDiff & 0 & no upper limit & - \\ \hline
\end{tabular} % for really simple tables, you can just use tabular
\end{table} % table makes a floating object with a title

The tuning effort resulted in best possible BM parameters for each use case. The process hall crane parameters are listed in table \ref{table:process_hall_crane_bm_parameters}

\begin{table}
\caption{Best BM parameter tuning set in process hall crane case.}
\label{table:process_hall_crane_bm_parameters} %label just after caption
\begin{tabular}[c]{|p{4cm}|l|}
\hline % The line on top of the table
\textbf{Parameter Name} & \textbf{Value} \\
\hline
Pre-Filter Size & 9 \\
\hline
Pre-Filter Cap & 63 \\
\hline
SAD Window Size & 5 \\
\hline
Min. Disparity & 0 \\
\hline
No. Of Disparities & 96 \\
\hline
Texture Threshold & 270 \\ 
\hline
Uniqueness Ratio & 20 \\
\hline
Speckle Window Size & 42 \\
\hline
Speckle Range & 10 \\
\hline
Disp12MaxDiff & 10 \\ \hline
\end{tabular} % for really simple tables, you can just use tabular
\end{table} % table makes a floating object with a title

\begin{table}
\caption{Best BM parameter tuning set in forestry crane case.}
\label{table:forestry_crane_bm_parameters} %label just after caption
\begin{tabular}[c]{|p{4cm}|l|}
\hline % The line on top of the table
\textbf{Parameter Name} & \textbf{Value} \\
\hline
Pre-Filter Size & 9 \\
\hline
Pre-Filter Cap & 43 \\
\hline
SAD Window Size & 13 \\
\hline
Min. Disparity & 0 \\
\hline
No. Of Disparities & 160 \\
\hline
Texture Threshold & 0 \\ 
\hline
Uniqueness Ratio & 21 \\
\hline
Speckle Window Size & 44 \\
\hline
Speckle Range & 17 \\
\hline
Disp12MaxDiff & 1 \\ \hline
\end{tabular} % for really simple tables, you can just use tabular
\end{table} % table makes a floating object with a title

\subsubsection{Semi-global Block Matching Algorithm Tuning}
\label{subsubsection:semiglobal_block_matching_algorithm_tuning}





\subsection{Segmentation Tuning}
\label{subsection:segmentation_tuning}

\section{Experiments}
\label{section:experiments}

\subsection{Overhead Crane Case}
\label{subsection:overhead_crane_case}

The following overhead crane image data sets were captured in August 2013 in a real overhead crane environment. A total of 13 data sets were captured that contain different kinds of tests aimed at revealing how the dimension measurement of the load object can function in different scenarios. The lighting conditions did not change during daytime in the overhead crane environment, so there was no need to test at different times of day, or different times of year because of possible changes in the environmental conditions. 

\subsubsection{Findings}

In the overhead crane data sets, a manual load selection approach was used to select an object as the target to measure. This was done when the end effector location was not available for more accurate selection process. Essentially, a load object was selected from near proximity of a pre-defined coordinate. Most of the time, the manual selection coordinate was set in the center of the image, where a load object would probably appear in e.g. the top-down camera configuration. Since no heuristics were used in the overhead crane load selection process, the test design introduced a lot of erroneous load object matches, such as people walking nearby the center of the image, or new objects selected as they came in the view as the platform moved in the process hall. These may be easily seen in the data set visualisations.\par
The decision to select the load object from a pre-defined 3d region of space was far from perfect, but it worked as a load selection condition without knowledge about the end effector location. In essence, it was guessed that a load object would be found near the center of the image, which was not always true. For example, for some data sets where the camera was located in the side of the overhead crane bridge, the load object was found in the bottom part of the image. A visual indicator for the manual load selection coordinate was added in the visualization tool to help set the manual selection center in a better location.\par
The data set coordinate system was not transformed to the world coordinate frame, but kept in the sensor frame at all times during data processing. The reason to do processing in the sensor frame was that the sensor frame implicitly encoded the moving platform signal in the data. An end effector location signal would have been needed if the processing was to be done in the world coordinate frame. Now, because such a signal was not available, the data was kept in the sensor frame, and as a result, a static coordinate depicted a point moving in space with the sensor platform. Also, this way the static camera implicitly encoded the platform movement, thus, no camera viewpoint signal was needed to construct. While the data in the sensor frame was not normalized, it did not introduce singularities or any mathematical problems that very large numbers tend to introduce in some complex algorithms. If the computation was done in the world frame, normalization would possibly be needed to prevent computational singularities if the origin of the framework was set far away from the actual overhead crane hall.\par

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
% 1372841000
\item Objects in the process hall are segmented in good detail. Geometry is well preserved in top-down camera views, and less well in the other used viewpoint (bird's eye).
\item Nearby objects, such as the upper balcony, can be detected by the machine vision system.
\item Some artefacts in the disparity map generate erroneous 3d constructs in the point cloud that are not present in the environment. In all the data sets a white bar can be seen on the right side of the disparity map, and its reprojection, which was a curved tall bar-like feature, can be seen in all 3d point cloud images on the right. A smaller image ROI could possibly remove such artefacts from the end result. 
\item A horizontal striped tape present in the data sets resulted in erroneous depth reprojection, and left a visible artefact under the floor level in the point cloud.
%1372842312
\item Nearby objects are grouped together as single objects by the segmentation algorithm.
%1372842573
\item Empty end effector gives such a small point response that it is discarded as not an object. It was not possible to track with the current system.
\item People near the load object merge into the load object and enlarge the reported bounding volume.
\item If the load object was lifted very high, it occluded most of the floor space in the image. If all the floor space is occluded by the load object, the floor removal will fail. Such an event was never encountered in testing.
%1372842898
\item The scene reprojection introduced large errors when the platform was moving. It was concluded that since the stereo camera head shutters are strictly not enforced to expose the scene at the same time, a moving platform will introduce a disparity map generation that does not match with the pre-defined camera baseline since the other camera is moving before it records the image. To counteract such inaccuracy, the platform should not move while measuring objects, or the shutter behaviour should be strictly enforced. For normal operation where the object is measured as it is lifted up, this is not problematic.
%1372843166
\item If people walk exactly under the top-down camera, their point response is so small that the response is discarded as an error in the point cloud data. This suggests that the lower limit of the point cloud object size should have been even smaller than the used value.
% 1372843682
\item If scene reprojection issues are present when the platform is moving, a lifted load object does not suffer from much inaccuracy because its image is not moving when the platform does. The errors introduced in the load object measurement while the platform is moving are negligible.
% 1372851661
\item The reprojection issues while the platform is moving depict the floor both too far away and too close in turns. This suggests that the HIMMELI platform shutters do not exposure images in the same left to right, or right to left order, but the order changes randomly due to network lag or other sync issues.
% 1372851907
\item In the bird's eye viewpoint the load being lifted in the background of the process hall results in a smaller point response than in the top-down view.
\item In the bird's eye viewpoint, the load object point cloud merges with the back wall point cloud when the load is near the wall. The load object could not be measured if it merged with the back wall. Merging is easily seen in the background because more noise is in features that are further away from the cameras. 
\item In the bird's eye viewpoint, the load object is self-occluded, and its bounding volume does not contain its backside. Thus, the bounding volume computation is not correct from this viewpoint for e.g. the large standard box.
\item In the bird's eye viewpoint, the hoisting wires are clearly visible in the point cloud output and the location is correctly reprojected in 3d.
%1372852148
\item In the bird's eye viewpoint, more points are generated to the 3d point cloud mainly because of the visible back wall. Increased number of points increased computation times.
\item In the bird's eye viewpoint, the distant objects introduced increasing noise and measurement error.
%1372852419
\item In the bird's eye viewpoint, the bounding volume will contain the hoisting wires that are attached to the actual load object. The hoisting wires will enlarge the bounding volume unnecessarily. The extra reported volume could be removed using knowledge about the end effector location and heuristics.
\item In the bird's eye viewpoint, the floor is not fully removed by the RANSAC search, because in the distance the noise spreads the floor's point response a lot. In data set 1372852419 the floor seems to curve up (objects may be seen above the floor level), which may be a result from calibration issues.
\item In the bird's eye viewpoint, the back wall is curved, which possibly is a result of a calibration issue near the image border region.
%1372852802
\item In the bird's eye view, the load object can be lifted out of the picture near the walls.
\item In the bird's eye view, the load object merges with the floor even though it is already lifted (near the opposite wall).
\item In the bird's eye view, the geometry of items near the camera are extracted accurately.
%1372853387
\item Partially see-through items are not registered (not seen) by the machine vision system because the block matching is not working in such an image region.
\item Shiny (e.g. metallic) items are difficult to block match.
\item Upright pole-shaped objects can be seen from the bird's eye viewpoint, but not very well from top-down cameras(for example, road construction markers).
%1372853605
\item See-through or shiny objects may be measured if non-reflective large markers are added on them so that they generate a visible point response in the point cloud. Still, the measurement on such an object is not possible with the currently implemented system used in this thesis. 
\end{itemize}

The effect of the fluorescent lighting in the environment was not analysed since the lighting conditions could not be controlled. The findings of this chapter were done in normal lighting conditions that may be found in a lot of warehouse environments. The fluorescent lights have some effect on the image quality, but it is unknown how the HDR cameras tune the image with the prevalent lighting conditions present in the data sets. The accurate effect of the lighting conditions to machine vision image quality are a vast research topic of their own.

\begin{figure}[ht]
  \begin{center}
    \includegraphics{dimension_measurements_box_ave.png}
    \caption{Dimension measurement data of a standard box from data set 1372841000 containing process hall data.}
    \label{fig:dimension_measurements_box_ave}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics{dimension_measurements_box_ave.eps}
    \caption{Dimension measurement data of a standard box from data set 1372841000 containing process hall data in EPS file format.}
    \label{fig:dimension_measurements_box_ave_eps}
  \end{center}
\end{figure}

\subsubsection{Analysis}

In order to verify the accuracy of the measurement, the output value of the geometry measurement software must be compared to a value produced by an actual measurement with e.g. measuring tape. In the overhead crane case, only the bounding volumes provided by the top-down camera viewpoint produced meaningful results, thus, the results of the top-down viewpoint will be primarily presented.
Figures x and y are screen captures from the data set 1372841000, which was used to analyse the bounding volume performance for the large standard box. As seen in figure x, the actual object lied on the ground so that the object was rotated about Y axis in a 34 degree angle compared to X-Y -plane. Now, the axis-aligned bounding box computes the maximum and minimum values along an axis, which results in a maximum projection along the axes in each dimension. The axis projection makes it difficult to verify that the measurement actually is correct, but we may estimate how much the measurement differs from reality for example in per centage. If a working online version of the software was available, it would be easy to rotate the standard box into correct orientation, and tune the system so that a correct measurement is given, provided that the calibration is good. Unfortunately this is not the case, and only offline data set may be used for verification in the thesis.
Now, a comparison criterion may be formulated that relates the output value of the geometry measurement software and the actual measures of the large standard box. The mean output values computed using MATLAB for the large box are shown in table \ref{table:large_standard_box_mean_values}.

% put here a table that shows delta X: 1671.6 mm delta Y: 1473.2 mm delta Z: 1058.5 mm
% put here a table that shows X = 152 cm Y = 94 cm Z = 92 cm
% why is the delta Z smallest when delta Z is in the sensor frame.... is it?

If the standard box is rotated 34 degrees, a geometric calculation using Pythagoras theorem results a correct bounding volume size of 1787 mm x 1875.42 mm x ???

In dataset 1372852802 an interesting phenomenon is visible in the visualisation of the data. In the end of the data set it seems like there is an object that continuosly changes size especially in the Z-axis direction. When the data was reviewed, it turned out that the load object was lifted near the ceiling, and as the load object was lowered, the hoisting wires become visible more and more as the load object is being lowered. Thus, the viewpoint generates this kind of continuously changing parameters that describe the bounding volume of the visible load object and the hoisting wires of the crane. 

% insert picture from data set 1372852802


\subsection{Forestry Crane Case}
\label{subsection:forestry_crane_case}

The forestry crane image data sets were captured in August 2013 and readily analysed in April 2014. A total of 30 data sets were captured in March 2013 and August 2013, but only 11 data from August 2013 sets were usable. The reason to this was that the winter data sets did not contain both left and right stereo head images, but only the left stereo image doubled as left and right pictures, resulting in loss of depth information. This setback was unfortunate, which is why the winter footage was not analyzed and the effects of reflective snow and winter conditions are not included in this thesis. On the contrary, the summer footage is excellent, and the computation using the captured imagery taken in August 2013 provided many useful insights into realising a measurement system that could provide the needed dimensions of a load object.

\subsubsection{Findings}

In the forestry crane data sets, the automatic TLD tracking was used in combination with a cylinder fitting scheme to provide more accurate results. It is good to understand that some of the findings in this section are exclusively based on the cylinder fitting results and its implemented heuristic checks.

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item Objects too close to each other will fuse as single object groups in the generated point cloud response. For example, if a person is in touch with the load object, the bounding volume will be enlarged to contain both the load object and the operator as a result. This phenomenon can be controlled using the kD-tree cluster tolerance if no occlusion is present in the image data.
\item Heuristics should be used to prevent cylinder fitting match wrong features from the environment. A check that the cylinder is not located above the end effector in the sensor frame was used, but it is not enough to prevent all erroneous matches.
\item If the end effector tool is empty, the cylinder fitting does not produce meaningful results. Currently, it will give fully or partially the volume of the end effector tool or parts of the crane boom depending whether the RANSAC search finds a cylinder fit in the data. This behaviour adds noise and erroneous measurements in the current AABB bounding volume data graphs as seen in the attachments section.
\item A rotation of the log perpendicularly to the optical axis will report different bounding volumes for different angles of rotation. The measurement is most accurate when the length of the log is fully shown in the image plane. Let this angle be the 0 degrees angle. A rotation of 0-65 degrees will give similar results, but a rotation of more than 65 degrees will cause a lot of self-occlusion of the log and generate a very poor object model. In the case of 90 degree rotation (maximum self-occlusion) only the other end of the log is visible, and in this case, it was not possible to measure the size of the log at all.
\item High smoothness constraint of the disparity map fills holes that are present in the environment. For example, the loops formed by the wiring seen above the end effector tool will generate a solid surface point cloud response. This phenomenon is present in the SGBM algorithm implementation, but not in the BM algorithm implementation. It was concluded that while the SGBM algorithm produces better 3d models of the environment with a high point response output, the BM algorithm actually can have finer detail in some features. In the end, the SGBM algorithm was more suitable for the development of the prototype software.
\item The cylinder fitting can handle picking of multiple logs at once. Multiple logs tend to be in a more hour-glass shape than a cylinder shape. Because of this, the cylinder fitting can fit most volume of the logs inside a single cylinder, but the furthest ends of the logs will stay outside the bounding volume. This is possibly problematic, but no verification was done in a real situation because the crane environment was not available after August 2013. It is possible that the enlarging effect of the disparity map generation increases the bounding volume so that the single cylinder fitting problem for multiple logs is not important.
\item The log can be measured even before it is grasped by the end effector. The measurement relies on close proximity constraint, thus, the log does not need to be actually gripped by the end effector before it can be measured. Further research is needed to verify usefulness of this phenomenon, but potentially the camera system can measure the log on the ground even before it is grasped, which can eliminate the need to pick the log up or rotate before a successful measurement.
\item If the load object is above a pile of logs, or touching or occluding nearby objects, the cylinder fitting will easily be disrupted by wrong features without heuristic checks. It is especially easy to fit a cylinder on a human or other logs.
\item People walking in the area are not easy to distinguish as their separate entity objects if they can jump over other objects, or be in their very near vicinity. For example, humans walking over logs are difficult to separate from the group of logs. If people are walking in the area where little or no other objects are in the near vicinity, then they are easy to separate as single objects.
\item As a result of some earlier findings, a person standing in close proximity to the end effector tool will easily be measured as the cylindrical load objects. Heuristics should be used to prevent mismatched load selections, for example: cylinders that are in upright position in the world frame will not be selected as load objects.
\item Static objects will generate more accurate data and provide means to statistically analyse the goodness of 3d model generation
\item The load object cloud only contains the surfaces visible to the camera, which will introduce errors in the cylinder fitting since only half a cylinder can be recovered as a point cloud. A RANSAC shape model data fitting will give an estimate on the log pose and volume, but it cannot recover any knowledge about the actual backside of the log. Another camera viewpoint and point cloud registration should be used to recover any knowledge about the backside of the log.  
\item If the crane boom is fully visible in both stereo images it can be measured and modeled. In normal operation with the current viewpoint, the crane partially goes out of the picture, which introduces challenges to continuos tracking. Another viewpoint that fully shows the crane boom must be used in case the boom modeling and tracking is needed.
\item It is possible to measure the state of the currently used end effector tool from the point cloud response. It would be possible to implement states that report whether the end effector is closed or open, and whether it carries a log or not. In some orientations of the end effector tool, it is a difficult problem, and in some orientations a fairly easy problem.
\item An occluded log cannot be measured properly. For example, if 50\% of the length of the log is visible in the image, then approximately 50\% of the actual length of the log can be measured by the machine vision system.
\item BM algorithm runs a lot faster than the SGBM algorith and produces similar results.
\end{itemize}

Some improvements to the implemented system may be instantly thought of. For example, the heuristic checks that select the correct load object can easily be improved. For example, if the distance between the candidate load object and the end effector is too large, then the candidate should be rejected. An improved heuristics model was not possible to implement during the thesis study due time constraints, thus, it is left as a suggested improvement in the works.

\chapter{Discussion And Conclusions}
\label{chapter:discussion}
% For Conclusions:
%Write down the most important findings from your work.
%Like the introduction, this chapter is not very long.
%Two to four pages might be a good limit.

\section{Machine Learning Extras}

Object recognition works on a classification principle: a classifier detects attributes in an object and works out into which object class it belongs to.

+ classification principle
    * statistical classifier
    
+ knowledge representations
    + syntactic pattern recognition
        + relations and description language
+ machine learning
    + neural nets
    + genetic algorithms
    
+ graph matching

+ fuzzy systems

%machine learning
A classifier is a concept of machine learning that is utilized in Haar training. A single classifier itself is a weak member of a more powerful committee of cascaded classifiers \cite{Freund96}. 

A number of simple, computationally light classifiers seem to outperform strong classifiers such as neural networks in principle \cite{Lienhart03}. 

In 2002, Lienhart and Maydt \cite{Lienhart02} proposed improvements upon the original work of the authors of Haar training classifiers. The Intel labs research team added a rotated feature classifier, which improves upon the performance of the original simple feature classifiers. One year later, they analysed different boosting algorithms in Haar training and concluded that Gentle Adaboost method performed the best in the training. 

The feature search space in Haar training is large in an image. The computational complexity depends on the image resolution, and on the chosen prototype of a feature.  For example, the number of features for a small window size of 24x24 is reported to total up to 117,941 features\cite{Lienhart03}. 

The computation of a single feature can be fast. The journal by... states that it can be done in constant time.

complexity for a single feature is constant. 

Currently, the 

+ 14 Haar-like features
+ 4 edge features, 8 line features, 2 center-surround features, 1 line diagonal feature
+ 

A lot of time was used for working on the ROS implementation of the load detection software. The software has a simple backbone implementation thanks to the well-documented readily available OpenCV and PCL libraries. Some of the code runs advanced algorithms underneath, such as RANSAC model iteration, but these are not visible for the user. It would have been possible to implement better algorithms for higher level functions, such as load detection computation and load geometric measurement computation, than what we are currently running on. Due to limited resources we decided to keep the algorithms simple and instead realised a concept software that will run with a supported camera setup. This way we got a working platform that can be used as a testbench for future upgrades and as a framework for implementing more advanced algorithms.  

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{taakka_tunnistettu1.png}
    \caption{Cylindrical load detection in the FAMOUS software using very low quality scenery input data.}
    \label{fig:load_detected1}
  \end{center}
\end{figure}

We lifted a tree trunk with a forestry machine crane and processed the video output with the FAMOUS software. A screenshot of the resulting output from the visualizer can be seen in Figure \ref{fig:load_detected1} where the parameters for stereo matching and 3D segmentation were not tuned optimally. The segmentation of the scene into objects turned out to be quite robust against missing data, fluctuating point clouds, and bad parameter values. The robustness is partly due to a good automatic stereo parameter functionality of OpenCV that was shamelessly used to obtain the stereo parameters. Other reasons for good robustness are the smart handling of uninitialized data in PCL library, and the software architecture that does not differentiate X, Y, and Z directions - no information about orientation of the scene is utilized in 3D segmentation. 
    Then again, we can generate more accurate information with optimal parameter sets, and the measurements from the load object will change for each timestep unless we use adaptive filtering to fight out the white noise and orientation differences in time. We have considered a Kalman filter that tracks the estimate of the actual bounding volume signal, but is was not implemented for simplicity.
    
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{taakka_tunnistettu2.png}
    \caption{Cylindrical load viewed from a virtual camera from across the crane working area.}
    \label{fig:load_detected2}
  \end{center}
\end{figure}

One problem present in Figure \ref{fig:load_detected2} was that the forestry crane boom belongs to the segmented load object after initial processing. We can see from the third viewport that the load object contains the crane boom, end effector tool, and the load itself. Normally we could assume that the load object is found fully under the end effector tool, but with use cases UCC2-1 and UCC2-2 this is not true. In Riihimäki tests, the tree trunks did easily reach above the end effector tool location when the operator picked them up so that they were tilted. Consequently, the bounding volume does reach above the end effector height, too. We decided to avoid accidental cutting of the load object after segmentation by not making any assumptions of the axis-wise load location other than it is located somewhere near the end effector tool for these 2 use cases. Instead, we opted to fit a cylinder object using RANSAC fitting, giving the approximate position and orientation for the cylindrical load as an output. 

Now, if the user wishes not to use the cylinder coefficients for pose measurements, and a crane-structure cutting AABB-based bounding volume is not a problem, then the AABB shown in Figure \ref{fig:load_detected2} can be used, too. The cylinder fitting implementation did pick up the cylindrical crane boom sometimes in testing, but we implemented a check for the condition that the load is located near the end effector tool location. With a little research, we found out that the cylinder fitting in a forestry crane machine vision system was not engineered previously in Aalto university. The advantage of the cylinder coefficients over the AABB bounding volume is that the oriented cylinder describes the actual volume and orientation of the tree trunk in much more detail compared to an AABB bounding volume, as discussed in chapter \ref{aabb_reference}. 

\section{OpenCV Extras}

The results achieved by image processing technologies have claimed a lot of interest in two major application areas.

First area is the enhancing of graphical content for human interpretation. In scientific image treatment it simply means that a noisy or corrupted original image can be treated in order to remove or diminish the corruption, resulting in useful content for human users to survey.

The second area is enabling autonomous machine perception. A machine can tirelessly inspect items on a product assembly line that would end up being dangerous, monotonous, or otherwise problematic for a human worker. Additionally, we may utilize the full electromagnetic spectral range in addition to visible light spectrum for quality control purposes.
 
Research effort in image processing technologies continues to result in better and faster algorithms for image enhancing.

\section{Other research}
+ Moving points in an image, and thus, objects, can be detected from photometric consistency in consecutive frames using discriminant analysis and image statistics (in Kanatani13)

%+ a great cylinder fitting scheme can be found in Pekka Forsman doctoral thesis

\chapter{References}
\label{chapter:references}

% Load the bibliographic references
% ------------------------------------------------------------------
% You can use several .bib files:
% \bibliography{thesis_sources,ietf_sources}
\bibliography{ref}


% Appendices go here
% ------------------------------------------------------------------
% If you do not have appendices, comment out the following lines
\appendix
% \input{appendices.tex}

\chapter{First appendix}
\label{chapter:first-appendix}

This is the first appendix. You could put some test images or verbose data in an
appendix, if there is too much data to fit in the actual text nicely.

For now, the Aalto logo variants are shown in Figure~\ref{fig:aaltologo}.


\begin{figure}
\begin{center}
\subfigure[In English]{\includegraphics[width=.8\textwidth]{aalto-logo-en}}
\subfigure[Suomeksi]{\includegraphics[width=.8\textwidth]{aalto-logo-fi}}
\subfigure[Pä svenska]{\includegraphics[width=.8\textwidth]{aalto-logo-se}}
\caption{Aalto logo variants}
\label{fig:aaltologo}
\end{center}
\end{figure}


% End of document!
% ------------------------------------------------------------------
% The LastPage package automatically places a label on the last page.
% That works better than placing a label here manually, because the
% label might not go to the actual last page, if LaTeX needs to place
% floats (that is, figures, tables, and such) to the end of the
% document.
\end{document}
