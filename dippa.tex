% Lines starting with a percent sign (%) are comments. LaTeX will
% not process those lines. Similarly, everything after a percent
% sign in a line is considered a comment. To produce a percent sign
% in the output, write \% (backslash followed by the percent sign).
% ==================================================================
% Usage instructions:
% ------------------------------------------------------------------
% The file is heavily commented so that you know what the various
% commands do. Feel free to remove any comments you don't need from
% your own copy. When redistributing the example thesis file, please
% retain all the comments for the benefit of other thesis writers!
% ==================================================================
% Compilation instructions:
% ------------------------------------------------------------------
% Use pdflatex to compile! Input images are expected as PDF files.
% Example compilation:
% ------------------------------------------------------------------
% > pdflatex thesis-example.tex
% > bibtex thesis-example
% > pdflatex thesis-example.tex
% > pdflatex thesis-example.tex
% ------------------------------------------------------------------
% You need to run pdflatex multiple times so that all the cross-references
% are fixed. pdflatex will tell you if you need to re-run it (a warning
% will be issued)
% ------------------------------------------------------------------
% Compilation has been tested to work in ukk.cs.hut.fi and kosh.hut.fi
% - if you have problems of missing .sty -files, then the local LaTeX
% environment does not have all the required packages installed.
% For example, when compiling in vipunen.hut.fi, you get an error that
% tikz.sty is missing - in this case you must either compile somewhere
% else, or you cannot use TikZ graphics in your thesis and must therefore
% remove or comment out the tikz package and all the tikz definitions.
% ------------------------------------------------------------------

% General information
% ==================================================================
% Package documentation:
%
% The comments often refer to package documentation. (Almost) all LaTeX
% packages have documentation accompanying them, so you can read the
% package documentation for further information. When a package 'xxx' is
% installed to your local LaTeX environment (the document compiles
% when you have \usepackage{xxx} and LaTeX does not complain), you can
% find the documentation somewhere in the local LaTeX texmf directory
% hierarchy. In ukk.cs.hut.fi, this is /usr/texlive/2008/texmf-dist,
% and the documentation for the titlesec package (for example) can be
% found at /usr/texlive/2008/texmf-dist/doc/latex/titlesec/titlesec.pdf.
% Most often the documentation is located as a PDF file in
% /usr/texlive/2008/texmf-dist/doc/latex/xxx, where xxx is the package name;
% however, documentation for TikZ is in
% /usr/texlive/2008/texmf-dist/doc/latex/generic/pgf/pgfmanual.pdf
% (this is because TikZ is a front-end for PGF, which is meant to be a
% generic portable graphics format for LaTeX).
% You can try to look for the package manual using the ``find'' shell
% command in Linux machines; the find databases are up-to-date at least
% in ukk.cs.hut.fi. Just type ``find xxx'', where xxx is the package
% name, and you should find a documentation file.
% Note that in some packages, the documentation is in the DVI file
% format. In this case, you can copy the DVI file to your home directory,
% and convert it to PDF with the dvipdfm command (or you can read the
% DVI file directly with a DVI viewer).
%
% If you can't find the documentation for a package, just try Googling
% for ``latex packagename''; most often you can get a direct link to the
% package manual in PDF format.
% ------------------------------------------------------------------


% Document class for the thesis is report
% ------------------------------------------------------------------
% You can change this but do so at your own risk - it may break other things.
% Note that the option pdftext is used for pdflatex; there is no
% pdflatex option.
% ------------------------------------------------------------------
\documentclass[12pt,a4paper,oneside,pdftex]{report}

% The input files (tex files) are encoded with the latin-1 encoding
% (ISO-8859-1 works). Change the latin1-option if you use UTF8
% (at some point LaTeX did not work with UTF8, but I'm not sure
% what the current situation is)
\usepackage[utf8]{inputenc}
% OT1 font encoding seems to work better than T1. Check the rendered
% PDF file to see if the fonts are encoded properly as vectors (instead
% of rendered bitmaps). You can do this by zooming very close to any letter
% - if the letter is shown pixelated, you should change this setting
% (try commenting out the entire line, for example).
\usepackage[OT1]{fontenc} %fontenc first, then inputenc if need be
% The babel package provides hyphenating instructions for LaTeX. Give
% the languages you wish to use in your thesis as options to the babel
% package (as shown below). You can remove any language you are not
% going to use.
% Examples of valid language codes: english (or USenglish), british,
% finnish, swedish; and so on.
\usepackage[british]{babel}


% Font selection
% ------------------------------------------------------------------
% The default LaTeX font is a very good font for rendering your
% thesis. It is a very professional font, which will always be
% accepted.
% If you, however, wish to spicen up your thesis, you can try out
% these font variants by uncommenting one of the following lines
% (or by finding another font package). The fonts shown here are
% all fonts that you could use in your thesis (not too silly).
% Changing the font causes the layouts to shift a bit; you many
% need to manually adjust some layouts. Check the warning messages
% LaTeX gives you.
% ------------------------------------------------------------------
% To find another font, check out the font catalogue from
% http://www.tug.dk/FontCatalogue/mathfonts.html
% This link points to the list of fonts that support maths, but
% that's a fairly important point for master's theses.
% ------------------------------------------------------------------
% <rant>
% Remember, there is no excuse to use Comic Sans, ever, in any
% situation! (Well, maybe in speech bubbles in comics, but there
% are better options for those too)
% </rant>

% \usepackage{palatino}
% \usepackage{tgpagella}



% Optional packages
% ------------------------------------------------------------------
% Select those packages that you need for your thesis. You may delete
% or comment the rest.

% Natbib allows you to select the format of the bibliography references.
% The first example uses numbered citations:
\usepackage[square,sort&compress,numbers]{natbib}
% The second example uses author-year citations.
% If you use author-year citations, change the bibliography style (below);
% acm style does not work with author-year citations.
% Also, you should use \citet (cite in text) when you wish to refer
% to the author directly (\citet{blaablaa} said blaa blaa), and
% \citep when you wish to refer similarly than with numbered citations
% (It has been said that blaa blaa~\citep{blaablaa}).
% \usepackage[square]{natbib}

% The alltt package provides an all-teletype environment that acts
% like verbatim but you can use LaTeX commands in it. Uncomment if
% you want to use this environment.
% \usepackage{alltt}

% The eurosym package provides a euro symbol. Use with \euro{}
\usepackage{eurosym}

% Verbatim provides a standard teletype environment that renderes
% the text exactly as written in the tex file. Useful for code
% snippets (although you can also use the listings package to get
% automatic code formatting).
\usepackage{verbatim}

% The listing package provides automatic code formatting utilities
% so that you can copy-paste code examples and have them rendered
% nicely. See the package documentation for details.
% \usepackage{listings}

% The fancuvrb package provides fancier verbatim environments
% (you can, for example, put borders around the verbatim text area
% and so on). See package for details.
% \usepackage{fancyvrb}

% Supertabular provides a tabular environment that can span multiple
% pages.
%\usepackage{supertabular}
% Longtable provides a tabular environment that can span multiple
% pages. This is used in the example acronyms file.
\usepackage{longtable}

% The fancyhdr package allows you to set your the page headers
% manually, and allows you to add separator lines and so on.
% Check the package documentation.
% \usepackage{fancyhdr}

% Subfigure package allows you to use subfigures (i.e. many subfigures
% within one figure environment). These can have different labels and
% they are numbered automatically. Check the package documentation.
\usepackage{subfigure}

% The titlesec package can be used to alter the look of the titles
% of sections, chapters, and so on. This example uses the ``medium''
% package option which sets the titles to a medium size, making them
% a bit smaller than what is the default. You can fine-tune the
% title fonts and sizes by using the package options. See the package
% documentation.
\usepackage[medium]{titlesec}

% The TikZ package allows you to create professional technical figures.
% The learning curve is quite steep, but it is definitely worth it if
% you wish to have really good-looking technical figures.
\usepackage{tikz}
% You also need to specify which TikZ libraries you use
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.pathmorphing,decorations.markings}
\usetikzlibrary{shapes}
\usetikzlibrary{patterns}


% The aalto-thesis package provides typesetting instructions for the
% standard master's thesis parts (abstracts, front page, and so on)
% Load this package second-to-last, just before the hyperref package.
% Options that you can use:
%   mydraft - renders the thesis in draft mode.
%             Do not use for the final version.
%   doublenumbering - [optional] number the first pages of the thesis
%                     with roman numerals (i, ii, iii, ...); and start
%                     arabic numbering (1, 2, 3, ...) only on the
%                     first page of the first chapter
%   twoinstructors  - changes the title of instructors to plural form
%   twosupervisors  - changes the title of supervisors to plural form
%\usepackage[mydraft,twosupervisors]{aalto-thesis}
%usepackage[mydraft,doublenumbering]{aalto-thesis}
\usepackage{aalto-thesis}
% Added packages 11.5.2014
\usepackage{amsmath,empheq}
\usepackage{textcomp}
% Pretty tables
\usepackage{booktabs}
% Algorithm writing
\usepackage[]{algorithm2e}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}
% Subfigure environment
\usepackage{caption}
\usepackage{subcaption}

% Hyperref
% ------------------------------------------------------------------
% Hyperref creates links from URLs, for references, and creates a
% TOC in the PDF file.
% This package must be the last one you include, because it has
% compatibility issues with many other packages and it fixes
% those issues when it is loaded.
\RequirePackage[pdftex]{hyperref}
% Setup hyperref so that links are clickable but do not look
% different
\hypersetup{colorlinks=false,raiselinks=false,breaklinks=true}
\hypersetup{pdfborder={0 0 0}}
\hypersetup{bookmarksnumbered=true}
% The following line suggests the PDF reader that it should show the
% first level of bookmarks opened in the hierarchical bookmark view.
\hypersetup{bookmarksopen=true,bookmarksopenlevel=1}
% Hyperref can also set up the PDF metadata fields. These are
% set a bit later on, after the thesis setup.


% Thesis setup
% ==================================================================
% Change these to fit your own thesis.
% \COMMAND always refers to the English version;
% \FCOMMAND refers to the Finnish version; and
% \SCOMMAND refers to the Swedish version.
% You may comment/remove those language variants that you do not use
% (but then you must not include the abstracts for that language)
% ------------------------------------------------------------------
% If you do not find the command for a text that is shown in the cover page or
% in the abstract texts, check the aalto-thesis.sty file and locate the text
% from there.
% All the texts are configured in language-specific blocks (lots of commands
% that look like this: \renewcommand{\ATCITY}{Espoo}.
% You can just fix the texts there. Just remember to check all the language
% variants you use (they are all there in the same place).
% ------------------------------------------------------------------
\newcommand{\TITLE}{Crane Load Object Geometry Measurement using Machine Vision}
\newcommand{\FTITLE}{Nosturin taakan geometrian mittaus konenäköavusteisesti}
\newcommand{\DATE}{May 27th, 2014}
\newcommand{\FDATE}{27. toukokuuta 2014}
\newcommand{\SDATE}{Den 27 Maj 2014}

% Supervisors and instructors
% ------------------------------------------------------------------
% If you have two supervisors, write both names here, separate them with a
% double-backslash (see below for an example)
% Also remember to add the package option ``twosupervisors'' or
% ``twoinstructors'' to the aalto-thesis package so that the titles are in
% plural.
% Example of one supervisor:
\newcommand{\SUPERVISOR}{Professor Ville Kyrki}
\newcommand{\FSUPERVISOR}{Professori Ville Kyrki}
\newcommand{\SSUPERVISOR}{Professor Ville Kyrki}
% Example of twosupervisors:
%\newcommand{\SUPERVISOR}{Professor Ville Kyrki\\
%  D.Sc. Sami Terho}
%\newcommand{\FSUPERVISOR}{Professori Ville Kyrki\\
%  TkT Sami Terho}
%\newcommand{\SSUPERVISOR}{Professor Ville Kyrki\\
%  Dr. Sami Terho}

% If you have only one instructor, just write one name here
\newcommand{\INSTRUCTOR}{D.Sc. (Tech.) Sami Terho}
\newcommand{\FINSTRUCTOR}{TkT Sami Terho}
\newcommand{\SINSTRUCTOR}{Teknologie doktor Sami Terho}
% If you have two instructors, separate them with \\ to create linefeeds
% \newcommand{\INSTRUCTOR}{Olli Ohjaaja M.Sc. (Tech.)\\
%  Elli Opas M.Sc. (Tech)}
%\newcommand{\FINSTRUCTOR}{Diplomi-insinööri Olli Ohjaaja\\
%  Diplomi-insinööri Elli Opas}
%\newcommand{\SINSTRUCTOR}{Diplomingenjör Olli Ohjaaja\\
%  Diplomingenjör Elli Opas}

% If you have two supervisors, it is common to write the schools
% of the supervisors in the cover page. If the following command is defined,
% then the supervisor names shown here are printed in the cover page. Otherwise,
% the supervisor names defined above are used.
\newcommand{\COVERSUPERVISOR}{Professor Ville Kyrki, Aalto University, School of Electrical Engineering}
% The same option is for the instructors, if you have multiple instructors.
% \newcommand{\COVERINSTRUCTOR}{Olli Ohjaaja M.Sc. (Tech.), Aalto University\\
%  Elli Opas M.Sc. (Tech), Aalto SCI}

% Other stuff
% ------------------------------------------------------------------
\newcommand{\PROFESSORSHIP}{Intelligent robotics}
\newcommand{\FPROFESSORSHIP}{Älykäs robotiikka}
\newcommand{\SPROFESSORSHIP}{Intelligent robotteknik}
% Professorship code is the same in all languages
\newcommand{\PROFCODE}{T-110}
\newcommand{\KEYWORDS}{stereo vision, crane load object, environment mapping}
\newcommand{\FKEYWORDS}{stereonäkö, nosturin taakka, ympäristön kartoitus}
\newcommand{\SKEYWORDS}{stereoseende, kran last, miljö kartläggning}
\newcommand{\LANGUAGE}{English}
\newcommand{\FLANGUAGE}{Englanti}
\newcommand{\SLANGUAGE}{Engelska}
% Author is the same for all languages
\newcommand{\AUTHOR}{Erkka Mutanen}

% Currently the English versions are used for the PDF file metadata
% Set the PDF title
\hypersetup{pdftitle={\TITLE\ \AUTHOR}}
% Set the PDF author
\hypersetup{pdfauthor={\AUTHOR}}
% Set the PDF keywords
\hypersetup{pdfkeywords={\KEYWORDS}}
% Set the PDF subject
\hypersetup{pdfsubject={Master's Thesis}}

% Layout settings
% ------------------------------------------------------------------

% When you write in English, you should use the standard LaTeX
% paragraph formatting: paragraphs are indented, and there is no
% space between paragraphs.
% When writing in Finnish, we often use no indentation in the
% beginning of the paragraph, and there is some space between the
% paragraphs.

% If you write your thesis Finnish, uncomment these lines; if
% you write in English, leave these lines commented!
% \setlength{\parindent}{0pt}
% \setlength{\parskip}{1ex}

% Use this to control how much space there is between each line of text.
% 1 is normal (no extra space), 1.3 is about one-half more space, and
% 1.6 is about double line spacing.
\linespread{1.1} % This is the default
% \linespread{1.3}

% Bibliography style
% acm style gives you a basic reference style. It works only with numbered
% references.
\bibliographystyle{acm}
% Plainnat is a plain style that works with both numbered and name citations.
% \bibliographystyle{plainnat}


% Extra hyphenation settings
% ------------------------------------------------------------------
% You can list here all the files that are not hyphenated correctly.
% You can provide many \hyphenation commands and/or separate each word
% with a space inside a single command. Put hyphens in the places where
% a word can be hyphenated.
% Note that (by default) LaTeX will not hyphenate words that already
% have a hyphen in them (for example, if you write ``structure-modification
% operation'', the word structure-modification will never be hyphenated).
% You need a special package to hyphenate those words.
\hyphenation{di-gi-taa-li-sta yksi-suun-tai-sta}

% The preamble ends here, and the document begins.
% Place all formatting commands and such before this line.
% ------------------------------------------------------------------
\begin{document}
% This command adds a PDF bookmark to the cover page. You may leave
% it out if you don't like it...
\pdfbookmark[0]{Cover page}{bookmark.0.cover}
% This command is defined in aalto-thesis.sty. It controls the page
% numbering based on whether the doublenumbering option is specified
\startcoverpage

% Cover page
% ------------------------------------------------------------------
% Options: finnish, english, and swedish
% These control in which language the cover-page information is shown
\coverpage{english}
% Abstracts
% ------------------------------------------------------------------
% Include an abstract in the language that the thesis is written in,
% and if your native language is Finnish or Swedish, one in that language.

% Abstract in English
% ------------------------------------------------------------------
\thesisabstract{english}{

In manufacturing industry, the electric overhead crane is used in lifting and moving operations of industrial goods, such as beams, engines, and large coils. Mostly, these cranes are manually operated in locations where people work, such as a factory floor. The cranes do not operate automatically and they do not have safety features that would prevent collisions of the lifted load object and other products or workers.

Geometry measurement of the load object is required to allow for new smarter automated features to the overhead crane environment, such as route planning, collision detection, and collision avoidance. The incentives for developing such features are increased goods handling safety, and the need to improve easiness of operation for the crane operator. 

A 3d machine vision process was selected to produce a load object measurement using a passive stereographic triangulation technique. A perception platform with two high dynamic range cameras was used to acquire data from a real overhead crane environment. Captured images were processed into 3d point clouds representing the environment. Finally, the point cloud was processed into a metric axis-aligned bounding volume measurement of the load object using Point Cloud Library (PCL) processing. The measurement software was implemented using C++ libraries and robot operating system (ROS). 

This thesis studies two different applications for machine vision -based load object geometry measurement. The first case introduces a technique for measuring axis-aligned bounding volume of the load object using simple tracking of the load object in indoor overhead crane environment. The second case introduces a technique for measuring cylindrical load objects using parameter fitting technique and 2d image tracking in outdoor rotary telescopic crane environment.  
}

%BADThis thesis will primarily study a machine vision approach that uses stereo vision for the work environment
%epth mapping. Other optional sensors, such as a laser scanner, are considered.

%BADExisting machine vision approaches will be applied to the crane load problem. 
%The location measurement of the crane load could enable additional improvements in obstable avoidance while
%operating the crane.}

%\fixme{Abstract text goes here (and this is an example how to use fixme).}
%Fixme is a command that helps you identify parts of your thesis that still
%require some work. When compiled in the custom \texttt{mydraft} mode, text
%parts tagged with fixmes are shown in bold and with fixme tags around them. When
%compiled in normal mode, the fixme-tagged text is shown normally (without
%special formatting). The draft mode also causes the ``Draft'' text to appear on
%the front page, alongside with the document compilation date. The custom
%\texttt{mydraft} mode is selected by the \texttt{mydraft} option given for the
%package \texttt{aalto-thesis}, near the top of the \texttt{thesis-example.tex}
%file.

%The thesis example file (\texttt{thesis-example.tex}), all the chapter content
%files (\texttt{1introduction.tex} and so on), and the Aalto style file
%(\texttt{aalto-thesis.sty}) are commented with explanations on how the Aalto
%thesis works. The files also contain some examples on how to customize various
%details of the thesis layout, and of course the example text works as an
%example in itself. Please read the comments and the example text; that should
%get you well on your way!}

% Abstract in Finnish
% ------------------------------------------------------------------
\thesisabstract{finnish}{
Valmistusteollisuudessa käytetään sähkökäyttöisiä prosessihallinostureita tuotteiden nosto- ja siirtotehtäviin. Tuotteet ovat usein yksittäiseriä, kuten moottoreita, palkkeja, tai isoja käämejä. Yleensä nostureita ohjataan manuaalisesti tiloissa joissa ihmiset työskentelevät, esimerkiksi tehdaslattiatiloissa. Nostureita ei edellä mainituissa tilanteissa yleensä voi ohjata tietokoneella, eikä niissä ole turvaominaisuuksia jotka estävät nostettavan kappaleen törmäykset tehdasympäristössä olevien muiden tuotteiden tai työntekijöiden kanssa.

Nosturin taakan geometrian mittaus vaaditaan uusien automaattisten toimintojen mahdollistamiseksi tehdasympäristössä. Uusia toimintoja ovat esimerkiksi reitinsuunnittelu-, törmäystarkastelu-, ja törmäyksenestotoiminnot. Edellä mainittujen toimintojen kehittäminen mahdollistaa turvallisemman taakan siirtämisen tilassa, ja helpottaa nosturin operoimista.

Taakan mittauksen tuottamiseksi valittiin 3d-konenäkömenetelmä, joka käyttää passiivista stereokolmiointimenetelmää. Työn toteutuksessa käytettiin kahta high dynamic range -kameraa, jotka tallensivat kuvadataa oikeasta nosturiympäristöstä. Kuvadata prosessoitiin 3d-pistepilviksi, joista edelleen tuotettiin metrinen koordinaattiakselien myötäinen bounding volume -mittaus Point Cloud Library -kirjaston (PCL) avulla. Mittausohjelmisto toteutettiin C++ -kirjastoilla ja Robot Operating System:llä (ROS).

Tämä diplomityö käsittelee kahta eri sovellusta konenäköavusteiselle nosturin taakan geometrian mittaukselle. Ensimmäinen sovellus esittelee menetelmän koordinaattiakselien suuntaiseen bounding volume -mittaukseen yksinkertaisella nosturin taakan seuraamisella sisätiloissa. Toinen sovellus esittelee menetelmän sylinterimäisten taakkojen mittaukseen parametrisovitusmenetelmän ja 2d-kuvaseurannan avulla ulkotiloissa teleskooppinosturilla.
}
% Abstract in Swedish
% ------------------------------------------------------------------
%\thesisabstract{swedish}{
%illa Vargens universum är det tredje fiktiva universumet inom huvudfäran av de
%ecknade disneyserierna - de övriga två är Kalle Ankas och Musse Piggs
%niversum. Figurerna runt Lilla Vargen kommer huvudsakligen frän tre källor ---
%els persongalleriet i kortfilmen Tre smä grisar frän 1933 och dess uppföljare,
%dels längfilmen Sängen om Södern frän 1946, och dels frän episoden ``Bongo'' i
%ängfilmen Pank och fägelfri frän 1947. Framför allt de två första har
%sedermera även kommit att leva vidare, utvidgas och införlivas i varandra genom
%tecknade serier, främst sädana producerade av Western Publishing för
%amerikanska Disneytidningar under ären 1945--1984.

%Världen runt Lilla Vargen är, i jämförelse med den runt Kalle Anka eller Musse
%Pigg, inte helt enhetlig, vilket bland annat märks i Bror Björns skiftande
%personlighet. Den har även varit betydligt mer öppen för influenser frän andra
%Disneyvärldar, inte minst de tecknade längfilmerna. Ytterligare en skillnad är
%tt varelserna i vargserierna förefaller stä närmare sina förebilder inom den
%verkliga djurvärlden. Att vargen Zeke vill äta upp grisen Bror Duktig är till
%exempel ett ständigt äterkommande tema, men om katten Svarte Petter skulle fä
%för sig att äta upp musen Musse Pigg skulle detta antagligen höja ett och annat
%ögonbryn bland läsarna.}


% Acknowledgements
% ------------------------------------------------------------------
% Select the language you use in your acknowledgements
\selectlanguage{english}

% Uncomment this line if you wish acknoledgements to appear in the
% table of contents
%\addcontentsline{toc}{chapter}{Acknowledgements}

% The star means that the chapter isn't numbered and does not
% show up in the TOC
\chapter*{Acknowledgements}

I wish to thank all students who use \LaTeX\ for formatting their theses,
because theses formatted with \LaTeX\ are just so nice.

Thank you, and keep up the good work!
\vskip 10mm

\noindent Espoo, \DATE
\vskip 5mm
\noindent\AUTHOR

% Table of contents
% ------------------------------------------------------------------
\cleardoublepage
% This command adds a PDF bookmark that links to the contents.
% You can use \addcontentsline{} as well, but that also adds contents
% entry to the table of contents, which is kind of redundant.
% The text ``Contents'' is shown in the PDF bookmark.
\pdfbookmark[0]{Contents}{bookmark.0.contents}
\tableofcontents

% Abbreviations & Acronyms
% ------------------------------------------------------------------
% Use \cleardoublepage so that IF two-sided printing is used
% (which is not often for masters theses), then the pages will still
% start correctly on the right-hand side.
\cleardoublepage
% Example acronyms are placed in a separate file, acronyms.tex
% \input{acronyms}

\addcontentsline{toc}{chapter}{Abbreviations}
\chapter*{Abbreviations}

% The longtable environment should break the table properly to multiple pages,
% if needed

\noindent
\begin{longtable}{@{}p{0.25\textwidth}p{0.7\textwidth}@{}}
AABB & Axis-aligned bounding-box \\
AC & Alternating Current \\
AI & Artificial Intelligence \\
BM & Block Matching \\
BVH & Bounding Volume Hierarchies \\
CCIR & Comité Consultatif International pour la Radio \\
CMOS & Complementary Metal Oxide Semiconductor \\
CUDA & Compute Unified Device Architecture \\
DC & Direct Current \\
FLIR & Forward Looking Infra Red \\
GPU & Graphics Processing Unit \\
HDR & High Dynamic Range \\
I/O & Input Output\\
LIDAR & Light Detecting And Ranging \\
OBB & Oriented Bounding Box \\
PC & Personal Computer \\
PCL & Point Cloud Library \\
PLC & Programmable Logic Controller \\
RADAR & Radio Detecting And Ranging \\
ROI & Region Of Interest \\
ROS & Robot Operating System \\
SAD & Sum of Absolute Differences \\
SGBM & Semi-Global Block Matching \\
SIMD & Single Instruction, Multiple Data \\
SOM & Self-organizing Map \\
SONAR & Sound Navigation And Ranging \\
SSD & Sum of Squared Differences \\
SSE & Streaming SIMD Extensions \\
TLD & Tracking, Detection, And Learning \\
TOF & Time Of Flight \\
UCC & Use Case Configuration \\
UDP & User Datagram Protocol \\
XML & Extensive Markup Language \\
XML-RPC & XML Remote Procedure Call \\ 
\end{longtable}

% List of tables
% ------------------------------------------------------------------
% You only need a list of tables for your thesis if you have very
% many tables. If you do, uncomment the following two lines.
% \cleardoublepage
% \listoftables

% Table of figures
% ------------------------------------------------------------------
% You only need a list of figures for your thesis if you have very
% many figures. If you do, uncomment the following two lines.
% \cleardoublepage
% \listoffigures

% The following label is used for counting the prelude pages
\label{pages-prelude}
\cleardoublepage

%%%%%%%%%%%%%%%%% The main content starts here %%%%%%%%%%%%%%%%%%%%%
% ------------------------------------------------------------------
% This command is defined in aalto-thesis.sty. It controls the page
% numbering based on whether the doublenumbering option is specified
\startfirstchapter

% Add headings to pages (the chapter title is shown)
\pagestyle{headings}

% The contents of the thesis are separated to their own files.
% Edit the content in these files, rename them as necessary.
% ------------------------------------------------------------------

% \input{1introduction.tex}


%===========================================================================================================
\chapter*{Symbols}

%\footnote{http://owl.english.purdue.edu/owl/resource/574/02/} of
%Purdue University or Strunk's Elements of
%Style\footnote{http://www.bartleby.com/141/}. Remember that footnotes

%\emph{cite}.

% An example of a traditional LaTeX table
% ------------------------------------------------------------------
% A note on underfull/overfull table cells and tables:
% ------------------------------------------------------------------
% In professional typography, the width of the text in a page is always a lot
% less than the width of the page. If you are accustomed to the (too wide) text
% areas used in Microsoft Word's standard documents, the width of the text in
% this thesis layout may suprise you. However, text in a book needs wide
% margins. Narrow text is easier to read and looks nicer. Longer lines are
% hard to read, because the start of the next line is harder to locate when
% moving from line to the next.
% However, tables that are in the middle of the text often would require a wider
% area. By default, LaTeX will complain if you create too wide tables with
% ``overfull'' error messages, and the table will not be positioned properly
% (not centered). If at all possible, try to make the table narrow enough so
% that it fits to the same space as the text (total width = \textwidth).
% If you do need more space, you can either
% 1) ignore the LaTeX warnings
% 2) use the textpos-package to manually position the table (read the package
%    documentation)
% 3) if you have the table as a PDF document (of correct size, A4), you can use
%    the pdfpages package to include the page. This overrides the margin
%    settings for this page and LaTeX will not complain.
% ------------------------------------------------------------------
% Another note:
% ------------------------------------------------------------------
% If your table fits to \textwidth, but the cells are so narrow that the text
% in p{..}-formatted cells does not flow nicely (you get underfull warnings
% because LaTeX tries to justify the text in the cells) you can manually set
% the text to unjustified by using the \raggedright command for each cell
% that you do not want to be justified (see the example below). \raggedleft
% is also possible, of course...
% ------------------------------------------------------------------
% If you need to have linefeeds (\\) inside a cell, you must create a new
% paragraph-formatting environment inside the cell. Most common ones are
% the minipage-environment and the \parbox command (see LaTeX documentation
% for details; or just google for ``LaTeX minipage'' and ``LaTeX parbox'').
% Alignment of sells: l=left, c=center, r=right.
% If you want wrapping lines, use p{width} exact cell widths.
% If you want vertical lines between columns, write | above between the letters
% Horizontal lines are generated with the \hline command:

% Place a & between the columns
% In the end of the line, use two backslashes \\ to break the line,
% then place a \hline to make a horizontal line below the row

%\begin{table}
%\begin{tabular}{|p{2cm}|p{3.8cm}|p{4.5cm}|p{1.1cm}|}
%\hline % The line on top of the table
%\textbf{Code} & \textbf{Name} & \textbf{Methods} & \textbf{Area} \\
%\hline
%T-110.6130 & Systems Engineering for Data Communications
%    Software & \raggedright Computer simulations, mathematical modeling,
%  experimental research, data analysis, and network service business
%  research methods, (agile method) & T-110 \\
%\hline
%\multicolumn{2}{|p{6.25cm}|}{Mat-2.3170 Simulation (here is an example of
% multicolumn for tables)}& Details of how to build simulations & T-110 \\
%\hline
%S-38.3184 & Network Traffic Measurements and Analysis
%& \raggedright How to measure and analyse network
%  traffic & T-110 \\ \hline
% \end{tabular} % for really simple tables, you can just use tabular
% You can place the caption either below (like here) or above the table
%\caption{Research methodology courses}
% Place the label just after the caption to make the link work
%\label{table:courses}
%\end{table} % table makes a floating object with a title
%The multicolumn command takes the following 3 arguments:
% the number of cells to merge, the cell formatting for the new cell, and the
% contents of the cell

\chapter{Introduction}
\label{chapter:introduction}
%Ongelman esittely, johdanto. Pituus noin 5 sivua.
%A machine can tirelessly inspect items on a product assembly line that would end up being dangerous, monotonous, or otherwise problematic for a human worker. Additionally, we may utilize the full electromagnetic spectral range in addition to visible light spectrum for quality control purposes.

In the modern world, products are being manufactured more than ever before, and at the same time the factory workflows should be optimised to guarantee profitable continuity for a business. These new requirements challenge all business owners, including industrial product makers who use overhead cranes in their manufacturing processes. Overhead cranes are found in most warehouses and factories, and they are often used in storage solutions, as part of the manufacturing workflow, and in the initial installation of machinery on the factory floor.

Since industrial revolution many different types of cranes were designed to help in loading and unloading of ship cargo in container crate terminals at seaports. Similar development happened to the overhead crane systems at factory sites, and the electric overhead crane (EOC) was introduced after the steam engine cranes. While the modern sea cargo handling process is highly automated, the de facto electric overhead crane installations are still mostly manually operated. Before the ideation of the modern container cargo handling, the lifting of barrels, pallets, and sacks was inefficient, and the ships were stalling longer times at docks than at sea, which reduced profitability\citep{Zrni04}. In sites where manual overhead cranes are used to move products, the crane operation could become more efficient with added automation systems.

The business of lifting solutions does get more automated as new crane systems are being developed to meet the ever increasing needs for performance. While speed is not important in all lifting operations, safety is important in all of them. Both speed and safety affect the productivity of lifting and moving goods with indoor cranes, and with ever lowering computing prices a lot of research is going on to find ways how added programming and sensors could improve productivity, or improve the life cycle of the crane.

 % measuring goods
Measuring products while they are being moved and processed at the industrial site is the very basis for all quality control in other manufacturing industries: for example, radio frequency identification tags are used to identify and control a batch of products in bulk manufacturing, video systems measure process quality in a paper factory, and a spectral analyzer measures the concentration of foreign substances in a soda bottle washing process. In an overhead crane environment, there is information available only on how large a mass is being lifted and where it is being lifted, but no information on what is being lifted or what volume is the load object. A machine vision perception system can add quality control in the crane environment by providing a feedback link with information on lifted object size, shape, orientation, color, movement, past locations, and visuals.

%In the modern world, goods are being moved more than ever before, which brings challenges to keep the goods handling easy and safe. Products are mostly transported from one location to another using a modern automatic container handling process, but for example loading and unloading events remain laborous when single industrial items, such as engines or other machinery, are moved to a new location. The electric overhead crane (EOC) is the de facto standard for heavy lifting work in factory and warehouse environments.

%The business of lifting solutions gets more automated as new crane systems are being developed to meet the ever increasing needs for performance. Speed and safety affect the productivity of the event of lifting and moving goods with cranes, and with ever lowering computing prices a lot of research is going on to find ways how added programming and sensors could benefit the crane operator for increased performance. While speed is not important in all cargo  operations, safety is important in all of them. Then again, same automation technologies could be potentially used to speed up operations where speed is important, such as log picking operation in forestry solutions.

%If an overhead traveling crane is used to move products inside a warehouse e.g. from one manufacturing station to another, then a measurement of the lifted load object can potentially provide useful information for automation systems that use the size of the load object as a parameter in smart applications. Currently, overhead cranes do not measure any geometry of the load objects that they lift. Thus, all the decision-making whether the load object is physically in correct spatial coordinates is left for the operator to decide.

%Ideas of the modern container cargo handling process only started after the second half of the 20th century, and before that the only known way of transport was sea cargo packed in barrels, sacks and pallets. It was cumbersome to load and unload a ship filled with such small units, and it was not unusual that the ships spent more time in docks than at sea because of inefficient handling of small cargo.\ref{Zrni04}\par

It is possible to find examples of fast, automated, optimized, profitable, and safe factory manufacturing workflows, but many industrial ventures still rely on manual movement of a product on a factory floor. Manual operations may suffer from non-optimized route selection, operator fatigue, damage to goods in collisions, safety issues for other workers, and other problems due human error. Some industries are not even profitable unless a certain speed of moving goods safely is reached, which applies for e.g. forestry log picking and tree felling work. To improve things, a real-time description of the load object and its environment could be available, so it would be possible to prevent unwanted events from happening using perception sensors and safety software. \citep{Kappi13}

\section{Interactive Control In A Crane Environment}
\label{section:collision_control_in_a_crane_environment}

% käyttökokemus
Industrial goods e.g. beams, engines, large coils, drums of dangerous goods, and pallets of stacked items are expensive, often unique one-time orders that are being lifted and transferred inside a warehouse every day. If in collision, a lifted product may get damaged or break other products, which can be prevented by using duplex interactive control. Duplex interactive control suggests duality in control of the overhead crane: while the operator designs the initial hoisting and moving action, a computer interactively smooths and recalculates the trajectory with an optimal control scheme. Moreover, the computer controller can notify the operator of the current state of the crane operation so that the operator may adjust his or her working style and get feedback from it. The computer could also notify the operator of future collisions, but it is possibly more effective to just take safety measures to prevent collisions just in time if the operator is about to collide the load object with the environment. An indoor overhead crane that uses collision avoidance technology or other interactive control technology helps the crane operator move products in a warehouse in semi-automatic manner, and provides added information about products and their properties.

% törmäyksenesto
Ideally collision avoidance techniques should automatically prevent collision based damage to the warehouse building, and warn the operator of safety hazards and engage safety measures if needed. The system should be able to detect people who walk on the factory floor, and prevent injuries with high priority.  To realise a working collision avoidance system, the load object model and the environment model should be known. If the load object is carefully tracked and its geometry is known at all times, a collision computation with a known environment model can be used to prevent collisions before they happen. The added value is potentially great, which is why the measurement of the load object is worth researching.

% technical difficulties
From a technical point of view, spatial tracking of a load object in time (4d tracker) is difficult, but if an understanding of the object model and its qualities is formed, then an estimate of the current state of the model can be used for decision making at any time. Tracked objects should continuosly provide information about their location, dimensions, and moving speeds at least. With such model information it is possible to implement collision avoidance functionality to a crane working environment. Some trackers that would be suitable for this purpose are object state vector management systems, and Kalman filter based trackers, but they are out of scope of this thesis. \citep{Leibe07} \citep{Corke11} \citep{Miller12}

% how to implement collision detection
The first step in implementing a collision avoidance technology is to provide a metric model of the environment, and segment all the objects that reside in it. A machine vision system can be used for environment modeling in general, and this thesis later introduces load object modeling with machine vision techniques. Further design of such a system should be done after an understanding of how the environment can be modeled to support a collision detection technique. New overhead crane systems do not currently have any collision detection features, but optimized route planning could become a standard feature in the future.
 
\section{Applications}
%+ visual odometry

Industrial applications for machine vision have become more ubiquitous as the price for machine vision systems have decreased. Applications range from space exploration to agriculture, and from mobile robotics to quality control at a manufacturing line. The use of machine vision brings the speed of the computer to many errands that are impossible or hazardous for a human to do.

If applications of machine vision are strictly limited to overhead cranes, some earlier research may be found and studied. For example, crane control was researched by Peng and Singhose using top-down cameras and a wand controllerin 2009. They used cameras to locate the wand controller and the crane hook with help of reflective materials, and reported that the system greatly increased the operating efficiency of the crane\ref{Peng09}. They also reported machine vision reliability issues because of limited onboard camera processing abilities, and changing lighting in the environment. An upgrade to operation in the infrared spectrum of light was suggested to counteract changing visible light conditions in their applications.


quality control in high speed manufacturing processes e.g. quality control of a multi-layer web forming in a paper factory. Machine vision is extensively used in packaging industry






Previously, machine vision has been  in the overhead crane environment for improved trajectory control using wand controller. 

\ref{}

A machine vision perception platform includes a stereo camera setup that is used to measure object dimensions in the crane working area. The installed cameras can be used for other purposes, too. For example, security camera surveillance can detect trespassers when the factory is shut down. 

recordings of states of the environment, provide video feed of accidents for insurance companies, and enable teaching material for new users as records of events. 



Size and weight statistics can help track the crane usage and lead to improvements in warehouse layout, or help a factory manager decide upon investing in a larger manufacturing floor if the capacity is at a limit.

% operointi
A human operator is better able to move load objects through narrow passages and execute difficult spatial tracks than a computer, but he or she can not minimise the used energy to perform a certain movement or minimise daily wear of moving parts. A computer control can positively improve the product life cycle and minimise maintenance breaks. 
Some automatic overhead cranes have been built, but usually they move bulk items, such as sand, steel, or other process-like material that is moved in environments where people are not working.




The biggest non-safety related features are the optimized route planning (using knowledge of geometric dimensions) and automatic moving of loads using a computer planner. During the life cycle of the crane installation such features can reduce vast amounts of used energy, possibly increase movement speeds, and sometimes increase service intervals.
 
 
 
 Some other properties can be e.g. warehouse surveillance, operations tracking, quality control etc.
 
% person lifter


%+ Environment sensing is needed for robotization of spaces
%+ 3d data of the environment is important in factory design, facility management and urban regional planning. \ref{Surmann03}

% stereoscopic teleoperation

%Teoriaa ja teknologioita. Pituus noin 20 sivua.
%Applications that can benefit from accurate positioning and tracking of a load object

%\section{Existing Positioning And Tracking Applications}
%\label{section:existing_positioning_and_tracking_applications}}

%\section{Safety Applications}
%\label{section:safety_applications}

%Traditional and new safety applications can utilize load positioning information for added functionality.

%A traditional safety application stops the crane process if a human is detected in a dangerous work zone \cite{Raula11}. Alternatively, an adaptive safety scheme can be utilised to restrict the operation range of the crane, but not halt it. If we are able to reliably detect a location for the moving load

%The work area of the crane can be modelled with state-of-the-art sensing of the environment enabling for example pre-collision avoidance.

%Adaptive safety techniques can be realised with the improved

%\section{Mapping Applications}
%\label{section:mapping_applications}
% application: city-scale 3d reconstruction from community-sourced online photos | ref Agarval09

%\section{Selected Use Cases}
%\label{section:selected_use_cases}
 
\section{Overhead Cranes}
\label{section:overhead_cranes}
% tällä hetkellä overhead cranes
Traditionally, overhead crane operation has been manual and not computer aided. An overhead crane is operated by a technical worker who is making all the decisions whether the lifted product can be moved in one direction or not without a risk of collision or injury to other workers in the area. Supposedly, other workers are going about their daily work in the same factory floor where the overhead crane operates. With current generation manual overhead cranes it is possible to collide a load object with other objects that lie on the factory floor or on the shelves of a warehouse. Overhead cranes have automation systems that improve some qualities, such as load anti-sway systems, single motor hoisting mechanisms, and wireless remote controllers. While they add value for the crane operator and improve safety and speed of the overhead crane operation, it is still far from automatic. Moreover, the computer control system cannot warn the operator of any hazards.
% big picture
The focus of this work is on measuring the dimensions of industrial goods being lifted and moved in an industrial site using an overhead crane. A glimpse in the not so distant history reveals a development of the overhead crane, a hoisting system more commonly known as the bridge crane or the process crane. First mass manufactured overhead crane systems were commissioned in 1876 in England, and they were powered by steam engines in the beginning of the industrial revolution. Today, 138 years later the overhead crane is widely used in different industries for efficient process lifting and service duties using electricity. The development of the electric overhead traveling crane (EOT) has introduced the modern overhead crane as we know it. It is electrically powered and often used in heavy process industry, and in custom industrial manufacturing.\par
%structure of a single overhead crane
    Overhead cranes have a beam-like bridge installed between two rails. The bridge spans the gap between the rails and travels from one end of the rail to the other. Effectively, a polyhedron shaped (cuboid) crane working area (portal of the crane) is formed underneath the rails encompassing most space between a factory floor and the height of the installed crane. Usually, a control pendant, or a wireless hand-held controller is used to move the load object to a desired location in the 3d crane working space. \par
% medium picture, applications
    Overhead cranes are used for medium to heavy lifting in industrial manufacturing and process industries. For example, in steel mills, heavy lifting is needed when cylinders of molten steel or other metal products are carried to the next process station. In heavy industry, the capacity of the crane and standard compliance is important, and the crane must operate reliably in hazardous environments. Efficient and safe usage patterns depend on the application at hand. In all applications, most important factors usually are time-wise efficient movement of the crane, safe operation of the crane, and easiness of operation.\par
% thesis focus on overhead cranes 
    Objects that are considered as load objects in this thesis cover a range of custom manufactured products whose size is unknown prior to the lifting operation. Since each item moved with the crane is unique, non-standard size object, an understanding of the load object size would be valuable to a software system that operates autonomously.
    In heavy process industry, it is not often meaningful to measure load objects or containers whose size is standard and known prior to the lifting operation. For practical reasons, all testing on overhead cranes have been done using a company test site installation that was capable of lifting medium to heavy objects up to 15 tonnes. All the objects lifted with the overhead crane were smaller than 2 meters for all dimensions, which is quite a normal size for a pallet. Only the volume of the object affects its geometry measurement.

\section{Forestry Machine Load Object Measurement}
\label{section:forestry_machines_cranes}

A forestry machine load object measurement is a secondary objective that is researched in this thesis. Forestry machines that include a rotary telescopic crane are included as research targets, mainly log picker trucks. In tree felling work, little automation is used currently, and to increase the level of automation, a load object measurement system has been suggested. A lot of research has been done in the area of stereographic 3d teleoperation of log picker trucks, but without a better spatial understanding of the truck environment and computer aided end effector tool control it has been deemed not feasible with current teleoperation technologies. A machine vision system can provide a geometry measurement of a log picker load object, which in turn could be further used to provide a better spatial description of the forestry crane working area for the teleoperator. 

Log picking work differs from lifting work in a warehouse environment mostly due its productivity constraints. A certain speed of picking logs must be maintained to keep the operation of the log picker truck profitable in business sense. While teleoperating a rotary log picking crane is possible, the speed of a teleoperated crane is much lower than a human operated crane. Teleoperation techniques do enable a single driver operate multiple log picker trucks simultaneously, but currently the work is only productive with drivers on site. 

Research in automatic forestry machines is an ongoing effort that tries to increase productivity in the field with help of teleoperation and automatic machinery. A fully automatic log picker truck that is as fast as a human operated truck would provide the biggest productivity increase, but currently the research is focused more on aiding a single driver increase his or her productivity with help of computers. For example, new augmented reality displays are being introduced in the forestry machine cockpits to help the driver get information that helps with the work. 

\section{Goals And Objectives}
\label{section:goals_and_objectives}

The secondary objective of this thesis is to be able to compute the dimensions of a picked up log with the help of a machine vision system. Further processing of the measurement information and the details of teleoperation applications are outside of the scope of this thesis.

Many research efforts report using laser range finders for 3d environment modeling \ref{Hähnel03}. This is natural since laser range finders and LADAR scanners output a high resolution point cloud that is accurate up to sub-centimeter lengths in range of hundreds of meters. In this thesis, a stereo camera based environment modeling approach is used since measured lengths are limited to maximum of 15 meters depending on the application. While the accuracy of the stereo camera depth map output is suitable for the bounding volume computation of a load object, the price of the system is much smaller when compared to a laser based solution. 

The main reason for choosing machine vision as the medium for the load object measurement system was the lower cost of cameras in comparison to LIDAR technology. Other benefits are better load modeling with more data entries in the load object point cloud, and passive triangulation technique that does not send electromagnetic radiation (e.g. laser) in the environment. LIDAR scanner can be used to verify a stereo vision camera calibration so that the load object measurement output is correct \ref{You11}. Himmeli was a natural choice for perception platform since the installed active ranging technique could be used to verify the performance of the stereo vision system.

% goal with the stereo camera acquisition is that the overall system would be cheap to purchase
% objective: point cloud processing runs in almost real-time, more often than 5 times per second

An energy-saving, semi-automatic, safe process crane working area is the desired final outcome for upgrading a traditional manually operated process crane installation. It is under review whether a detailed, almost real-time description of the environment could provide enough information to realize the envisioned system.

\chapter{Perception Sensors For Environment Model Acquisition}
\label{chapter:perception_sensors_for_environment_model_acquisition}

Perception sensors in a machine vision system include sensors such as digital cameras, time-of-flight cameras, structured light cameras, and other equipment that may recover information about the surrounding environment by capturing emitted electromagnetic radiation. Machine vision cameras may use the visible spectrum of the light, or sometimes other spectrum of light e.g. ultraviolet light spectrum. A lot of different sensor options are commercially available for machine vision systems. The design of the machine vision system is highly affected by the selection of the sensor and the digital format of the information that is output from the sensor.

The problem of the load object geometry measurement inherently requires a sensor that can recover 3d geometry of the environment. Extensive research has been done in the field of 3d geometry recovery, and by utilising these vast sources of research many different sensors can be selected that can produce a good 3d geometry model of the environment.

Besides machine vision, also other perception sensors exist that can be used to recover 3d geometry, such as laser scanners and radars. These are not considered strictly machine vision sensors, but for example the laser scanner technology is accurate enough to be used to verify a 3d geometry that any of the camera based solutions have recovered. Thus, a Velodyne laser scanner was used in capturing an accurate model of the 3d environment in this work.

Next, different machine vision sensor options will be presented that could provide data input to the envisioned load object geometry measurement system.

\section{Machine Vision Perception Sensors}
\label{section:perception_sensors}

Camera based vision sensing can be active or passive depending on the technology used. An active sensor emits energy into the environment and measures the energy radiated back to a sensor array. A passive sensor does not emit energy on its own, but registers and measures energy radiated to the sensor array from the environment. In general, the energy that is registered and digitally sampled by a camera is visible light, which will be the technology used in this work.



+ active sensor: a sensor that controls and uses its own images. An active sensor controls its image's illumination with an electromagentic energy source, such as laser or white light.
+ time-of-flight cameras (TOF)
According to other research, the Kinect sensor cannot observe items cast in direct sunlight, preventing extensive outdoor usage besides its range limitations \cite{tikkanen13}.  

\subsection{Stereo Cameras}
\label{subsection:stereo_cameras}

\subsubsection{Optics}
\label{subsubsection:optics_and_photometrics}

% todo add teledyne white paper to optics
%\ref{teledyne_whitepaper}

Intensities perceived by a visual system are a function of the target geometry, reflectance of the target surface, illumination, and the camera viewpoint.
% http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/GOMES1/marr.html

The machine vision cameras include optical systems that control the amount of light entering the photosensitive CMOS sensor array in the camera. A basic understanding of parameters that affect the physical behaviour of light entering the sensor is needed to realise a working design for the application at hand. The most important parameters that must be set in the optical system include focal length, field of view (FOV), depth of view, and camera shutter speed. Other parameters that may affect the performance of the camera system are the selection of materials, and how light enters through the interface of two different materials. Different materials may be used inside the lenses and on the lens coatings. 

The aperture of the camera lens system controls the depth of field (DOF) perceived in a digital or analog image of a scene. The depth of field describes the range of depth that can be viewed in sharp detail through the lens system. If the DOF is large, then only a small depth range is in sharp detail, and the rest of the image is blurry for near-field objects and faraway objects. If the DOF is small, then near-field objects and faraway objects are sharply in focus both alike. For the design of a stereo camera depth measurement system a small DOF, or extended DOF, is used to fully enable a deep range of a scene in sharp detail. Thus, the load object will stay in sharp detail before and after the lifting nearer to the cameras. Since a small DOF is achieved by using a larger aperture value, a tradeoff is done between feature sharpness and the amount of noise present in the image. Aperture sizes are limited by diffraction, thus, small DOF images may suffer from high variance noise effects and low light conditions.

The aperture of the camera is described by a unitless number that is presented as f over a number

\begin{equation}
\label{eq:lens_aperture}
f/# = N = \frac{f}{D}
\end{equation}

where f is the focal lenght used and D is the diameter of the pupil of the lens system. Higher f numbers have a large depth of field since the camera lens approaches the asymptotic limit of the pinhole camera.


% angle of view of the lens???

% resolution of the sensor array,
%camera dynamic range
%electromagnetic spectrum range 
%Radiance is a measure of power of light radiated from a unit surface area of an object to some spatial angle. 
%Irradiance is a measure of power of light cast on a unit surface are of an object.
%The relation between an object's surface that radiates onto the camera sensor array can be formulated as

%\begin{equation}
%\label{equation:radiance}
%E = L \frac{\pi}{4} (\frac{d}{f})^2 \cos^4{\alpha}
%\end{equation}

%where L is object's radiance of unit surface area, and $\frac{d}{f}$ is the f-number of the lens in use. The $cos^4{\alpha}$ is the vignetting term. Not all radiating energy from the object is captured by the CMOS sensor array, but only the fraction that enters the camera lens and falls on the effective sensor array surface.

%weather conditions
    


\section{Environment Models}
\label{section:3d_environment_representations}
%+ possible algorithms: clustering, water-filling, and convex hull computations

Environment models are different types of digital data representations that describe the environments acquired by a perception sensor. The work implemented in the Chapter \ref{chapter:implementation} uses point cloud representation, which will be presented.



In the virtual reality research the modeling of real-world objects has been an issue for many years. It is not easy to recreate a real-world item in a digital system, but many approaches have been suggested.

One option is to record video from multiple cameras that look at the object from different points of view. In order to obtain a digital model we may utilise a multi-camera stereo process and compute range images for further processing. Obviously, this approach is very computationally difficult and results vary depending on the object convexity, lighting environment and the used algorithms.  

A study by the Carnegie Mellon university robotics institute call the model acquired from the intensity image and the corresponding range imagery a visible surface model (VSM) \cite{Rander97}. Basically a VSM contains a subset of real-world surface features and texture in the digital format, and the VSM can be further placed into a more comprehensive representation of the world, namely a complete surface model (CSM). 

\subsection{Point Cloud Representation}
\label{subsection:point_cloud_representation}

%Challenges
%(Hornung et al.) a point cloud uses up a large amount of memory and are only suitable for modeling static environments with high precision sensory equipment.

\subsection{Range Image Representation}
\label{subsection:range_image_representation}

Range image is a type of 3d data point representation that uses a 2d image array whose pixels correspond to a known location in the imaged scene. A range image is conceptually equivalent to an intensity image where image intensities are replaced with range measurements acquired from a sensor.

Range images have limitations in the ability to reproduce a detailed original point cloud. The term detailed point cloud here is a reference to an object model that loosely describes the object shape and details using a convex hull of points in space, including points that may be not visible from a single viewpoint. Then, if the detailed original point cloud includes multiple points on a line that projects to a single pixel in the range image, some data will be lost in the conversion to a range image. 

Also, if the ranging sensor has a spatially different measurement lattice structure compared to the 2d-array structure of range image, some pixels may be left blank because of a mismatch in spacing of range measurements.

for range measurements than what the range image uses, then some pixels in the range image will be left empty.

Since we create the point cloud with a camera system, we would not have the described projection data loss problem. Another problem with range images       \cite{Unnikrishnan08}.

\subsection{Volume Pixel Representation}
\label{subsection:volume_pixel_representation}

A volume pixel, or a voxel, is a prototype of a single entry in a 3d grid structure consisting of voxels. A voxel indicates occupancy in space, and it has at least a volume, and a location, which usually is in 3-dimensional array with constant dimensional sampling. A perfect example of a prototype voxel is a wooden box, which occupies a constant volume in space, and has a location in terms of, for example, a living room space. In digital systems, a voxel is more of a conceptual item that has a location, size, and optional services, such as statistical information about the encompassed voxel subspace. 

A 3d scene captured with a stereo camera system can be represented with a voxel grid. A standard voxel grid will have a constant sampling grid, thus, the scene will be composed of same-sized cubes. Anoter option is to use a varying sampling grid structure, or an octree structure (described in...?).

limitations: grid resolution, high-resolution grid consumes a lot of memory

\subsection{Tree Structures}
\label{subsection:tree_structures}
% multi-resolution octree representations

\subsubsection{Octree Structure}

Octree is a tree structure whose nodes always have exactly eight child nodes. The octree structure is used to partition 3d data recursively into smaller partitions of space around a point region. The point region and the sub-quadrants of an octree node are symmetrical cubic volumes also known as voxels. In an octree, the size of the voxels change depending on the depth of the tree search at hand according to Figure \ref{fig:octree_quadrants}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{octree_quadrants.pdf}
    \caption{Bla bla bla.}
    \label{fig:octree_quadrants}
  \end{center}
\end{figure}

Octrees are used in 3d computer graphics applications for nearest neighbour search, efficient collision detection, and set operations (e.g. union, intersection, and difference) among other uses. The nodes of the octree include states for the regions of space with a free, occupied, or unknown status. This property makes the octree structure able to support a detection, classification and tracking of moving objects scheme in a 3d environment \citep{Azim12, Ouyang12}.

Octree representation is suitable for fast interactive physics simulation purposes or feedback control, because the memory efficient structure enables a detection of virtual object collisions in just milliseconds \citep{Noborio99}. Using these properties, detection and tracking of dynamic moving objects in a static environment can be done.

%opinion
Octree would be a suitable representation format for load object environment data because it supports many of the operations needed to realise a collision avoidance system, such as the collision detection possibility.


Octree structure is a 3d extension to the quadtree, which is a four-pointer binary tree. Originally, the idea to use octrees for spatial representation was introduced by Donald Meagher in 1982 \cite{Meagher82]. Today, many different implementations exist, for example, a probabilistic occupancy estimation approach of the Octomap library, \citep{Hornung10}.

% how to build an octree
The octree structure may be computed from non-convex polygonal contours from multiple different viewpoint cameras (structure from silhouette), but up to 5 - 15 different viewpoints are needed to attain good precision \citep{Noborio88}.

%Octomap
+ Octomap can model occupied space, free space, and unknown space all at the same time.
We can assume that in all cases of range measurements a lot of noise and uncertainty will be introduced in the measured values. Thus, it is a good idea to represent the range measurements in a map that is probabilistic in nature.
Octomap
    + memory-efficiency principle (memory and disk space handling)
    + differentiates unmapped and obstacle-free areas
    + supports probabilistic sensor fusion
    + can represent arbitrary shapes in space (elevation map cannot)
    
Octrees are used in
    + id Tech 6 game engine
    
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{velodyne_octree.jpg}
    \caption{An octree whose leaf nodes are visualised with a coarse resolution. The red object represents a large standard box (see section \ref{subsubsection:analysis_overheadcrane}) lifted with an overhead crane. The green object represents a crane operating person near the load object. Data from Velodyne HDL-32e LIDAR scanner.}
    \label{fig:velodyne_octree}
  \end{center}
\end{figure}

\section{weather}
Quality of measurements degrade in bad weather, such as fog, mist, rain, or snow \cite{Kawai12} 
Especially camera systems are affected with possibly inhomogenous degradation in image quality.

Different types of degradation:
    + lens flare
    + condensation in humidity changes
    + changes in illumination
    + shadowing
    + occlusion
    + noise from moving droplets of water e.g.

If a stereo camera is used, inhomogenous degradation in image quality


Colored markers are sensitive to natural light in outdoor environments \cite{Kawai12}.

\section{Geometric Environment Acquisition Techniques}
\label{section:environment_acquisition_techniques}

In this work the focus on environmental acquisition is on systems capable of 3d modeling the geometry of surrounding environments. Other environmental variables, such as temperature, wind conditions, humidity or other qualities of the environment are not considered in the context of 3d acquisition, although they could possibly have an effect on the perception sensor parameters in a real-world acquisition system. 

\subsection{Object And Environment Modeling}

In general, an unordered real-world scene is difficult to model using perception sensors. A lot of study in real-world object modeling has been done using multiple sensors including laser range scanners, multiple viewpoint cameras, color cameras, inertial measurement units (IMU) and global positioning system (GPS). Multiple perception sensors and large image quantities from a single environment are needed in order to compute a realistic virtual environment model that displays a correct geometry of the environment, and presents textured object models with realistic lighting. \ref{ElHakim98}



Since the used HIMMELI platform based solution that is used in this work enables only a single viewpoint stereo camera produced depth map at a single timestep, the model of the acquired environment is limited to a sparse point cloud presentation. Thus, the output 3d geometry will not resemble any realistic scene, but it does contain keypoints coordinate information that enables Euclidean dimension measurements. Additionally, a single-channel black and white high definition range image is available for e.g. object classification purposes. More in depth specifications of the HIMMELI platform are presented in chapter \ref{section:himmeli_sensor_platform}.

If multiple depth maps are used to acquire a complete model of an object, then data acquisition, registration between different viewpoints, and integration of the different registered views are compulsory steps in modeling.  Multiple camera view registration is a classic technique that can estimate geometric surfaces of objects in more complete detail than a single camera view depth mapping. Multiple view registration is outside of the scope of this thesis, but an interested reader may find extensive research in literature on complete object modeling using multiple registered camera images.  \ref{Chen91}

All geometric acquisition techniques either use an active perception sensor that emits energy in the environment, or use a passive perception sensor that only detects energy reflected or radiated from the environment. Next, some techniques that are used to acquire geometric environment depth maps are introduced. Triangulation and structure from motion (SFM) are techniques that uses passive sensing of the environment with cameras. Laser scanning and structured light patterns use active sensing of the environment using light sources and cameras. Triangulation is the basic mechanism that most 3d acquisition techniques are based on, which is why it is discussed before other techniques are introduced.

\subsection{Triangulation}
\label{subsection:triangulation}

Triangulation solves a distance to a point by measuring two angles from known points that are located at two ends of a fixed baseline. Thus, the known camera viewpoints and the unknown point form a triangle. Before the invention of the Global Positioning System (GPS), mapping of land areas (calculating distances and directions of landmark features) was performed using triangle meshing with the triangulation technique.

The term triangulation was originally used in trigonometry calculus, but in machine vision systems it refers to the indirect process of measuring the z-coordinate, or depth, of image features using two sensors, often producing a depth map. Triangulation system that uses two cameras is called a passive triangulation system, and a system that uses a single camera and a light pattern projector is called an active triangulation system. In this thesis only passive triangulation techniques are used, which is why the two camera simple stereo geometry will be introduced next.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{simple_stereo_geometry.pdf}
    \caption{Simple stereo geometry where distance Z may be solved using similar triangles ($p_l$, $P$, $p_r$) and ($O_l$, $P$, $O_r$). Adapted from \citep{Trucco98}.}
    \label{fig:simple_stereo_geometry}
  \end{center}
\end{figure}

%is a passive range acquisition mechanism that produces a range measurement from the solution of the correspondence problem from two differing images. The difference between the images should be a translational shift along a camera baseline axis, for example a stereo pair using simple stereo geometry will suffice. Tri
%Often, a stereo camera is used, and the correspondence problem is solved using rectification and block matching techniques.

As soon as the correspondence problem is solved (see section \ref{section:stereo_correspondence_problem} for details) the distance to point P may be calculated using the similar triangles ($p_l$, $P$, $p_r$) and ($O_l$, $P$, $O_r$) in Figure \ref{fig:simple_stereo_geometry}. $\pi_L$ and $\pi_R$ are the image planes for the left and right cameras accordingly, and $p_r$ and $p_l$ are the projections of the point P on the image plane. 

%when a corresponding point shift information is known for a feature in a scene. 

%Reconstruction by triangulation is the process of generating a depth map from two camera images e.g. parallel stereo cameras. If both intrinsic and extrinsic parameters of the camera are known, the reconstruction problem can be solved unambiguosly producing absolute coordinates in the world coordinate frame. As a simplification, two other cases would be that only intrinsic parameters are known, a

%The results of triangulation mainly depend on the solution of the correspondence problem. 


%In order to better understand the triangulation technique the simple stereo camera geometry will be introduced next.




%+ lidar cannot measure the depth of highly reflective materials, or very low-reflectance materials such as glass or paint-black steel.
%+ Mechanical coordinate measuring machine (CMM) (3D modeling)
%    + DCC CMM

%+ Structured Light Triangulation
%    +laser plane range finder

\subsection{Structure From Motion}
\label{subsection:structure_from_motion}

Structure from motion (SFM) is a machine vision technique that uses only a single camera and a sequence of images to  reconstruct a 3d geometry from a scene. Many other similar techniques exist, for example, structure from silhouette
, and structure from structured light, which are just applications of the SFM technique.

Structure from motion is similar to biological visual systems that easily infer 3d structure from motion with very little a priori knowledge of the world. For example, if a person closes one eye, it is possible to understand the 3d geometry of the surrounding environment only by moving in it, and the structure from motion mechanisms in the brain computes the correct geometry without binocular vision.

A simple structure from motion system is able to reconstruct static scenes with a moving camera, but reconstructing a moving scene e.g. a busy process hall with a crane load object in the middle is a more difficult problem. In the case of reconstructing dynamic environments with a single camera, some additional problems must be addressed, such as what image regions correspond to moving objects and which do not, and how does the illumination change spatially and temporally.

%Motion field
\subsubsection{Motion Field And Optical Flow}
\label{subsubsection:motion_field_and_optical_flow}

In SFM processing, the reconstruction of a 3d scene from a sequence of images is based on motion field computation. Motion field is a 2d vector field of velocities of image points induced from a scene viewed by the camera. It represents the projections of 3d motion of visible features. Only approximations of the ideal motion field can be computed, for example the optical flow is such an estimate of the motion field.

Optical flow describes the apparent motion of an object in a visual scene. It can be estimated using differential methods, feature-based methods, or phase correlation methods. In practice, a good differential method that is easy to implement and use is a method originally published by Lucas and Kanade, which is referred to as an optical flow algorithm in \ref{Trucco98}.\ref{Lucas81}

In Lucas's method the optical flow is estimated using the image brightness constancy equation. The image brightness constancy equation computes the optical flow only in the direction of the normal of the spatial image gradient as shown by the aperture problem. The aperture problem shows in more detail how it is not possible to understand the actual direction of movement of a spatial gradient seen through an aperture.  

The optical flow is well estimated using the brightness constancy equation for some cases where the objects are assumed to exhibit lambertian surface properties, be illuminated by a point light source at the distance of infinity, and  no photometric distortions are present. Under the previously mentioned assumptions the error of the estimated normal of motion field is small at image points with high spatial gradient. The error is zero only for components whose motion is translational only, or whose illumination direction is parallel to the angular velocity. 

Motion field of an SFM system is similar to a disparity map in a stereo vision system: they both represent a point displacement map. While motion field is a differential concept, the disparity map is not, which means that the motion field is an estimate of the point displacement map.

\subsubsection{Structure From Motion Challenges In Crane Environment}
\label{subsubsection:structure_from_motion_challenges_in_crane_environment}

Structure from motion is a mathematically challenging approach to reconstruct a 3d geometry using a single camera, mostly suited for applications of reconstructing static 3d geometries from aerial images, city streets or landmark items. It is not well-suited to solve the problem of load object measurement in an overhead crane environment for three main reasons. 
First, the load object measurement must be available when the camera is not moving and the load object is not moving. Structure from motion reconstruction will not be available in such an event. 
Secondly, the overhead crane environment is dynamic containing multiple moving objects, and a camera platform that moves relative to the objects. In such dynamic cases, a simple SFM process described earlier cannot solve a rigid transformation between consecutive frames since the environment is morphing between frames. A general multibody structure from motion solution has yet not been formulated, but theoretical approaches and requirements for a system that can process dynamic multibody environments using SFM have been researched\ref{Ozden10}.
Last, an empty overhead crane hall contains a planar scene, which is difficult to reconstruct correctly in a SFM system. Coplanar points have similar depth in an image sequence, and for example, for a dense 3d motion and structure algorithm it is assumed that local observer motions include large variations in depth, which is not the case in a planar scene\ref{Trucco98}.

% another snippet
%A histogram is a two-dimensional graphical representation of an image grey-level distribution. The grey-levels correspond to the image intensity. As depicted in figure X, the X axis represents the number of independent grey levels that the image contains. The grey-levels range from 0 to N levels. For example, in an 8-bit single channel image there can be up to 255 independent different tones of gray. The Y axis represents per centage information about a single grey-level ranging from 0\% to 100\%. Thus, the histogram essentially displays a distribution of all grey-levels.
%end of chapter 3
\chapter{Stereoscopic Machine Vision}
\label{chapter:stereographic_machine_vision}
% stereoscopic means to perceive and/or see in 3d | stereographic means projecting a sphere onto a plane
Stereoscopic perception is a natural phenomenon first discovered in human and animal vision systems. Stereoscopy in human vision is based on stereopsis effect, which means perceiving depth in a scene using binocular vision with two monocular vision eyes. The same depth perception can be programmed to a computer, but instead of eyes, cameras are used. Instead of the brain, a PC is used to recover and process depth information from a pair of camera images. A calibrated two-camera system that is capable of extracting depth information from objects in such a way is most often called a \emph{stereo vision system} or a \emph{3d vision system}.

While humans perceive and estimate object sizes with the help of their previous knowledge of the world and their implicit stereo vision calibration by the brain, a machine vision system must be carefully calibrated to output metric measurements for objects. For simplicity, in this thesis a single-time calibration is considered and no machine learning techniques are used to improve the calibration at a later time.

After capturing a pair of images and processing them, it is possible to request a 3d coordinate value for almost any feature found in one of the images. In this way, the machine vision system can deliver coordinates for many object features in a scene simultaneously, which is quite impossible for a human to do. By selecting visible features of the load object in the 2d image, the 3d coordinates for those points are recovered. Finally, the load object dimensions are recovered by taking difference of the minimum and maximum coordinates along all axes. Transforming the 2d image into a partial 3d feature model is important since the measurement is ultimately done by requesting depths of points of interest and measuring their difference. While the approach does have its limitations, a partial 3d model is acquired that can be used to successfully find a bounding volume that contains the load objects. Improvements and limitations are discussed more in depth in the evaluation chapter (\ref{chapter:evaluation}).

\section{From 2d to 3d}
\label{section:from_2d_to_3d}
%Different 3d information recovery techniques are introduced in section \ref{section:environment_acquisition_techniques}. 

In general, 3d vision systems used in engineering applications solve problems of 3d scene reconstruction, understanding of object properties, and minimisation problems depending on the scientific application at hand. Often a stereo camera setup is used to solve the problem of 3d information recovery using knowledge about the camera setup geometry and the imaged content\cite{Sonka07}. 

In this thesis, the machine vision system primarily solves the 3d scene reconstruction problem. Some object properties are computed in the implemented prototype software, and minimisation problems are not encountered. The object-centered 3d representation of the load objects and other objects is based on Marr's theory, but the implementation only loosely follows his framework. 

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{marrs_framework.jpg}
    \caption{Marr's representational framework according to\ref{Marr82}.}
    \label{fig:marrs_framework}
  \end{center}
\end{figure}

According to Marr's theory machine vision systems should aim for a better understanding of the visualised environments in general, and not just aim to create another vision application suited for a single specific purpose. For example, to improve description of objects, a three-dimensional object-centered representation is suggested favourable to a pixel-based intensity image representation. It is suggested that this transformation from intensity-based, viewer-centered description to the feature model based, object-centered description can be achieved in three steps that follow Marr's representational framework shown in Figure \ref{fig:marrs_framework}.

\begin{itemize}
\label{list:objectcenteredmodeling}
\setlength{\itemsep}{0pt}
\item 1. Pixel representation is processed into a surface representation (primal sketch)
\item 2. Surface representation is processed into an oriented surface model (2.5d - depth map of visible surfaces)
\item 3. Surface orientation model is processed into a 3d description (3d model)
\end{itemize}

\cite{Sonka07}

In this thesis the pixel representation of the real scene is processed simply into a disparity map, which can not be regarded as a primal sketch since no separate object surface knowledge is available. The disparity map is then reprojected into a full 3d model with the existing depth information. No object models, or feature surfaces are known, thus, the machine vision process only produces 3d data, and does not track 2d image features. The described object-centered representation is partially implemented with point cloud operations, which are introduced later in section \ref{section:point_cloud_library_operations}.


\section{The Pinhole Camera Model}
\label{section:the_pinhole_camera_model}
%1. pinhole camera model
%2. orthographic projection
%3. scaled orthographic projection
%4. paraperspective projection
%5. perspective projection
%source: NUS CS4243 camera.pdf slide
%The real-world object projection on the camera image plane is a lossy transformation where a lot of information is lost due transformation. For example, when a cube is projected on the image plane, we cannot compute the texture of all 6 cube faces anymore because the data of all non-visible cube faces is lost in the image plane.
%In general, a perspective projection, or central projection, is the de facto type of projection for human beings. 
%In perspective projection objects that are far away appear smaller than objects that are near. This phenomenon is called a perspective. 

The pinhole camera model maps 3d objects onto a 2d image plane using simple pinhole camera geometry. A three-dimensional object in real space $\mathbf{R}^3$ is mapped into a two-dimensional projection in the subspace $\mathbf{R}^2$ according to $\mathbb{R}^3 \mapsto \mathbb{R}^2$, also known as a perspective projection. The depth component z of a 3d coordinate is lost in the transform, which is natural for any projective camera models. The transform is irreversible, which means that the original 3d coordinates cannot be recovered from a projective 2d image. The pinhole camera model is introduced because it is the simplest approximation that can be used in computer vision applications. Additionally, a pinhole camera model is used in the implementation of some openCV computation in Chapter \ref{chapter:implementation} \citep{Sonka07, OpenCVWeb}.

%Typical camera model is usually based on ortographic projection or perspective projection models. In 3D imaging a suitable camera model would be the perspective projection model with lens distortion modeling included. The selection really depends on the applications and its accuracy requirements for model accuracy, but in simple computer vision applications the linear ortographic model would be appropriate. 

\subsection{Pinhole Camera Geometry}
\label{subsection:pinhole_camera_geometry}
%\begin{itemize}
%\label{list:intrinsiccameraparameters}
%\setlength{\itemsep}{0pt}
%\item -fa = represents scaling in the $u_a$ axis
%\item -fb = shear coefficient that gives the alignment difference of camera coordinate system axis $x_c$ and affine image coordinate system axis $u_a$ at the length of focal length in pixels in the direction of affine image coordinate system axis $v_a$.
%\item -fc = represents scaling in the $v_a$ axis
%\item $u_0$ = u coordinate for the optical axis intersection on the image plane on the $u_a$ axis
%\item $v_0$ = v coordinate for the optical axis intersection on the image plane on the $v_a$ axis
%\end{itemize}
%The width of the image sensor can be related with field of view (FOV) angle by
%\begin{equation}
%w = 2f\tan{\frac{\theta_{fov}}{2}}
%\label{equation:width}
%\end{equation}
%If we consider the effect of resolution on the disparity computing, or depth value computing, we can start with the number of pixels in the image sensor by equation
%\begin{equation}
%number of pixels = wh = \frac{w^2}{a}
%\end{equation}
%Furthermore, we can substitute width \emph{w} from \ref{equation:width} getting the equation for the number of pixels as in \cite{Gallup08}:
%\begin{equation}
%nop = \frac{{z_{far}}^4}{{\epsilon_z}^2} \frac{4\tan^2{\frac{\theta_{fov}}{2}}}{b^2 a}
%\label{equation:zdepth}
%\end{equation}
%As we can see from equation \ref{equation:zepth} the increase in pixel resolution to match a specified accuracy also must match the depth resolution proportionally to ${u_{0},v_{0}z$_{far}}^4$. Consequently, the required increase in resolution may be impossible due to engineering limitations of new hardware or computations limits of increased overheard due to increase in number of pixels.
%The increase in stereo processing computation overhead is tightly coupled with the selected image resolution and depth error. According to \cite{Gallup08} the number of pixel comparisons needed is in the growth range of $ \Omega({z_{far}}^6 {\epsilon_z}^-3)$. For example, if we wish to extend the depth range by a factor of 3 the required number of comparisons would be $3^6 = 729$ times more computationally expensive.
%When faced with this kind of image computation problem that should ideally be run in real-time the design of the imaging system must be carefully planned. It should be noted that increase in image resolution seems not to solve a stereo correspondence problem, but it may severely increase the computation time. 

The pinhole camera is a simple box that has an ideal pinhole aperture on one of its sides, see Figure \ref{fig:pinhole_camera}. The camera forms an upside down image of a 3d object located at a distance of $p$ from the camera lens (pinhole) on its imaging plane $\pi$. An optical axis $I_o$ (dashed line in Figure \ref{fig:pinhole_camera}, positive Z direction in the camera frame $O_c$) lies perpendicularly to the image plane pointing towards the object. In a pinhole camera, the object and its real projected image are related by the thin lens equation

\begin{equation}
\frac{1}{f} = \frac{1}{p} + \frac{1}{q}
\label{eq:lens_law}
\end{equation}

where f is the focal length of the thin lens. The upside down image is formed on the image plane as long as the 3d object or its image are not nearer than $f$ units from the focal point of the lens. Using similar triangles it can be shown that the point P coordinates are projected onto the image plane point Q according to

\begin{equation}
x = f\frac{X}{Z}
\label{eq:image_projection}
\end{equation}

\begin{equation}
y = f\frac{Y}{Z}
\label{eq:image_projection2}
\end{equation}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{pinhole_camera.pdf}
    \caption{A simple pinhole camera projects imaged objects upside down on an image plane. The image formation is based on similar triangles between the optical axis $I_o$ and the red light ray (hypothenus).}
    \label{fig:pinhole_camera}
  \end{center}
\end{figure}

In homogenous form, a projected point $\tilde{Q}$ = ($\tilde{x}$,$\tilde{y}$,$\tilde{z}$) can be further formulated as

\begin{equation}
\tilde{x} = f\frac{X}{\tilde{z}}, \tilde{y} = f\frac{Y}{\tilde{z}}, \tilde{z} = Z
\label{eq:image_projection}
\end{equation}

where the origin of the homogenous coordinate system is point (0,0,1). This coordinate system can be shown in homogenous matrix form as 

\begin{equation} \tilde{Q} =
\label{eq:homogenous_matrix_form}
\begin{bmatrix}
f & 0 & 0 \\
0 & f & 0 \\
0 & 0 & 1\end{bmatrix}
\begin{bmatrix}
X \\
Y \\
Z \\
\end{bmatrix}
\end{equation}

Point P can be written in homogenous matrix form as well as 

\begin{equation*}
\tilde{P} = (X,Y,Z,1)
\end{equation}

so that the projection becomes a linear matrix operation

\begin{equation} \tilde{Q} = 
\label{eq:homogenous_world_point}
\begin{bmatrix}
f & 0 & 0 & 0 \\
0 & f & 0 & 0 \\
0 & 0 & 1 & 0 \end{bmatrix}
\tilde{P}
\end{equation} 

or

\begin{equation}
\label{eq:homogenous_projection}
\tilde{Q} = C\tilde{P}
\end{equation}

where C is a 3 x 4 matrix called the camera matrix, and $\tilde{Q}$ and $\tilde{P}$ are the homogenous image plane projection and 3d object surface point coordinates, respectively. The camera matrix can be factored into form

\begin{equation}
\label{eq:factor_camera_matrix}
\tilde{Q} = KP_{0}\tilde{P}
\end{equation}

where K is the camera calibration matrix that contains the intrinsic parameters of the camera, and $P_0$ is the projective matrix of the camera model. Since stereo calibration later requires the model for both intrinsic and extrinsic parameters, a general model including both intrinsic and extrinsic parameters can be written on the basis of equation \ref{eq:factor_camera_matrix} as

\begin{equation}
\label{eq:general_camera_model}
\tilde{Q} = KP_{0}T_{c}^{-1}\tilde{P}
\end{equation}

where $T_{c}^{-1}$ is the extrinsic calibration matrix containing at least 6 parameters that describe translation and orientation of the camera from the world frame origin. All matrices K, $P_0$, and $T_{c}^{-1}$ can be combined into a single 3 x 4 camera matrix C that performs translation, rotation, scaling, and perspective projection\citep{Corke11}. 

\subsection{Extension To Pixel Frame}
\label{subsection:extension_to_pixel_frame}

The image sensor contains light sensitive photosites that are arranged in an array whose width \emph{w} and height \emph{h} determine the resolution of the camera. In a modern digital camera, the pixels are rectangular with no skew, which means that the angle between the width and height component is strictly 90$\textdegree$. The pixel coordinates are by convention in a non-negative $(u,v)$ coordinate frame whose origin is located at the top left corner of the image plane (value $(0,0)$). 

The principal point of the image sensor is the center of the sensor array where the principal ray enters the camera through the pinhole aperture perpendicularly to the image plane. If the principal point is shifted due to lens alignment error, then the shift can be adjusted by shifting the principal point $(u_{0},v_{0})$ using the intrinsic camera calibration matrix K. A camera calibration matrix that can adjust for lens alignment error and take pixel skew into account is written so that

\begin{equation} K =
\label{eq:extended_camera_calibration_matrix}
\begin{bmatrix}
f_x & 0 & u_{0} \\
0 & f_y & v_{0} \\
0 & 0 & 1
\end{bmatrix}
\end{equation}

where $f_x$ and $f_y$ are the orthogonal focal length components, and $u_{0}$ and 
$v_{0}$ are the principal points for the camera. The K matrix can be written with a pixel skew correction value in component K(1,2), but with modern precise semiconductor technologies the u and v axes are very precisely orthogonal, which is why in Equation \ref{eq:extended_camera_calibration_matrix} the skew term is left zero\citep{Corke11}.

The point in the world according to the camera coordinate system now transformed to point
$X_c$. Next, the point will be projected onto the image plane $\pi$ according to the pinhole camera model. 
Point $X_c$ projects onto the image plane as Euclidean point $U_c$ where the projected point can be computed from the similar triangles of image \ref{image:projectedpoint}. 

\begin{equation}
\label{equation:projectedpointUc}
\textbf{U}_c = \Biggl[ \begin{array}{c}
\frac{-fx_c}{z_c} \\
\frac{-fy_c}{z_c} \\
-f \end{array} \Biggl]
\end{equation}

The image point that the camera outputs is the affine transformed $\textbf{U}_c$. Next, we must formulate a homogenous coordinate representation that allows to compute the affine transformation with a $3x3$ matrix multiplication for \ref{equation:projectedpointUc}.

We can represent a homogenous coordinate point in the image plane $\pi$ as 

\begin{equation}
\label{equation:affinedefinition}
\tilde{u} = \Biggl[ \begin{array}{c}
U \\
V \\
W \end{array} \Biggl]
\end{equation}

so that a Euclidean distance representation of point $\tilde{u}$ is 

\begin{equation}
\textbf{u} = \Biggl[ \begin{array}{c}
u \\
v \end{array} \Biggl] = \Biggl[ \begin{array}{c}
\frac{U}{W} \\
\frac{V}{W} \end{array} \Biggl]
\end{equation}

Now we have calculated the projected point \ref{equation:projectedpointUc} and we have determined that the resulting affine transformation results in $\tilde{u}$ in \ref{equation:affinedefinition}. We still need to define a principal point of the image plane $\pi$, which is the point where the optical axis and the image plane $\pi$ intersect. This point defines the homogenous transformation according to the intrinsic camera parameters. The principal point is defined in the affine image coordinate system as

\begin{equation}
\textbf{U}_{0a} = \Biggl[ \begin{array}{c}
u_0 \\
v_0 \\
0 \end{array} \Biggl]
\end{equation}

The equation for the affine transformation of projected point $U_c$ is

\begin{equation}
\tilde{u} = \Biggl[ \begin{array}{c}
U \\
V \\
W \end{array} \Biggl] = \Biggl[ \begin{array}{ccc}
a & b & -u_0 \\
0 & c & -v_0 \\
0 & 0 & 1 \end{array} \Biggl] \Biggl[ \begin{array}{c}
\frac{-fx_c}{z_c} \\
\frac{-fy_c}{z_c} \\
1 \end{array} \Biggl] 
\end{equation}

and with reordering the multiplication of $-f$ we obtain

\begin{equation}
\tilde{u} = \Biggl[ \begin{array}{ccc}
-fa & -fb & -u_0 \\
0 & -fc & -v_0 \\
0 & 0 & 1 \end{array} \Biggl] \Biggl[ \begin{array}{c}
\frac{x_c}{z_c} \\
\frac{y_c}{z_c} \\
1 \end{array} \Biggl]
\end{equation}

The obtained $3x3$ transformation matrix is called the camera calibration matrix K where

\begin{equation}
\label{equation:cameracalibrationmatrix}
K = \Biggl[ \begin{array}{ccc}
-fa & -fb & -u_0 \\
0 & -fc & -v_0 \\
0 & 0 & 1 \end{array} \Biggl]
\end{equation}.

We have followed the definition of Sonka et al. \cite{Sonka07} so that the resulting camera calibration matrix is an upper triangle containing three unknown parameters $a$, $b$, and $c$ for shearing and rescaling. If we study the coefficients of K carefully, we can see that the matrix actually contains the parameters focal length, the principal point, shear coefficient, and the affine distortion coefficients \cite{CaltechWeb10}.



All the units are given in pixels.

It is obviously desirable to transform multiple coordinate points at a one time. Multiple coordinate point sets are called point clouds, and the transformation for a point cloud can be computed with the same square matrix used for single point transform.

If the point cloud $P$ contains $n$ points, we may structure a matrix P that contains the set of points as column vectors where

\begin{equation}
P = \Biggl[ \begin{array}{ccccc}
X_0 & X_1 & X_2 & \dots & X_n \\
Y_0 & Y_1 & X_2 & \dots & Y_n \\
Z_0 & Z_1 & X_2 & \dots & Z_n \end{array} \Biggl]
\end{equation}


\section{Intrinsic Camera Calibration}
\label{subsection:intrinsic_camera_calibration}

% T. Briggs article on autocalibration of a moving camera...?
%Camera calibration can be weak or strong. If strong camera parameters are known it is possible to calculate image scene Euclidian metrics. If weak camera parameters are known, only a pixel to pixel transformation from one image to another is possible, for example, an epipolar projection transformation can be done with weak parameters \cite{Rander97}.
%+ Why do we have to calibrate the cameras?
%Many fast but inaccurate algortihms for calibration parameter estimation have been introduced in the literature. The increase in processing power of computing platforms helps to develop new non-linear camera calibration methods that are more accurate. A subpixel accuracy of 1/50 of a pixel can be achieved with a modern CCD imaging cell if such non-linear calibration is used\ref{Heikkila00}.
%\begin{itemize}
%\label{list:intrinsic_parameters}
%\setlength{\itemsep}{0pt}
%\item Focal length
%\item Principal point
%\item Skew coefficient
%\item Distortion coefficients
%\end{itemize}
%Focal length is a 2x1 vector that reports the linear focal length components for projection of the final pixel coordinates $x_p$ and $y_p$ onto the image plane. Principal point is a 2x1 vector that reports the intersecting point of the optical axis and the image plane. After a distortion model is applied using the 5x1 vector to a normalised pinhole image projection an equation for the distortion corrected projection is
%follow the notation and the workflow of Heikkilä's paper, and the camera calibration toolbox provides its implementation\citep{Fetic12}. 

Intrinsic camera calibration is the process of estimating unknown camera model parameters, for example the coefficients of the camera calibration matrix K (\ref{eq:extended_camera_calibration_matrix}). Ideally, the calibration technique should estimate the camera parameters in an unbiased manner. Currently used calibration methods assume unbiased estimation, which is not true for most techniques, but the results can still be good. In this thesis, a MATLAB Camera Calibration Toolbox by Jean-Yves Bouguet was used to calibrate the stereo camera heads individually. The camera calibration toolbox includes a simple polynomial lens distortion model that extends the pinhole camera model introduced in section \ref{section:the_pinhole_camera_model}.

\subsection{MATLAB Camera Calibration Toolbox}
\label{matlab_camera_calibration_toolbox}

The intrinsic calibration parameters for the Himmeli platform were computed using Jean-Yves Bouguet's Camera Calibration Toolbox for MATLAB. The camera calibration toolbox uses internal camera model described by Heikkilä and Silven \citep{Heikkila97}, which supports the pinhole camera model introduced in section \ref{section:the_pinhole_camera_model}. The toolbox uses rectangular or circular calibration grids for calibration. A rectangular standard 9x6 crossings calibration grid was used to 

When a real imperfect camera lens system is used for image capture, lens distortions must be compensated by software to enable successful stereo block matching after image rectification. Lens distortions may include geometric distortions, such as tangential and radial distortions, and other imperfections, such as optical aberrations (chromatic aberration, spherical aberration, coma) and astigmatism. Geometric distortions hinder the performance of machine vision processes most, which is why the distortion model mainly corrects for the geometric qualities of the image. The need for different distortion model components depends on the used lens system, and it is probable that a low-quality webcam calibration requires a more complex lens distortion correction model than a high-quality built camera. The lens distortion model is added to the perspective projection 

\begin{empheq}[left=\empheqlbrace]{align}
\label{eq:projection_to_image_plane}
&x_p = f_{x}\Big(x_{d}+\alpha_{c}*y_{d}\Big)+pp_{x} \\
&y_p = f_{y}*y_{d}+pp_{y}
\end{empheq}

and added to normalised image coordinate compomenents $x_d$ and $y_d$ following the calibration procedure described in \citep{Heikkila97}. 


Bouguet's camera calibration toolbox provides lens distortion models up to a 6th order polynomial, but in the calibration of HIMMELI only a 4th order model was needed, leaving the 5th distortion coefficient in the 5x1 distortion coefficient vector $kc$ zero. Thus, the distortion corrected normalised point coordinates $x_{d}$ and $y_{d}$ follow:

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{extrinsic_parameters_left_cam.pdf}
    \caption{Left camera locations seen in a calibration grid centered world frame.}
    \label{fig:extrinsic_parameters_left_cam}
  \end{center}
\end{figure}

\begin{table}\centering
\ra{1.3}
\caption{Results for the left GigE $\mu$Eye camera from MATLAB Camera Calibration Toolbox calibration.}
\begin{tabular}{@{}lcclc@{}}\toprule
Left camera calibration results\\
\midrule
Focal length &  $f_x$ & 470.539535386206353 \\
 & $f_y$ & 470.052824804136947 \\
Principal point & $u_0$ & 361.485121180613135 \\
 & $v_0$ & 297.113415787221697 \\
Skew coefficient & $\alpha$ & 0.000000000000000 \\
Distortion coefficients (5x1 vector) & $k_{c}$ & [-0.007370335905467; \\
&  & 0.024800599631410; \\
&  &  0.005078340270310; \\
&  &  0.001150231090674; \\
&  &  0.000000000000000] \\
Focal length uncertainty & $\delta f_x$ & 3.131305478597541 \\
 & $\delta f_y$ & 3.092452183029686 \\
Principal point uncertainty & $\delta u_0$ & 1.869824435620952 \\
 & $\delta v_0$ & 1.931414801674318 \\
 Skew coefficient uncertainty & $\delta \alpha$ & 0.000000000000000 \\
Distortion coefficients uncertainty & $\delta k_{c}$ & [0.003655672859666; \\
&  &  0.003249915713652; \\
&  &  0.000992528547384; \\
&  &  0.000950170391783; \\
&  &  0.000000000000000] \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}\centering
\ra{1.3}
\caption{Results for the right GigE $\mu$Eye camera from MATLAB Camera Calibration Toolbox calibration.}
\begin{tabular}{@{}lcclc@{}}\toprule
Right camera calibration results\\
\midrule
Focal length &  $f_x$ & 483.971280001076536 \\
 & $f_y$ & 483.271035972874131 \\
Principal point & $u_0$ & 379.593549872983715 \\
 & $v_0$ & 308.868868269393147 \\
Skew coefficient & $\alpha$ & 0.000000000000000 \\
Distortion coefficients (5x1 vector) & $k_{c}$ & [-0.000205610973032; \\
&  & 0.018152928553306; \\
&  &  0.004792843185362; \\
&  &  0.000051370094457; \\
&  &  0.000000000000000] \\
Focal length uncertainty & $\delta f_x$ & 2.938569299488302 \\
 & $\delta f_y$ & 2.865562274787014 \\
Principal point uncertainty & $\delta u_0$ & 2.060568863512944 \\
 & $\delta v_0$ & 2.078319498413819 \\
 Skew coefficient uncertainty & $\delta \alpha$ & 0.000000000000000 \\
Distortion coefficients uncertainty & $\delta k_{c}$ & [0.003684674550123; \\
&  &  0.002636282917465; \\
&  &  0.001073704145219; \\
&  &  0.001045998483709; \\
&  &  0.000000000000000] \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{distortion_model_right_cam.pdf}
    \caption{Right camera lens distortion model with vector fields.}
    \label{fig:distortion_model_right_cam}
  \end{center}
\end{figure}

\begin{empheq}[left=\empheqlbrace]{align}
\label{eq:normalised_distortion_corrected_pinhole_projections}
&x_d = \Big(1 + kc_{1}r^2 + kc_{2}r^4 + kc_{5}r^6\Big)x_{n}+dx \\
&y_d = \Big(1 + kc_{1}r^2 + kc_{2}r^4 + kc_{5}r^6\Big)x_{n}+dy
\end{empheq}

where dx and dy are the tangential distortion vectors

\begin{empheq}[left=\empheqlbrace]{align}
\label{eq:tangential_distortion_vectors}
& dx = 2kc_{3}xy+kc_{4}\Big(r^2 + 2x^2\Big) \\
& dy = kc_{3}\Big(r^2 + 2y^2\Big)+2kc_{4}xy
\end{empheq}
.\par
The parameter $r$ in equation \ref{eq:normalised_distortion_corrected_pinhole_projections} is short for $r^2 = x^2 + y^2$. The tangential distortion coefficients calibrate decentering of the image and other defects in compound lens structure. The radial distortion coefficients corrects a barrel or pincushion distortion depending whether a telephoto lens system or wide angle lens system is used. A more in-depth analysis of the distortion modeling of radial and tangential distortions may be found in the original publication by Brown\citep{Brown71}.

The output of the calibration procedure will be a camera matrix K that relates the pixel coordinates in the image reference frame to the distorted projections of the actual objects in the camera reference frame. The camera matrix K is defined as

\begin{equation} K =
\label{eq:camera_matrix_k}
\begin{bmatrix}
f_x & \alpha_{c}*f_x & pp_x \\
0 & f_y & pp_y \\
0 & 0 & 1\end{bmatrix}
\end{equation}

where $\alpha_{c}*f_x$ is the skew coefficent stating the angle between the u and v pixel axes. K matrix relates the image reference frame and the camera reference frame so that

\begin{equation}
\label{eq:image_ref_frame_relation_to_camera_ref_frame}
\begin{bmatrix}
x_p \\
y_p \\
1\end{bmatrix}=K\begin{bmatrix}
x_{d} \\
y_{d} \\
1\end{bmatrix}
\end{equation}

Modern cameras do not have all the distortion properties that older cameras had, and some coefficients, such as the skew coefficient (suggesting rectangular pixels), and tangential distortion are normally defaulted to zero. For example, it is not recommended to use the 6th order distortion model for normal field of view cameras, but instead a 4th order polynomial should be used \ref{BouguetRef}. In most cases the principal point of the camera is located in the center of the sensor array, and its estimation is not recommended because it is difficult to estimate correctly.  
% number of calibration images available affect the selection distortion models
The coefficients of the camera calibration matrix K \ref{equation:cameracalibrationmatrix} contain the intrinsic camera parameters. If the coefficients of K are known, it is possible to extract metric values from the image plane \cite{Sonka07}. If a more complex camera model is used, the matrix K may have larger dimensions, but the intrinsic parameters still are the coefficients of K.  

\section{Extrinsic Camera Parameters}
\label{section:extrinsic_camera_parameters}

Since the camera lies in some arbitrary world coordinates, and the detected scene changes according to the camera orientation, we can easily see that the extrinsic camera parameters change when the camera pose changes in the world coordinate system. The relation between the world coordinate system and the camera coordinate system can be computed with an Euclidean transformation consisting of a translation and rotation. 

Extrinsic camera parameters describe the position and orientation of all the camera locations in a multiple camera stereo system relative to a world coordinate frame $W$. In stereo vision applications, extrinsic camera parameters are additionally used to describe the relative transformation from the left camera frame origin to the right camera frame origin. Thus, the extrinsic parameters can relate two viewpoints of the stereo camera setup to each other enabling a depth map computation. If an arbitrary world point $P_w$ is found in a world coordinate frame whose origin $O_w$ is explicitly defined in some world location, the translation and rotation into some camera coordinate system can be formulated as   

\begin{equation}
P_c = \begin{bmatrix}
x_c \\
y_c \\
z_c\end{bmatrix} = R(P_w - t)
\end{equation}

where $x_c$, $y_c$, and $z_c$ are the coordinates of point $P_w$ transformed into a camera frame C. The homogenous coordinate frame transformation is formulated similarly

\begin{equation}
\tilde{P_c} = \begin{bmatrix}
\tilde{x_c} \\
\tilde{y_c} \\
\tilde{z_c} \\
1 \end{bmatrix} = \tilde{R}(\tilde{P_w} - \tilde{t})
\label{eq:homogenous_transformation}
\end{equation}

where R is a $4x4$ matrix concatenating multiple Euler angle rotations. Since rotations in Euler angle system are applied to the rotating coordinate frame, the order of the axis rotations is crucial unless the matrices are commutative. Commutativity is not a common property in transformation matrices, thus, the order of applying of rotation matrices changes the orientation outcome accordingly. The combinations of Euler angle rotations are presented in detail in Introduction to Robotics by Craig in Appendix B \emph{The 24 angle-set conventions} \citep{Craig05}. 

%If only the extrinsic parameters of a multiple camera system are known and no intrinsic parameter knowledge is available, 3d reconstruction of the scene is possible with some limitations. In case of a two camera system, the scene 3d geometry can be recovered only up to a scale, 
%at least 5 cameras must be used to reconstruct a scene  
    %If the intrinsic parameters are known, then a metric reconstruction may be obtained using a set of calibration points. The very minimum amount of points in a two viewpoint stereo setup is 5 points, but usually stereo calibration objects, such as calibration grids, contain more points. There is a risk  of acquiring multiple solutions using 5 points (the very minimum).
    %TODO Tähän metrisiä rekonstruktio tekniikoita niin perkeleesti

\subsection{Coordinate Frames}
\label{subsection:coordinate_frames}

The scene that the camera sensors are viewing is called \emph{world frame} (or \emph{scene} interchangeably), which uses a standard (x,y,z) right-hand 3d coordinate convention. The origin of the world coordinate system is $O_w$ located at a corner of the overhead crane hall floor, for example. In the works, the origin of the world frame was coincident with the \emph{camera frame} (or \emph{sensor frame}) origin $O_c$ since the load object measurement computation only requires relative coordinates to function properly. Relative coordinates can be used as long as metricity of the scene geometry is guaranteed, which means that the translation of the camera frame to the world frame need not be precisely correct. Thus, the world frame origin and the camera frame origin both are located at the camera sensor array center. Third frame, the \emph{image frame}, is needed to convert between metric camera frame locations and the pixel array values of the image. 

The world frame depicts a three-dimensional system where a point lies in an Euclidian space $R^n$ and its span is $R^3$. The world coordinate frame follows the framework used in a report by Terho in 2010 \cite{Terho10}. A precise transformation from the world frame to the camera frame requires position and orientation information of the camera, which can only be estimated to a certain accuracy. Two options could be used for positioning: GPS signal or crane end effector position services. GPS signal is not very good for an indoor application with the accuracy of 1-10 meters, and as such the crane positioning services should be used instead. The orientation of the camera can be acquired using an inertial measurement unit (IMU) installed in the camera platform. In this thesis, the world frame transformation was not solved since it was needed. The transformation $T_{c}^{w}$ from camera frame to the world frame would be needed if the software is integrated with other software and a common coordinate frame is required.

The camera sensor array defines the first two orthogonal dimensions of the camera coordinate frame: x-y- axis plane is parallel to the camera image array, and the positive Z-axis points out from the image plane perpendicularly towards the lens system of the camera. The camera frame positive Z-axis is the optical axis of the camera by definition. The origin $O_c$ of the camera frame is located in the focal point of the camera, that is the point through which all the light rays reflected through the lens system go.

The image frame is the 2d projection plane where the three-dimensional point of the imaged scene is transformed with perspective transformation and pixelation process. The plane of the image frame is fully coincident with the camera frame image. The origin of the image frame is in the upper left corner of the sensor array, thus, having only non-negative integer values that map pixel intensity values to the pixel grid of the sensor array.

\section{Epipolar Geometry}
\label{section:epipolar_geometry}
%+ reference to the optical centers C and C', and baseline of the camera setup

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{epipolar_geometry.pdf}
    \caption{Epipolar geometry (central projection). The image planes are non-parallel so that the epipoles $e_1$ and $e_2$ stay inside the projective image area. In a parallel configuration, the epipoles are ideally located at infinity. Adapted from \citep{Corke11}.}
    \label{fig:epipolar_geometry}
  \end{center}
\end{figure}

Epipolar geometry is a two-view camera geometry where the corresponding image point locations are restricted by an epipolar plane. In figure \ref{fig:epipolar_geometry} the epipolar plane is the plane spanned by both camera focal points (camera frame origins $O_{c_{1}}$ and $O_{c_{2}}$) and an arbitrary world point P. The epipolar plane $PO_{c_{1}}O_{c_{2}}$ intersects the image planes $\pi_L$ and $\pi_R$ at epipolar lines (see red lines in figure \ref{fig:epipolar_geometry}). A single point P is projected as points $p1$ and $p_2$ in two different camera frames. These projected points are called conjugate points that are found for all points P visible in both cameras. The most important feature of the epipolar geometry is that the conjugates of a point P in the camera frame must lie on corresponding epipolar lines, a fact which reduces the correspondence search space from 2d to 1d in block matching processing (see the correspondence problem in section \ref{section:stereo_correspondence_problem}).

In figure \ref{fig:epipolar_geometry} the epipoles $e_1$ and $e_2$ are located along the epipolar line, marked in red dots. An epipole is always found on the stereo camera pair baseline (see Figure \ref{fig:simple_stereo_geometry}), thus, by definition the epipolar line and the camera baseline intersect at an epipole. In a parallel stereo camera configuration the epipoles lie ideally at infinity, because the image plane is parallel to the stereo baseline, and the lines never intersect. In reality, the epipoles may be found at finite distance outside the image projection plane, but for simplicity of describing the concept of the epipolar geometry the figure \ref{fig:epipolar_geometry} shows a a verged (toed-in) stereo setup where the camera baseline and the epipolar lines in the image planes intersect inside the imaging area. If the epipole is formed inside the image plane, then the other camera optical center, thus the camera, must be visible in the picture.

A stereo camera setup where the camera optical axes are in parallel is called a canonical configuration. In the canonical configuration the epipoles move to infinity, and the effect known as disparity can be seen on the two resulting images. Disparity means the shifting property of image features taken with a canonical stereo pair setup. For true disparity effect the camera must only shift along a single coordinate axis of a camera frame. Since the cameras project the world in a perspective projection on the image plane, the disparity effect follows the rules of perspective projection, for example, the object further away from the camera optical center shift less in pixels than objects in the near field of the camera. Any stereo configuration with non-parallel optical axes can be transformed into canonical one by image rectification processing as long as the camera images overlap. 

A precise relationship between a conjugate pair $p_1$ and $p_2$ is expressed using $3 x 3$ fundamental matrix $F$ 

\begin{equation}
\tilde{p_1}^T F \tilde{p_2} = 0
\label{eq:fundamental_matrix}
\end{equation}

where $\tilde{p_1}$ and $\tilde{p_2}$ are the projected (conjugate) points expressed in homogenous coordinates. The line equations for the epipolar lines are solved using the fundamental matrix so that epipolar line $l_2$ is a function of point $\tilde{p1}$ 

\begin{equation}
\tilde{l_2} = F \tilde{p_1}
\label{eq:epipolar_line_1}
\end{equation}

and accordingly 

\begin{equation}
\tilde{l_1} = F^T \tilde{p_2}
\label{eq:epipolar_line_2}
\end{equation}

A lot of different techniques have been discovered by different researchers of how to recover the fundamental matrix $F$. If the camera intrinsic parameters are known, then the camera calibration matrix $K$ may be used to recover $F$. If no calibration is available, then $F$ can be recovered using equation \ref{eq;fundamental_matrix} with a calibration technique, such as the normalised 8-point algorithm, or other methods\citep{Salvi01}. Since $F$ is a $3 x 3$ matrix that is defined only up to a scale, it must have 8 degrees of freedom. Thus, the 8-point algorithm will provide a minimum of 8 linear equations that will solve the coefficients of the matrix. Other methods of estimating the fundamental matrix include the 7-point algorithm and random sample consensus based parameter estimators, such as RANSAC, MSAC, and PROSAC\citep{Carro12}.

\section{Stereo Camera Calibration}
\label{section:stereo_camera_calibration}

The stereo camera pair calibration computes the rotation and translation between a stereo camera pair. This is also known as the extrinsic calibration of the stereo pair cameras. The calibration results in a trans-rotational matrix that describes the origin and direction of the optical axis of the camera in the other camera's sensor coordinate system.\par
A known calibration for each single camera is required if metricity data needs to be produced using the stereo pair calibration. This section will focus on stereo calibration where the intrinsic parameters of each camera are known. The stereo calibration with these parameters can be done most simply using a stereo image pair and the 8-point algorithm that solves the correspondence problem between two different viewpoints. 8-point algorithm computation from two images is simple and straightforward, but in order to obtain better results, many more linear correspondence equations must be added to the problem.\par

The OpenCV stereo calibration uses the same implementation used by the MATLAB Camera Calibration Toolbox by Bouquet. Bouquet's implementation mixes multiple techniques from other camera calibration original research including papers from Zhang, Heikkilä, Silven, and Tsai \ref{BouquetRef}. Basically, the technique used estimates the fundamental and essential matrices between two stereo images who contain calibration chessboard patterns. The calibration is based on the knowledge of chessboard crossing locations in high accuracy.\par



The convergence distance of the stereo system optical axes affects the perceived location of the disparity range. The convergence distance can be computer controlled \ref{Chen et al. 2010 from Kyto14}, but in this work the convergence distance is not approximated accurately since the collected stereo video is not meant to be recorded for human vision system viewing. Increasing the convergence depth will also increase possibility for frame violations for negative disparity values near the edges of the screen. A frame violation would break the 3d perception illusion for a viewer, but for a computer system a violation will result in only an erroneous depth match that is eliminated from the final disparity image.
%what is negative disparity? => in front of the screen => does this matter in a measurement system

+ toed-in or parallel camera setup

The stereo cameras can be physically rotated towards each other, or they can be fixed in parallel configuration so that the optical axes intersect at infinity.

A parallel optical axes alignment is preferred to avoid vertical disparity caused by keystone distortion correction. The toed-in camera setup also induces depth plane curvature, which is an unwanted warping phenomenon of the image.\ref{Kyto14}    

+ convergence distance of the stereo system optical axes: approx. 264cm





























No additional domain-specific information is needed since the 

Interesting problems
+ Feature observability in an image
+ Perspective projection challenges

Bottom-up reconstruction
    + from multiple image construct range images

Top-down recognition
    + CAD model recognition especially

Space Carving
    + volumetric representation


+ object surface parameters
    + reflectivity e.g.
    
    
+different approaches: top-down (model-based) or bottom-up (reconstruction)


The depth of a measured feature can be computed if the geometry of the stereo setup and the intrinsic camera parameters are known. The depth measurement value $Z_m$ is calculated using equation \ref{equation:disparity_eq},

\begin{equation}
\label{equation:disparity_eq}
\mathsf{Z_m} = \frac{\mathsf{bf}}{\mathsf{d_x}}
\end{equation}

where \emph{b} is the baseline width between the principal points of the camera image planes, f is the focal length for both cameras, and $d_x$ is the measured horizontal disparity value in a standard parallel stereo camera configuration. Now, if \ref{equation:disparity_eq} is differentiated with respect to horizontal disparity $d_x$, an equation for the effect of disparity change in the output measurement is achieved. 

\begin{equation}
\label{equation:disparity_derivative}
\frac{dZ_m}{dd_x} = \frac{-bf}{d_x^²}
\end{equation}

Furthermore, we may insert disparity value as $d_x = X_L - X_R$






Following the constraints that the epipolar plane induces to the stereo matching problem a fundamental matrix is introduced. The co-planar constraint 

\begin{equation}
\mathbf{X}_{L}^T (\mathbf{t} \times \mathbf{X'}_{L}) = 0
\end{equation}

is transformed into 

\begin{equation}
\nathbf{u}^T (K^{-1})^T S(t) R^{-1} (K')^{-1} \mathbf{u'} = 0
\end

An interested reader can find the full formulation of the fundamental matrix in Sonka et al. book \cite{Sonka07}. The fundamental matrix F is according to the bilinear relation introduced by Longuet and Higgins

\begin{equation}
\mathbf{u}^T F \mathbf{u'} = 0
\end{equation}

where the fundamental matrix F is 

\begin{equation}
F = (K^{-1})^T S(\mathbf{t}) R^{-1} (K')^{-1}
\end{equation}

The fundamental matrix captures all the available information from a pair of views between two different images.
The relative motion of the camera scheme is present in the used stereo camera setup, or namely two cameras with known caliberation in space. As we know the camera calibration matrices the normalized measurements can be described with a similar bilinear relation that includes the essential matrix $\mathbf{E}$. E can be estimated from the image measurements and it has a rank of 2 and it can be decomposed with a singular value decomposition (SVD). A normalization must be done in order to solve the epipolar geometry problem so that the solution is numerically stable \cite{Sonka07}.'
%Butterworth 97 also 

The fact that the essential matrix is a $3x3$ matrix with a rank of 2 means the projection of point X onto the image plane is a lossy transformation. 

Solving the overdetermined linear equation system in epipolar geometry requires a minimum of 6 image points. Noise affects the solution, and real image measurements use an algorithm called eight-point algorithm or a modification of it. The solution is minimised with least squares (Frobenius norm) effort. 
+ at least 8 points for a linear solution in 2 image system \cite{Sonka07}
+ at least 7 points for a linear solution in 3 image system (trilinear tensors used)

+ Mismatch errors 

+ 3D similarity reconstruction
    + X is found up to a scale and not Euclidean
    
+ case of unknown intrinsic and extrinsic parameters is not shown here
    + can do, depth from unknown video source

+ stereopsis => scene reconstruction in Euclidean reconstruction from 2 calibrated cameras

Using only two cameras instead of multiple cameras helps to avoid occlusion of objects and use existing powerful stereo processing tools.

+ 3 camera-setup can understand all available information in an orthographic projection => 4th camera does not add any more information
%page 473 Sonka07


\section{Metricity Requirements}
\label{section:metricity_requirements}

An easy method of calibrating a reconstructed 3d scene is to know the relative positions of 5 or more points in the environment. The metric coordinates of the 5 beacons must be known so that the projectively reconstructed world points $S_b$ may be transformed from projective model to a metric model. The metric model can be further transformed into an Euclidean 3d model with some additional constraints, e.g. orthogonality and parallelism constraints on spatial structures. For example bundle adjustment techniques are suited well for estimating the correct transform that upgrades the projected model to a metric or Euclidean model via global minimisation of the reprojection error \ref{Pollefeys04}. Global minimisation techniques may not find the global minimum easily without heuristics, and the field of global minimum search is a vast research area in its own, thus, it will not be elaborated more.

\subsection{Stereo Assumptions}
\label{subsection:stereo_assumptions}

Some basic assumptions about the measured crane working area must be made in order to use stereo correspondence algorithms. These assumptions operate on the collected images from the two stereo cameras. 

In general, the assumptions hold for any stereo camera setup that work on disparity principle unless otherwise stated. 

A single pixel must correspond to a single surface point in the world in order to construct a linear mapping function from the left stereo camera image to the right camera image. This restriction means that no opaque, or see-through, materials can be accurately measured with the used technology. Additionally, occluded and self-occluded surface pixels will be discarded in order to comply with the single pixel correspondence assumption.

+ A single pixel corresponds to a single surface point (restriction: no opaque materials | fish + fish bowl)
+ Disparity values are generally continuos (smooth within a local neighbourhood | discontinuities occur only at object boundaries)
+ Ordering constraint (if an object A is to the left of the object B in the left image, then the object A will be left of the object B also in the right image | sometimes violated by pole-like objects)
+ Lambertian surfaces => surfaces do not change appearance when viewed from another angle.
    + opaque
    + ideal diffusion
    + reclefts light energy in all directions

Stereo vision produces a dense disparity map . (If we compare to laser scanner sensor, the denseness of the produced point cloud is much higher)
Some requirements for the disparity map are elaborated in Zitnick et al. report, namely the disparity map should be smooth and detailed . The desirable result would provide smooth continuos mapping that detects small surface elements as separable regions. It turns out that these requirements are opposing to each other: a smooth continuos disparit map tends to filter out small details, and a detail preserving mapping is affected by noise \cite{Zitnick00}.



\subsubsection{Calibration Targets}
\label{subsubsection:calibration_targets}

Planar calibration grids are the most common calibration method for a stereo camera setup. Mainly checkerboard patterns and rectangular grid patterns are used for calibration purposes.

\subsubsection{Calibration Errors}
\label{subsubsection:calibration_errors}
%In most cameras the scenes are projected on a planar image plane. For perspective projection cameras the borders of the image are less illuminated due to light refraction in the lens due the law of cosines. 
%lähde Kuvatekniikan perusteet


+ Focal length estimation errors
It was found out in study by Kytö that the focal length affects the depth threshold of a stereo setup more dominantly than the baseline separation of the cameras. %what is really depth threshold 

+ Tangential distortion
+ Radial distortion (barrel effect)
It is important to compensate for lens system radial distortions, because they cause vertical disparity on the edges of the image. \ref{Kyto14}

+ Chromatic aberration

+ Affine distortion (aspect ratio)

+ Camera electronics, light intensity changes affect the phase locked loop (PPL) in the electronics => line jitter with sync signal systematic or random noise change

+ Calibration target location



\section{Stereo Imaging Techniques}
\label{section:stereo_imaging_techniques}

One of the fundamental issues in computer vision is how to display a solid shape \cite{Sonka07}. 

Stereo imaging is divided into two different categories by the used algorithm. The first category is dense matching algorithms where stereo matching algorithms run on all pixels. The second category is sparse matching algorithms where only distinctive features will be matched unambiguosly \cite{Terho10}.



\subsection{Stereo Errors}
\label{subsection:stereo_errors}

+Embedded geometric and radiometric difficulties

In this work the machine vision system is used as means of extracting coordinate data from a pictured scene. Full recovery of the spatial 3d scene is not possible, because the stereo cameras only see the surface parts that project onto the 2d image plane.
In our case, a full recovery of a 3d object model is not needed to solve the problem of load positioning and dimensional measurement. As a result of depth mapping, a point cloud will provide coordinate data for the visible surface, which we can mask three-dimensionally and process further.


It can be seen from \ref{equation:disparity_limit} that the accuracy of such a stereo system is linearly proportional to the disparity measurement error, and there are trade-offs for most factors, such as focal length, camera separation, optical axes convergence, and aperture selection.

The lenght of the camera separation in a stereo camera system, also called the baseline width, affects the triangulation accuracy of the depth measurement setup to some extent. A common width for comparison is the human eye baseline width of X
% camera separation or baseline
+ hyperstereo = baseline much higher then the human vision system baseline
%=> distorts perception of motion for humans
%=> distorts perception of glossiness of materials
%=> also crosstalk = image leaks to the area of the other eye => visible ghosting in stereo perception


According to equation \ref{}


In standard stereo processing the object measurement error increases with items that are located far away. If a single baseline and resolution is used the error grows quadratically with increasing depth \cite{Gallup08}.

A simple equation for the depth error is introduced in \cite{Gallup08}:

\begin{equation}
\epsilon_z = \frac{z^2}{bf}\dot \epsilon_d
\end{equation}

where \emph{z} is the depth value, \emph{b} is the baseline value, \emph{f} is the focal length, and \epsilon_d is the disparity matching error (in pixels). The resulting depth error $\epsilon_z$ 


\subsubsection{Occlusion}
\label{subsubsection:occlusion}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{occlusion_pollefeys.pdf}
    \caption{An occlusion front is seen in scenes with depth differences and planar features. Image from \citep{Pollefeys04}.}
    \label{fig:occlusion_pollefeys}
  \end{center}
\end{figure}

Occlusion means partial or full loss of line of sight of an object in an image. In stereo matching, an object may be partially occluded by another object, which means that a surface that is visible in the left camera image is not visible in the right camera image. The correspondence search should not return a corresponding point for an occluded surface, but in real machine vision systems occlusion tends to generate erroneous correspondence matches. An object may be occluded in only left or right stereo image, or partially occluded in both. In both cases, the other image has visible object surface that cannot be matched in the other image.

In figure \ref{fig:occlusion_pollefeys} an example of an occluded part of wall is shown on the left side. The right side depicts a correspondence search space along epipolar lines with an ideal correspondence correlation path for the rectified occluded scene. It can easily be seen from figure \ref{fig:occlusion_pollefeys} that when on occlusion is found in the scene, the correct corresponding image point is found further away from the optimal path. Furthermore, the correspondence search space is restricted along the epipolar lines with certain disparity difference limit (bandwidth) because the depth range of the imaged features is limited\citep{Pollefeys04}. 

Most solutions do not explicitly detect occluded regions, thus errors may be found in the disparity output of block matching if bidirectional matching is not enforced. Bidirectional matching searches corresponding surface points from both images, thus, occluded regions may be identified if there is certain surface areas are detected as missing. Also, an occluding boundary is found on the perimeter of objects, for example, a cylinder surface will curve until the other camera view becomes occluded. Thus, on the edge of objects that curve out of view there is always some occlusion in the scene. Occlusion induces errors in disparity map estimation, which can be improved if occlusion detection systems are used in the block matching process. Occlusion detection is out of scope of this thesis, but basically the occlusion can be detected using energy functions that optimise the correspondence search path, for example Kolmogorov et al. describe such a technique that uses graph cuts\citep{Kolmogorov01, Woo-Seok11}. 

\section{Stereo Correspondence Problem}
\label{section:stereo_correspondence_problem}

The correspondence problem is a general multiple camera system problem regarding whether multiple images contain a same 2d feature or not. A feature is an understanding of some feature that has a visible shape response in its 2d projection on the image according to natural image formation in a camera. A simplification of the shape may be a result of image segmentation using histogram or color based thresholding, k-means clustering, a water-filling algorithm, region growing algorithm or some other segmentation method.

On one hand, in figures \ref{fig:harris_features_left} and \ref{fig:harris_features_right} it is easily seen that the same lifted box is seen from different viewpoints, but on the other hand, the machine vision system does not understand this unless some feature descriptor is formulated that returns a similar response for both images of the lifted box.

A feature descriptor measures the shape of a segmented image region using invariant moments of the image, or other constant shape descriptors. A good feature descriptor response is invariant to changes in scale, orientation, and viewpoint, and it should tolerate changes in the camera system used for imaging, such as white balance, exposure, and illumination changes (specular reflections, shadows etc.).\citep{Corke11}

Image feature detection is a large field on its own, which is why only a few general feature descriptos are introduced that can be used to solve the correspondence problem. Some full-fledged feature descriptor techniques include scale-invariant feature transform (SIFT), speeded up robust features detector (SURF), and histogram of oriented gradients (HOG). Also blob detectors, such as the maximally stable extremal regions (MSER) could be used to solve a general correspondence problem. The Harris corner detector (see Figure \ref{fig:harris_features_left}), and Shi and Tomasi corner detector are useful for tracking the corners of a lifted load objects as part of the measurement software. Moreover, the features returned by a corner detector could be used as control points in the point cloud verification process introduced in section \ref{section:data_quality_in_the_machine_vision_process}.

\begin{figure}
\centering
\begin{subfigure}
    \centering
    \includegraphics[width=\linewidth]{harris_feature_detector_left.jpg}
    \caption{Left camera Harris corner detector result.}
    \label{fig:harris_features_left}
\end{subfigure}%
\begin{subfigure}
    \centering
    \includegraphics[width=\linewidth]{harris_feature_detector_right.jpg}
    \caption{Right camera Harris corner detector result.}
    \label{fig:harris_features_right}
\end{subfigure}
\label{fig:harris_features}
\end{figure}

In the implementation of the prototype software the correspondence problem is solved using the epipolar geometry property that enables a 1-dimensional correspondence search space using image rectification. If the geometry of the stereo setup is unambiguosly known, the extrinsic calibration knowledge makes image rectification possible. 


(UNEDITED)It is important to choose a proper window size for stereo matching. In low contrast regions of the image too small a window cannot guarantee a unique match because of too little intensity variation. 
In general, a small window is desirable to avoid unnecessary smoothing, but optimal window size depends on the region intensity variation, texture, and disparity \cite{Zitnick00}.
The problems of window size can be solved with iterative window size methods, but increase in computation overhead or problems at occlusion boundary are still left unsolved \cite{Zitnick00}..

% correspondence problem
+ the ambiguity of the correspondence problem can be reduced using one or multiple constraints such as
disparity limit constraint (disparity must be < limit, e.g. finger in front of the face cannot be imagined in stereo)
disparity smoothness (disparity changes only a little in any direction)
epipolar constraint (search space for matching pixels 1D on the epipolar line)
figural disparity constraint (corresponding elements should lie on an edge element in both images)
feature compatibility (physical origin of the matched points should be the same)
geometric similarity constraint (geometric features differ only a little)
mutual correspondence constraint (occluded points are not found thus ruled out)
ordering constraints (typically points lie on the same epipolar line for similar depth items, exception narrow close-up sticks)
photometric compatibility (little intensity differences)
uniqueness constraint (1 pixel can only correspond to 1 pixel in the 2nd image, exception two or more points on 1 ray in 1 image)

+ specularity edges cannot be used for feature matching because specular lighting changes depending on the viewpoint \cite{Sonka07}

\subsection{Image Rectification}

Image rectification is the process of virtual optical axis parallelisation done by software. Rectification transforms the image planes so that the same features seen in both stereo images are perceived on the same epipolar lines. The epipolar geometry in chapter \ref{subsection:epipolar_geometry} can explain how the rectification is done, but the interesting result is that the epipolar constraint makes a 1-dimensional correspondence search space possible. After a successful image rectification, the corresponding feature points are found on the same vertical lines, and the correspondence search space is reduced to 1-d search space instead of heavier 2-d search space. This property of the epipolar geometry reduces computation time a lot for the correspondence search implementation.








\subsection{Block Matching}
\label{subsection:block_matching}

\section{Depth From Disparity Map}
\label{section:depth_from_disparity_map}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{disparity.jpg}
    \caption{A disparity image. Near field objects are bright, and far field objects are darker. The image has only one intensity channel with 256 different brightness levels.}
    \label{fig:disparity}
  \end{cente}
\end{figure}

After the image rectification is done for a stereo image pair captured by the calibrated stereo camera setup, then the 3d point cloud generation becomes a problem of extracting the estimate of the disparity map (d(x,y).

Q is the reprojection matrix that encodes information about the stereo camera setup. Q matrix will provide information about camera optical axis convergence (parallel or cross-eyed), image plane principal points, and the stereo setup baseline width b.

\chapter{Implementation}
\label{chapter:implementation}
%test overhead crane dimensions: width 40 meters, length 10.7 meters, height 5.5 meters

The software implementation for the thesis work was prepared with the FAMOUS project team in Generic Intelligent Machines (GIM) research group. The implemented software includes many software nodes, mainly the stand-alone load object measurement tool that processes point cloud data into load object bounding volume information. Additionally, the point cloud data publisher node was implemented that integrates the HIMMELI camera platform hardware as the sensor input platform for the load object measurement tool.\par
The programming work done for the thesis was carried out from March 2013 to February 2014. In addition to standard C and C++ libraries, external libraries specialized in image processing and point cloud processing were used. OpenCV library by WillowGarage Inc. was used for image processing and stereo rectification purposes, Point Cloud Library (PCL) was used for all data intensive point cloud processing, and Boost library was used for all file system operations. All the data networking, runtime configuration, and other online networking solutions were implemented using the Robot Operating System (ROS).  \par
The design of the software was finished in December 2013 after successful compatibility testing between library functionalities and proofing of concepts with offline data sets. The final version of the load object measurement system that supports online perception sensors in the ROS network was finished in May 2014.\par
During the implementation period, new versions of most of the libraries became available, but the implementation uses latest currently stable versions: ROS Groovy and PCL 1.6 (compatible with ROS Groovy). If the software would be upgraded, then a newer version of ROS (Hydro) can only be used with the next version of PCL (1.7). PCL 1.7 does simplify some things, for example PCL visualization camera controls, and brings some extended features that could benefit the load object measurement system (for example, the new moment of inertia and eccentricity based descriptors functionality). Other than that, the old versions have all the functionality that the system needs, and they are successfully used to run the software at the time of writing this.

\section{Overall System Design}
\label{section:overall_system_design}

\subsection{Software Architecture}
\label{subsection:software_architecture}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{software_architecture.pdf}
    \caption{Software architecture for the load object measurement workflow using robot operating system ROS.}
    \label{fig:software_architecture}
  \end{cente}
\end{figure}


\subsection{Use Cases}
\label{subsection:use_cases}

The software component design in this thesis was planned modular so that different parts of the measurement, e.g. stereo rectification and object segmentation, can be developed independently. The full system design will consist of many sequential components, some of which will be designed and implemented by other researchers in the FAMOUS research project.

\subsubsection{Use Case Configurations}
\label{subsubsection:use_case_configurations}


Four different use case configurations (UCC) are considered for the evaluation of the machine vision system implemented. Different use case configurations are titled with a paired numbering scheme, e.g. 1-1 or 2-1. The use case configurations will describe the overview of installed hardware, used parameters, and output data. Additionally, each UCC has very different advantages and drawbacks, which will be discussed in brief detail.

The first use case configuration UCC 1-1 had a process crane setup where the stereo camera rig is attached to the crane trolley with cameras facing down. The crane trolley can move freely with cameras attached, and the camera image will show the crane load approximately in the same location at all times in the image center. Hoisting the load up will not decenter the crane load showing in the output image. Instead, the crane load will be shown enlarged.

The second use case configuration UCC 1-2 had a process crane setup where the stereo camera rig is attached to the extreme end of the crane bridge beam on the left. The cameras are facing the load when the trolley is located near the left end of the crane bridge and the load is hoisted half way. Hoisting the load up will show the load moving towards the upper part of the output image. Hoisting movement does not enlarge the load,  instead driving the crane bridge trolley near the cameras will enlarge the load image in the output image.


The third use case configuration UCC 2-1 had a small telescopic crane installed on a forestry machine with a log lifting tool. The cameras are attached on the right side of the forestry machine chassis. The cameras are facing the load when the load is hoisted off of the ground behind the machine. The working area of the telescopic crane is occluded on the left side of the telescopic boom. Thus, the crane load will be occluded when the boom is actuated to the leftmost visible working area. 

In this use case the crane load shape is considered always cylindrical.

The fourth use case configuration UCC 2-2 had a small telescopic crane installed on a forestry machine with a log lifting tool. The cameras were attached on top of the operator cabin slightly to the right side of the telescopic crane boom . The cameras are facing the crane working area, and the crane boom is mostly visible in the output image. The log lifting tool at the end of the boom is mostly visible. The crane load is mostly visible, but partly occluded by the telescopic boom. 

In this use case the crane load shape is considered always cylindrical.

\subsection{Prototype Software}
\label{subsection:prototype_software}

\subsubsection{End Effector Tracker Integration}
\label{subsubsection:end_effector_tracker_integration}

Machine learning techniques can be used in tracking objects from 2d images. A common machine learning technique for facial detection in images is Haar training that can be utilised for other purposes, too, for example end effector detection. Haar training was originally introduced by Viola et al., and improved by other researchers with additional ... features. Haar training uses boosting algorithms, which are adaptive parameter learning algorithms. A basic cascade classifier cannot detect scaling of images, but a cascade of classifiers can be constructed that detects features scale-invariably. A cascade of classifiers is a decision tree structure which is constructed out of simple classifiers called stage classifiers. The cascade of classifiers results in a positive detection of a feature if the test image can pass all the stages of the cascade. Accordingly, we may estimate the probability for a false alarm rate given the probability of a single weak classifier detection and the number of stages \ref{Lienhart03}.

Object recognition works on a classification principle: a classifier detects attributes in an object and works out into which object class it belongs to.
+ machine learning
    + neural nets
    + genetic algorithms
    + fuzzy systems

%machine learning
A classifier is a concept of machine learning that is utilized in Haar training. A single classifier itself is a weak member of a more powerful committee of cascaded classifiers \cite{Freund96}. 
A number of simple, computationally light classifiers seem to outperform strong classifiers such as neural networks in principle \cite{Lienhart03}. 

In 2002, Lienhart and Maydt \cite{Lienhart02} proposed improvements upon the original work of the authors of Haar training classifiers. The Intel labs research team added a rotated feature classifier, which improves upon the performance of the original simple feature classifiers. One year later, they analysed different boosting algorithms in Haar training and concluded that Gentle Adaboost method performed the best in the training. 

The feature search space in Haar training is large in an image. The computational complexity depends on the image resolution, and on the chosen prototype of a feature.  For example, the number of features for a small window size of 24x24 is reported to total up to 117,941 features\cite{Lienhart03}. 

\section{Perception Sensor Platform \emph{Himmeli}}
\label{section:perception_sensor_platform_himmeli}
%HIMMELI sensor platform provides means for creating digital models of the environment with stereo cameras and a LIDAR laser scanner. The environment modeling accuracy depends on the distance of the target area to the stereo cameras, and consequently the area of the modeled environment depends on the distance to the cameras. The laser scanner can model larger areas with higher accuracy.
%Histogram equalization maps the current image intensity response over the full available intensity range. 

Himmeli sensor platform is a modular sensor rig that was engineered in Aalto university department of automation and systems technology in a research project in 2012. The sensor platform was created for use in future automated machinery research and its purpose is to provide flexible sensing of environment with multiple sensors. Himmeli sensor platform was used as the perception platform for the implementation work done in this thesis. It can collect data from the crane environments using stereo cameras or laser scanner, and it can provide orientation change data using an inertial measurement unit \ref{Karhu13}.
%todo reference from Sami

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{himmeli_modules.jpg}
    \caption{Himmeli platform includes a sensor module, a computation module, and a support module with IR light source.}
    \label{fig:himmeli_modules}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{himmeli.pdf}
    \caption{Himmeli platform perception sensor schematic\ref{Terho10}.}
    \label{fig:himmeli_schematic}
  \end{center}
\end{figure}

The Himmeli platform includes three modules who are namely the sensor module, a computation module, and a support module with an IR light source seen in Figure \ref{fig:himmeli_modules}. On the front side of Himmeli, the sensor module includes a high resolution stereo camera pair, a single high resolution camera, a thermal imaging camera, an automotive ultrasound RADAR, and an inertial measurement unit (IMU). There is an optional Velodyne HDL-32E LIDAR sensor on top of the sensor module. The Velodyne HDL-32 can be used to scan the environment geometry with 32 micromirror controlled lasers. The schematic of all the sensors can be seen in Figure \ref{fig:himmeli_schematic}.

The automotive radar installed in Himmeli was not used to model the environment because of its low accuracy of 5 cm to 1 m depending on band used\ref{Ahtiainen12}. In general, radars are not often used in machine perception systems due to their low resolution output. They can operate in adverse weather, such as fog, so for some outdoor applications the radar might be useful. For example, if the load object measurement system was used to measure geometry of container cranes in a seaport terminal, the radar could be used for partial modeling of the load object under harsh weather circumstances.

\subsection{Stereo Camera Technical Details}
\label{subsection:stereo_camera_technical_details}

The Himmeli sensor module includes two high resolution GigE uEye UI-5120 RE HDR cameras with industrial grade casings. The uEye UI-5120 RE HDR camera is a high dynamic range camera with a CMOS (complementary metal oxide semiconductor) sensor array with a PAL resolution of 768 x 576 pixels seen in Figure \ref{fig:gige-camera}. The model in use (UI-5120RE-M-GL Rev.2) has a monochrome sensor that transmits intensity images at a maximum of 50 frames per second using Gigabit Ethernet data connection. 

If the 175 gram GigE uEye cameras are installed in a vibrating environment (e.g. a rotary crane boom) the camera's image output quality may suffer, because the optics do not include image stabilisers. Using the stereo pair installed in the Himmeli sensor module may improve the image quality in some environments due to passive damping properties of a larger mass of a sensor module body.

The cameras are installed with lens tube shieldings (class IP65/67) so they can operate even in outdoor winter conditions. The rest of the components of the camera comply to the splashproof and dustproof IP65/65 class so the camera and its casing are well-suited for industrial imaging purposes.

The camera CMOS mono sensor is manufactured by NIT and has a pixel size of 10 $\mu$m and an optical size of 5.75 mm x 7.68 mm. It has a colour depth of 12 bits, and the dynamic range of the camera is logarithmic with a 120dB sensitivity\ref{IDSImagingWeb13}.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=5cm]{camera-gige-ueye-se-cmos-1.jpg}
    \caption{The GigE uEye HDR camera with no protective casing. Image from \cite{IDSImagingWeb13}.}
    \label{fig:gige-camera}
  \end{center}
\end{figure}

\subsection{Installation}
\label{subsection:installation}

The Himmeli platform was installed in two different test environments as described in the use cases.

\subsubsection{Overhead Crane Installation}
\label{subsubsection:overhead_crane_installation}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{himmeli_installation_12.jpg}
   \caption{The Himmeli sensor module installation in the overhead crane trolley in a test session.}
    \label{fig:himmeli_installation_option_1}
 \end{center}
\end{figure}

In the overhead crane environment the Himmeli perception platform was installed modularly so that the sensor module was installed in the overhead crane, and the computation module was used at the operator balcony of the process hall. Since most overhead cranes have little available space to install anything, only the sensor module was installed in the overhead test crane. Two installation options were available for the sensor module: first option was to install the sensor module to a moving trolley, and the second option was to install the sensor module to the side of the bridge. 

In the first installation option the sensor module was attached directly to the moving crane trolley housing so that the camera units were facing down towards the crane working area in a top-down manner with a slight angle to the warehouse floor as seen in figure \ref{fig:himmeli_installation_option_1}. 
%In option 1 the laser scanner rotation axis points towards the floor, aligning the laser scan lines along the shorter length of the warehouse space (Y axis of the warehouse coordinate system).

In the second installation option the sensor module was attached to the side of the moving bridge using a custom made metal frame. In this installation option the camera units were facing towards the crane working area from side and above in a bird's eye view manner. 
%The laser scanner rotation axis points towards the opposite end of the moving bridge, aligning the laser scan lines along the longer length of the warehouse space (X axis of the warehouse coordinate system).

\subsubsection{Forestry Crane Installation}
\label{subsubsection:forestry_crane_installation}

In the forestry crane environment the installation of the Himmeli platform is even more constrained by spatial limitations. In an actual log picker truck the installation location options for the camera module would include the operator booth rooftop and the base of the rotary crane boom. In testing situation a ground installed rotary crane boom was used and the Himmeli platform was installed first near the base plate of the rotary crane boom, and then on the rooftop of the operator booth.

In the first installation option the cameras are facing the working area so that the crane boom is slightly visible when it reaches for logs from above. In the second installation option where the cameras are located on the rooftop of the operator booth the crane boom is fully visible. This is a somewhat occluded viewpoint since the crane boom can be blocking the view of the load object.

The mass of a GigE uEye camera is 0.175 kg, which means the camera is susceptible to vibrating environments, and the image quality may be affected if no active or passive damping is used in a vibrating crane environment. While overhead crane environments dampen the vibrations passively, the smaller forestry crane rotary booms do not have mass that would damp vibrations as well, which is why the selection of the install location is important. According to Pedro et al. it is not possible to directly install electronic sensors (e.g. encoders, resistive elements) on a log picker truck rotary actuators because they break due roughness of the surrounding environment\ref{Pedro09}. Similarly, a camera would break, or produce low quality images in a vibrating environment if attached to locations that are in direct mechanimal contact with the rotary actuators. Since the log picker truck operator's booth is isolated from the vibrating environment, and may even have active vibration damping, it could be beneficial to install any camera systems on the rooftop of the driver's booth in an actual log picker truck.

\subsection{Computation Module}
\label{subsection:computation_module}

The Himmeli platform computation module includes different components that control the power and the data flow of the perception platform. The integrated computers run on 230 volts AC power with an option for 24 volts DC operation. The PC that controls the camera software runs on Windows 7 operating system. There is also a Linux distribution PC that runs the sensor fusion software for the captured images. The computing module can be remotely operated via a wireless local area network. We do not consider the details of the Himmeli platform in more detail, except some issues that were encountered will be reported in the evaluation chapter \ref{chapter:evaluation}.

%\begin{itemize}
% item separation length in a list
%\setlength{\itemsep}{0pt}
%\item HDR stereo camera pair
%\item High resolution camera
%\item 3D LIDAR laser scanner \url{http://velodynelidar.com/lidar/hdlproducts/hdl32e.aspx}
%\item Infrared (IR) camera
%\item Automotive radar
%\end{itemize}
%\begin{table}
%\caption{Device and its connector type in HIMMELI sensor rig.}
%\label{table:connectortypes} %lable just after caption
%\begin{tabular}{|p{6cm}|p{5cm}|}
%\hline % The line on top of the table
%\textbf{Device} & \textbf{Connector Type} \\
%\hline
%High dynamic range camera & Gigabit Ethernet \\
%\hline
%High resolution camera & IEEE 1394b (FireWire) \\
%\hline
%Automotive radar & CAN bus \\
%\hline
%Thermal camera & CVBS (PAL) or serial bus \\
%\hline
%LIDAR laser scanner & LIDAR \\
%\hline
%GNSS receiver & Serial bus 1 \\ \hline
%\end{tabular} % for really simple tables, you can just use tabular
%\end{table} % table makes a floating object with a title

%\begin{figure}[ht]
%  \begin{center}
%    \includegraphics[width=\textwidth]{himmeli_modules.jpg}
%   \caption{HIMMELI platform modules: a PC module, a support frame module, and the sensor module.}
%    \label{fig:himmeli_modules}
% \end{center}
%\end{figure}

\section{Point Cloud Library}
\label{section:point_cloud_library}

Point Cloud Library, or shortly PCL, is a fully templated modern C++ library for 3d point cloud processing purposes. 
PCL uses optimizations such as Intel SSE with Eigen library backend, GPU CUDA processing, and parallel programming in order to maximize the computation efficiency on specific computation platforms.

\subsection{Limitations of Point Cloud Computing}
\label{subsection:limitations_of_point_cloud_computing}

Although PCL library is optimized for point cloud computing, the biggest limitation in point cloud processing currently is the slow processing speeds of some operations. The number of data entries per point cloud has a big effect on some operations, such as the K-nearest neighbour search.


One estimate for the processing speed of a single cloud into a bounding volume output can be done by looking into other implementations in other research teams' efforts. As all the computation for PCL processing is done using an Intel Core i5 3.2GHz CPU on a 64-bit system with 



\section{Robot Operating System}
\label{section:robot_operating_system}


%Additionally, different versions of ROS distributions are not cross-compatible in the same ROS network. That means we need to decide upon a single ROS version which we are using. This is also a problem, because it is easiest to run PCL on a ROS Hydro distibution, where the other parts of the software (e.g. the TLD tracker) runs on older ROS Fuerte distribution. The old software component code base may be difficult to upgrade, so quite probably we will use ROS Fuerte and run older versions of PCL on it.   

Robot Operating System, or shortly ROS, is an open source operating system-like framework targeted specifically for robotics application use. It was originally developed by Stanford AI laboratory in 2007 and the support is currently continued by Willow Garage Inc., who released the latest ROS Hydro release in September 2013. ROS is structured with a hierarchy of nodes, packages, stacks (legacy), and community-supported repositories, who provide the user with a large code base of readily available robotic applications. ROS system can also utilize a heterogenous computation network which is administered using a master-slave network setup.

ROS is mainly used in this thesis for its excellent capability of networked messaging through topics and subscribers. With a little effort we may use readily available data types for point cloud data networking, and interaction between popular operating system platforms, such as Windows and Unix environments. Data can be published via topics if we wish to disseminate it in the ROS network without receiving any feedback. In case we need feedback whether someone uses the data or not, ROS provides with a server-client scheme. 

ROS also includes a multitude of tools that can be used for data traffic verification.

For the networked data approach, ROS was considered as the number one framework choice in terms of cross-platform communications for the load object measurement system. Other options considered were an own implementation of the UDP communication protocol, some other real-time operating system, such as FreeRTOS, or an implementation with no networking capabilities at all. The other options were not as readily available for a full-fledged PC network in an industrial LAN setting as ROS was, thus, ROS was selected for use in the software development effort.

\subsection{ROS Naming Conventions}

The current naming convention is meant to protect from colliding topic, node, and parameter names. 

\subsection{Parameter Server}
\label{subsection:parameter_server}

ROS parameter server is a cross-platform shared dictionary service that enables parameter retrieval at runtime in a ROS network. It is one of the advantages of using ROS since it can be used for reliably configuring a multi-device network. The parameter server is implemented usig XMLRPC libraries in the ROS master node.

\begin{table}\centering
\ra{1.3}
\caption{The data types supported by the ROS parameter server XMLRPC library.}
\begin{tabular}{@{}lll@{}}\toprule
Java type & XML name tag & Description \\
\midrule
Integer & int & 32-bit signed integer, non-null \\
Boolean & boolean & 0 or 1, non-null \\
String & string & A string, non-null\\
Double & double & A 64-bit signed floating point number, non-null \\
java.util.Date & dateTime.iso8601 & A ISO860 timestamp with milliseconds and time zone information missing \\
java.util.List & array & An array of objects \\
java.util.Map & struct & Key-value pairs \\
byte[] & base64 & Base64-encoded byte array \\
\bottomrule
\end{tabular}
\label{table:parameterserver}
\end{table}
%\begin{tabular}{|p{7cm}|p{3cm}|}

Setting parameters in the ROS parameter server is done using an XML launch file when launching a ROS node. All the variables are evaluated before launching any nodes, and all information is uploaded to the parameter server before launching the nodes. We can force certain data types for the parameters if the data types are not unambiguos. Supported types in the launch file are \emph{str}, \emph{int}, \emph{double}, and \emph{bool}. Additionally, we may set parameters in child namespaces or fully upload the contents of a parameter file to the parameter server as text or binary.

\chapter{Computation}
\label{chapter:algorithms}

\section{Bounding Volume Computation}
\label{section:bounding_volume_computation}

+Results from the software:
+ Big standard box dimensions
+ Mean X: 1.67 meters
+ mean Y: 1.47 meters
+ mean Z: 1.06 meters
+ Real values:

% oriented bounding box tree => find minimum bounding box, cut with a plane against main axis, find more minimum bounding boxes

%Keywords: bounding volume, collision detection, urban simulation, AABB (axis-aligned bounding box)

Bounding volume is the main product we wish to generate out of all the computation done in the PCL library previously. The format of the information output from the underlying software depends on the user, but we can present some suggestions for standard solutions. 

Usually, a volume of an arbitrary object is presented as a linear combination of simple geometric objects. These prototypes of geometric  objects support some or all of the linear operations available, such as summation, subtraction, multiplication and division.

One of the simplest bounding volume outputs is an axis-aligned bounding box, or AABB, applied in 3 dimensional Euclidean space. AABB is the minimum perimeter along the directions of the axes that span the Euclidean space that fully contain the target object. AABB is straightforward to find by computing minimae and maximae along each axis and by spanning a convex polyhedron so that we select all minimum values in one corner of the polyhedral graph, and we select all maximum values for the opposing farthest possible corner of the polyhedron.

AABB computation can be easily optimized, but the biggest drawback of this technique is the large amount of empty space not occupied by the object that the AABB volume contains. In the special case of stick-like objects, or planar objects, whose orientation is maximally off-axis aligned, an AABB volume representation will show a volume that is occupied less than 10 \% by the actual object.

Another simple solution is an oriented bounding box. If we first specify a main longitudal axis for an object, and then calculate its orientation, we can iterate the smallest possible bounding volume that is oriented along the object axis. This will tremendously help to more accurately represent a volume of special shaped objects, such as cylinders and planar items.

We can find more advanced bounding volume techniques for even more accurate object volume representation, such as bounding volume shapes, bounding volume hierarchies (BVH), discrete oriented DOPs, k-DOPs and boxtrees. For example, BVH can be used to detect collisions, or object interference, and it is computed using raytracing and culling. 


\subsection{Bounding Volume From 3d Axis-aligned Bounding Box}
\label{subsection:bounding_volume_from_3d_axisaligned_bounding_box}

+ possibly previously unknown object who is suspended from the crane end effector tool. The form of the output information should be simple while still carefully thought in order to 


It is computationally expensive to update the AABB bounding box as the load orientation changes via rotation, and it is suggested that in order to fight excessive computation, the bounding box should be loosely defined so that with small angle changes there is no need for recomputation \cite{Ericson05}.

\subsection{Bounding Volume From 3d Oriented Bounding Box}
\label{subsection:bounding_volume_from_3d_oriented_bounding_box}

The 3d oriented bounding box (OBB) is similar to the axis-aligned bounding box (AABB) with one difference: the bounding volume fills the cubic shape of the OBB optimally so that the bounding volume is minimized. This means that the OBB first computes a main axis for the load object and then the object frame is rotated along the axis. Now, the bounding box for the load object is seen as rotated in the world frame along the object, and the bounding volume becomes invariant to load rotation in the world frame.\par
The PCL library versions 1.7 and higher support a version of the oriented bounding box computation using moment of inertia and eccentricity based descriptors. With these descriptors the eigen vectors of the point cloud object can be computed, and the bounding box can be oriented along the largest eigen vector in a right-hand normalised coordinate system. The OBB along the major eigen vector axis does not guarantee minimality of the bounding volume, but the method does make the bounding volume computation more invariant to changes in the load object orientation in the world coordinate space.\par
Unfortunately, the current version of ROS Groovy does not include the moment of inertia and eccentricity descriptors required to compute the OBB. Consequently, future versions of the software running on ROS Hydro with PCL 1.7 and higher can improve the bounding volume accuracy by implementing the oriented bounding box computation. The AABB bounding volume computation currently used should be replaced with the OBB computation if the reported bounding volume accuracy is increased.

\section{Point Cloud Library Operations}
\label{section:point_cloud_library_operations}

\subsection{Object Segmentation}
\label{subsection:object_segmentation}
+ Euclidean Cluster Extraction

% the used scale of the point cloud point entries does have an effect on the computation time
% a computation processed using meters is much faster than computation on the same point cloud represented in micrometers, a million times larger values

Object segmentation is the first operation done for the point cloud after initial preprocessing is finished. The current implementation uses k-means clustering that clusters N-dimensional data points into k different spatial clusters using the PCL Euclidean Cluster Extraction class. For three-dimensional point cloud data, clustering uses $N = 3$ and an Euclidean metric for measuring distance between clusters and data candidates. Following the dissertation by Rusu \citep{Rusu09} the k-means clustering search would be presented algorithmically as



\subsection{Point Cloud Filtering}
\label{subsubsection:point_cloud_filtering}

Point Cloud Library provides many readily available filtering products to be used for point cloud processing purposes. For example, filters that remove NaN values, voxel grid filtering, and outlier data point filtering are available online. Next, all the filters used in the pre-processing, and downsampling phases of the load area point cloud are introduced. The load area point cloud contains the depth data computed from the stereo cameras, which describes the nearby environment from a single viewpoint.

In pre-processing of the load area point cloud, a NaN removal filter, and a Inf value filter are used. The filters remove any values marked as infinity ($Inf$/$std::numeric_limits<float>::infinity()$), or not a number ($NaN$). The NaN removal is a PCL feature that was readily used, and the Inf removal was implemented in the work.

Next, the point cloud is downsampled using the voxel grid filtering scheme available in the PCL library. The voxel grid filter approximates the world with a voxel grid that is a grid of cubic 3d volumes representing whether a space is occupied or not. The accuracy of the voxel grid depends on the selected cube length. Consequently, the voxel grid filtering can be used to compute different fidelity levels for the environment representation (analogous to Gaussian image pyramids). For example, in the implemented software a value of 0.12 m was selected as the voxel cube side, which produces a point cloud presentation of the original point cloud downsampled to a 2.2\% size compared to the original point cloud size. The downsampled cloud that is significantly smaller than the original still contains the geometry data in decent detail. The subset of the downsampled point cloud that represents the load object is used to produce the actual load object measurement.

All data points that coincide in the same voxel grid voxel are downsampled into a single data point that is then output as the voxel grid cell result. Two different approaches are used in the approximation: the voxel cell can simply output its center coordinate as the result, or a more often used center of gravity (centroid) of the original point set can be used as the output result. In this work, the computationally more expensive centroid of the points approximation is used. The resulting downsampled point cloud will thereby consist of a set of centroids computed from the original point cloud that was divided in voxel grid cells.

\subsection{Ground Plane Estimation}
\label{subsubsection:ground_plane_estimation}

A parameter estimation technique was used to find the ground plane that is visible in the test data images. Parameter estimation is a technique for finding models in data sets. In this case, the parameter estimation tries to fit a model of a geometric primitive shape to a geometry data set. The geometric shape that is selected for the parameter model is in this thesis a plane or a cylinder model depending on the function. Shapes such as planes, cylinders, and spheres are usually used, but any model that can be presented implicitly using parameter model may be used. The existence of the shape in the data set is tested by finding inliers and outliers that determine whether the data set supports the model or not for some initial guess.

Classic parameter estimation techniques, such as least squares minimization, effectively optimize the fit of the selected parameter model using all points in the data set. In 1981, Fischler and Bolles presented a new parameterized model estimation technique called random sample consensus (RANSAC) that can improve upon classic techniques by detecting outlier points in the data, and by taking into account the gross errors they induce to the fitting algorithm \ref{Fischler81}.

While classic parameter estimation techniques incorrectly assume that most outliers are smoothed out in favor of good results, the RANSAC technique initializes the inlier point search with a small initial data set, and accumulates inliers when consistent data points are found. This way, not all data set points are used in the geometric fitting computation, and the solution is found easily even when gross errors are present in the data set. In the implemented test software, the RANSAC algorithm from PCL package is used, which finds the ground plane from point cloud data sets successfully for most cases. 

Originally, the RANSAC algorithm was used to solve the location determination problem (LDP), which states that for a set of control points whose 3d location is known, a location of the camera is computed when a sufficient set of control points are visible in the projected image taken from the unknown location. The LDP problem is actually the camera calibration problem, which was presented in chapter \ref{subsection:single_camera_calibration}. RANSAC can solve the external location of the camera according to landmarks seen in the image, but in this thesis RANSAC is used to find the ground planes from the geometric data sets. If the ground plane data points are located correctly and removed from the point clouds, then the remaining points are parts of visible objects in the scene. 

Some variants of the RANSAC algorithm are 
\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item M-estimator Sample Consensus (MSAC)
\item Maximum Likelihood Estimator Sample Consensus (MLESAC) 
\item Progressive Sample Consensus (PROSAC)
\end{itemize}

but there are many more depending on the estimator technique used. When RANSAC algorithm produces poor results in parameter estimation application, other variants can be used if they are more suited for the data set. Many options can be found in the literature. \ref{Torr00} \ref{Huber81}

While most of the time the RANSAC algorithm finds the ground plane correctly, there are some conditions that must be fulfilled in order to find a correct solution. First, the ground plane must contain the largest number of points in a planar fashion. Otherwise, some other plane will be found by the algorithm, such as a large warehouse wall.

\subsection{Cylinder Fitting}
\label{subsection:cylinder_fitting}
%+ a great cylinder fitting scheme can be found in Pekka Forsman doctoral thesis

\section{Algorithms}
\label{section:algorithms}

\subsection{Load Selection Algorithm}
\label{subsection:load_selection_algorithm}

\subsection{Segmentation Center Selection Algorithm}
\label{subsection:segmentation_center_selection_algorithm}

\subsection{Cylinder Growing Algorithm}
\label{subsection:cylinder_growing_algorithm}

The Point Cloud Library cylinder fitting process outputs a parameter set pcl::ModelCoefficients that contains 7 parameters: 6 of them cover the coordinates of two random points on the main cylinder axis, and the final parameter reports the radius of the cylinder. Thus, the starting point and the ending point of the cylinder are not computed at all by the RANSAC computation. To solve the issue of missing cylinder length, an algorithm was written that solves the cylinder length. The algorithm projects points on the main cylinder axis using standard dot product computation, and grows the cylinder length until all the points in the cylinder data are iterated through. The working of the algorithm is as follows:

\begin{algorithm}[H]
 \KwData{Load Object Cylinder Segmented Data From RANSAC}
 \KwResult{Fitter cylinder end point coordinates}
 compute N units long vector along the main cylinder axis so that all data points project on it\;
 compute closest data point to centroid of data\;
 initialise a start point at the closest data point from centroid projected on the main cylinder axis\;
 initialise two subcylinders who represent distance along the cylinder main axis starting from start point to the end points in two opposing directions\;
 \While{data points left}{
  iterate to next point cloud data entry\;
  compute projection on cylinder main axis using dot product\;
  \eIF{projection is in negative coefficient vector direction}{
       eIF{check if distance to start point greater than current subcylinder 1 height value}{
        set projection point as the new end coordinate for subcylinder 1\;
        }{
        continue\;
        }
   }{
       eIF{check if distance to start point greater than current subcylinder 2 height value}{
        set projection point as the new end coordinate for subcylinder 2\;
        }{
        continue\;
        }
   } 
} 
return subcylinder 1 top coordinate and subcylinder 2 top coordinate\;
\caption{Cylinder growing algorithm}
\end{algorithm}

\chapter{Evaluation}
\label{chapter:evaluation}
%Pohdintaa ja future workia

\section{Software Overall Design Evaluation}

The design of the load object measurement system evolved a lot during its implementation phase. The original design was a non-linear, component-based design running on a C++ code base. The early software schematic is presented in Figure \ref{figure:original_softwaredesign}. 

%figure:original_softwaredesign

Originally, a network of atomic processing components was designed where data would be sent to all the required processing stages using ROS topics and subscribers. Since the amount of data generated in the stereo cameras was quite large, the design had to be changed to a more linear design where the bandwidth of the ROS networked messages per stereo pair was minimised. The final design included more logical, larger processing components that had many positive effects on the overall performance of the software compared to the first designs: 

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item The overall workflow was more usable and easier to launch (less nodes were started)
\item Less bandwidth was used in the ROS network (less publishers and subscribers needed to be configured)
\item Data reduced and information increased per component principle was enforced by the new design   
\end{itemize}

This chapter evaluates the implementation of the final software schematic presented in Figure. The design proved to enable stable heterogenous PC network communications with the Himmeli platform on wired LAN architectures in tests. Tests were run in an actual overhead crane environment, and also in laboratory conditions with simple networks (no hardware routers). Many readily available ROS software packages for visualization, configuration and debugging tools were used to get the networks running as designed. These utilities proved to be of tremendous help in configuring and troubleshooting the network settings. No tests were run in a wireless network environment, but it is quite possible that the current system would be bottlenecked in a wireless network due high data bandwidth requirement. \par
For the selected stereo camera setup and tested networks, any amount of data could be relayed from any software component to the next one successfully by using networking with topics and subscribers. No issues were found, although some limitations posed by the architecture were discovered: 

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item The ROS network introduced varying length delays, as expected
\item Custom ROS message contents (LoadArea.msg) could not be viewed on subscriber machine that had no message headers built
\item The PCL visualization introduces large periodic delays of more than 1000 milliseconds 
\end{itemize}

Any compatibility issues were not discovered during multiple PC network testing. All the ROS installations used in the PC network were running on ROS Groovy, and no compatibility issues were found in the bounding volume measurement tool chain even with mixed build environments (catkin and rosmake). It is reported that different versions of ROS distributions are not cross-compatible, thus, ROS Groovy was selected as the ROS platform installation. \par
Currently, the ROS distribution functionality poses most issues for consideration in the overall design. The biggest risk in the project start was missing ROS knowledge in the project team, and it was known that the ROS programming learning curve is steep for beginners. The missing knowledge risk realised as longer development time of the system, but as such it was an acceptable slowdown in the process. Since ROS is a community-based effort to provide software nodes for robotic programming, a well-maintained index of all the available content did not exist, and it was sometimes difficult to find the right tools and learn how to use them. Tutorials and examples were used to implement all ROS node functionality, and some features, such as the dynamic reconfigure server, suffer from some unstable functionality.\par
On the other hand, the PCL library documentation proved to be excellent, easy to navigate, and very well documented. First, all point cloud processing features were implemented using PCL tutorials, which were excellent. Then, the code was further advanced with the help of the PCL API documentation. All point cloud processing functions fast, and the user can successfully interact with the visualization. The only hiccup in the PCL design was the visualization introducing periodic delays. This may be due large amounts of memory reserved and freed periodically, but a further analysis of the source of the delay was not possible in the scope of the thesis. A further development suggestion is to run memory management and process thread bug testing for the software to counteract errors in continuos operations. Another suggestion would be to branch out a version of the software where the visualization may be disabled after the machine vision system is properly configured. This would effectively enable the use of the client software node in near real-time continuos operations while eliminating the delay induced in the visualization.\par
In the end, the evaluation of the load object measurement system was done using offline data sets that were recorded from use cases in real crane environments. Unfortunately, the online implementation was finished but not tested during the thesis writing, and as such the online performance is not included in this work. The next section will analyse the data quality that was seen in the implemented software in more depth.

\section{Data Quality in the Machine Vision Process}
\label{section:data_quality_in_the_machine_vision_process}

The point cloud data quality was affected by the overall machine vision process including stereo calibration, disparity tuning, and the hardware setup. The hardware setup and the stereo calibration were used as monolithic initial calibration items. The calibration was done automatically for the hardware setup, and after a good basic point cloud output generation was achieved, no further engineering was done for the hardware or the stereo calibration. After an initial point cloud output was generated, the problem of understanding the data quality could be presented: how accurately did the point cloud data resemble the real world scene? \par
The machine vision process generated a point cloud that illustrated the 3d real-world scene viewed by the stereo cameras. The point cloud was constructed from the image areas that were visible for both cameras, and could be modeled according to section \ref{section:depth_map_generation}. For some areas the point cloud data resembled the real scene, and for some areas the point cloud did not look like what it was supposed to look like. The problem is how can this resemblance of the real scene be quantified by a machine? Is it possible to automatically get a value of how good the data describing the real scene is? This section tries to formulate a goodness measure for the output point cloud quality where most important factors of point cloud quality are 

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item location accuracy of the surface 
\item surface noisiness
\item speckle regions, misinterpreted Z depth regions, and artefacts of any kind
\item number of finite data point entries
\end{itemize}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{riegl_reference.jpg}
    \caption{A point cloud of the forestry crane boom environment taken with an industrial Riegl terrestial laser scanner.}
    \label{fig:riegl_reference}
  \end{center}
\end{figure}

Accuracy of the point cloud representation can be quantified using point cloud data quality indicators. Point cloud data quality indicators are used in modeling of urban constructed environments and in airborne LIDAR mapping etc. In these fields, the quality evaluation is done using two different goodness analysis mechanisms: spatial structural analysis and positioning analysis. The need for different quality indicators depends on the application at hand, but the constructed industrial environment encourages to try spatial structure analysis for indoor crane environment, too. Positioning analysis was not done in this thesis since multiple view registration was not used\ref{Feng08}.\par
In literature, also reverse engineering of shape primitives from point cloud response have been used for point cloud data quality verification in industrial settings. In industrial environments, a lot of box-shaped or cylinder-shaped objects can be found and be used for verification. Also, for any primitive geometric shapes it is fairly simple to quantify a goodness of fit value using geometric fitting schemes via least squares error minimization\ref{Fidera04}.
In spatial structure analysis, some real world feature qualities are measured from the environment, and the same qualities are then computed from the scene's point cloud response. Such qualities are for example: location, orientation, volume, smoothness of surface, or some other attribute of the item. Next, these measures are compared and it is possible to determine a goodness of fit -value that can be further processed into a goodness of data quality information. Of course, it is difficult to formulate a meaningful goodness indicator since point cloud data from stereo cameras always contains missing surfaces, measurement noise and errors increasing with more distant objects and other errors. Still, it is good to understand how much the reported measurement from the point cloud is off (in per cent) when compared to the actual object dimensions. \par
For position accuracy verification, landmark objects may be installed in the environment, and their correct locations can be searched for and verified in the point cloud data later. For orientation verification, eigenvectors may be calculated for a point cloud describing an object and the largest eigenvector direction can be compared to the real-world object main axis. For volume verification, an oriented bounding volume may be computed for fully visible objects, and compared to the real-world object volume.\par
In this work, installed landmarks or beacons would have been useful in the offline data sets since they could have been used to verify the correct scale of the point cloud, and correctness of local measured lengths. Since the data set environments did not contain any installed landmarks, a scale verification is not available. A further development suggestion would be to add landmarks objects (e.g. standard size boxes) in the environment and measure their locations prior to capturing the data sets to be able to do a better spatial structure analysis.\par
In this thesis, such objects whose qualities are known a priori are being called standard objects. For example, in the process hall data sets, a standard object titled \emph{large box} with dimensions of 1.52 x 0.94 x 0.92 meters was used. It can be seen in most process crane data sets as part of the setup. The large box object was constructed out of metal sheets, and it was a box-shaped, partially see-through mass weighing in 3.97 tonnes. In the forestry crane measurements, no standard objects were used, because the tree stumps were not measured nor tracked as individual standard objects. As such, the forestry crane data sets cannot be used for point cloud data quality control. In case a CAD model of the forestry crane boom was available, spatial structure analysis could be done for forestry data sets, too, but in this work no CAD model was available.\par
A single effective spatial structure analysis that was possible to do with the captured offline data sets was to check how big was the angle between standing cabinets and the floor in the process crane data sets. For example, a cabinet might be known to form a 90 degree angle with a floor, and such a scene would generate a point cloud response where the angle is not exactly 90 degrees (it could be e.g. 85 degrees). Obviously, this will give indication of goodness of structural integrity only for the local neighbourhood of the feature in the point cloud. For example, good structural integrity in the near-field feature does not guarantee similar accuracy for far-field features.\par
The effects of different stereo camera hardware setups, for example toed-in optical axes installation against parallel installation, were not researched and no definite results can be said of different stereo camera setups on the point cloud quality. A toed-in hardware installation was used in this work, and some vertical disparity definitely affected the point cloud quality, but it was a minor hindrance. By visual inspection, physical moving of the cameras and recalibration of the system generated no visible change in the point cloud quality. The toed-in setup does enable negative disparity values for z-depths that do not exceed the crossover depth for optical axes point of intersection, which was approximately at 1.6 meters. Toeing-in made the overlapping image areas smaller, but it also increases the stereo imaging volume to more near the camera than a parallel setup. Of course, unless disparity values less than zero are enabled, objects that are located in between the cameras near the image plane will still not generate points to the output point cloud. Using the disparity search space property in stereo processing, all the objects near the camera can be removed from the point cloud data response. This property was used to remove the hoisting wires from the output point cloud in the overhead crane data sets where the camera is installed in the trolley facing downwards. Hoisting wires that were more than 1.6 meters away from the camera still could make it into the generated point cloud, but their location was computed erroneously most of the time.\par
After the camera hardware setup was reconfigured, the toe-in angle became larger then previously. This resulted in a smaller region of interest in the rectified images, which means that the usable image area becomes smaller as the toe-in angle is made larger. OpenCV takes this change into account in the stereo rectification process, but as the rectified image becomes more and more distorted with increased toe-in angle, the region of interest becomes smaller, resulting in less accuracy in the reprojection phase.


It should be reported that one time after the right stereo pair camera was moved due a collision of the platform, the cameras were re-installed and a new stereo calibration was done for the HIMMELI platform. In the new hardware setup, the toe-in angle between the camera optical axes was larger then previously, resulting in a smaller overlap of the image regions because of very cross-eyed camera images. The point cloud data quality was affected so that the effective region of interest in the rectified stereo image pair was smaller than previously due to the toe-in angle increase. This was discovered when comparing the calibration files from the original camera setup to the calibration files computed from the new installation. According to the current knowledge by the author, the point cloud data quality was affected very little by the hardware setup change and a re-calibration. Still, more testing and a point cloud quality goodness measure needs to be formulated in order to verify the point cloud data quality indifference towards different hardware setups with calibration.

Measured height of the cameras installed from the floor was 545 cm.

\section{Parameter Tuning Evaluation}
\label{section:parameter_tuning_evaluation}

% reference Kim05 | Jet Propulsion Lab  

\subsection{Disparity Tuning}
\label{subsection:disparity_tuning}

The disparity map generated with standard OpenCV functions needed parameter tuning to provide the best possible reprojection of geometric data from a 2d image to a 3d point cloud. The basic tuning of the block matching algorithm (BM) parameters was done first, and then a semi-global block matching algorithm (SGBM) was tuned with additional parameters. The tuning was done with a custom software tool that displays slider ranges for each parameter. The tool was made for what you see is what you get (WYSIWYG) operation, and the resulting disparity image is displayed instantaneously next to the sliders.
    
\image{disparity_space_from_zitnick_report}

\subsubsection{Block Matching Algorithm Tuning}
\label{subsubsection:block_matching_algorithm_tuning}

The block matching algorithm is tuned with 10 parameters who reside in a BM state object. The parameters are listed in table \ref{table:block_matching_state_parameters}. Additionally, a pair of regions of interest (ROI) are set up in the BM state to mark off the new effective image regions after rectification process. To be able to understand the parameter effects on the block matching performance, a quick note about the inner works of the algorithm is introduced next.

The feature matching works in three steps:

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item pre-filtering to normalize image brightness
\item correspondence search with SAD window along the epipolar lines 
\item post-filtering to remove bad matches
\end{itemize}

The tuning of the BM parameters was done against offline data collected from both selected use cases. The forestry crane data was preferred in the tuning effort since more objects can be seen in the images, thus, the saliency content of the captured video is higher compared to the process hall crane area. High saliency image regions produce good results in block matching, and a rule of thumb would be: more visible 2d objects result in more correctly matched points in the region for the 3d point cloud. For example, the texture rate of change for a matt painted warehouse floor is small, and in the process hall crane case the collected data had minimal floor feature matching response for some BM tuning sets.

\subsubsection{Block Matching Parameter Effects on Feature Matching Performance}
\label{subsubsection:block_matching_parameter_effects_on_feature_matching_performance}

First, the tuning was started with a coarse selection of parameters to get a visible disparity map. Especially, the SAD window size was kept higher than normal to produce a lot of visible matched points. The most important parameters in the final BM state tuning set are SAD window size, number of disparities, and speckle range parameters. These parameters should be tuned first, and other parameters can be used for fine tuning of the disparity map afterwards. 

The SAD window size, or sum of absolute differences window size, affects the size of the sliding kernel window used for correspondence search in the 1d epipolar search space of the rectified images. The smaller the value, the smaller the sliding smoothing kernel used is, and with a minimum value the search is called pixel-to-pixel correspondence search. For metric measurement purposes the disparity map should be as accurate as possible, thus, the minimum value of the SAD window size will be used at all times. If the rectified images are texture rich, then a match can be found for all pixels on an epipolar line. Then again, if the image pair is less rich in texture, for example, a matt painted warehouse floor, the process will find less matches, which usually is the case. A low texture environment will produce little found matches because the local neighbourhoods of the sliding window are too similar in order to confirmed a match. If the point cloud is used for 3d visualization purposes, a higher SAD window size value selection should produce more points for the delight of the viewer.

The number of disparities parameter sets the maximum disparity difference in pixels for a search space in a disparity map. The higher the value, the higher a pixel difference value is allowed in the disparity map. For example, if a very near-field object has a disparity difference of N pixels, and number of disparities property is set smaller than N, then the near-field object is not detected at all in the disparity map leaving a 'hole' in it. This property can be used to remove near-field occluded objects that are impossible to triangulate successfully. For example, in the process hall crane case, the hoisting wires are problematic since they are occluded and visible only from one side in each camera. With number of disparities parameter it is possible to remove the wires from the disparity search space, and altogether from the final point cloud depicting the crane working area. 

The wires are included in the disparity search from certain height onwards, but with a proper selection of number of disparities it is possible to minimize the erroneous reprojection of the hoisting mechanism in the final output point cloud. The user can use the number of disparities property in combination with the minimum disparity property for setting the horopter - the 3d volume that is covered by the stereo matching algorithm \ref{Bradski08}.

Horopter can be enlarged towards the camera image plane with changes in the stereo system parameters. Some changes that enlarge the horopter are decreasing the stereo camera baseline width, decreasing the focal length of the cameras, and increasing the disparity search space.

Speckles are high and low disparity patches generated by the block matching algorithm near object boundaries. The block matching sliding window will catch object foreground and background in the image pair, which is the problem that generates speckle in the disparity map. Thus, the third important parameter is the speckle range parameter. The speckle range controls the threshold for letting the local patches of speckle be matched to the resulting disparity map. If the threshold is met, then the local region is eliminated and no disparity matches will be available for that region. The speckle detector that uses the speckle range value also requires the speckle window size parameter. The value for the window size should be small enough to keep the computation to a minimum, but large enough to detect speckle regions properly.

For each use case, multiple tuning sets were designed, which can be visually inspected by viewing at the point cloud output quality. On one hand, some processes will not work with a small number of output points, such as the RANSAC iteration for a ground plane search. On the other hand, the metric accuracy of the point cloud points decrease as the number of points in the cloud increase. The resulting point cloud from stereo cameras is always a best possible trade-off between accuracy, quality, and size of the cloud. Since the amount of point cloud points heavily affects the time used for further processing of the cloud, accuracy is the number one priority decreasing the computation times simultaneously.

Over time, the quality of the bounding volume measurement depends on many things: primarily the amount of noise in the point cloud object over time, and secondarily on the orientation of the object over time. 

\begin{table}
\caption{List of block matching parameters in a BM state object.}
\label{table:block_matching_state_parameters} %label just after caption
\begin{tabular}{|p{4cm}|l|l|p{4cm}|}
\hline % The line on top of the table
\textbf{Parameter Name} & \textbf{Min.} & \textbf{Max.} & \textbf{Requirements} \\
\hline
Pre-Filter Size & 5 & 21 & Odd Number \\
\hline
Pre-Filter Cap & 1 & 63 & - \\
\hline
SAD Window Size & 5 & 255 & Odd Number \\
\hline
Min. Disparity & -100 & 100 & - \\
\hline
No. Of Disparities & 16 & 256 & Divisable By 16 \\
\hline
Texture Threshold & 0 & no upper limit & - \\ 
\hline
Uniqueness Ratio & 0 & 255 & - \\
\hline
Speckle Window Size & 0 & 100 & - \\
\hline
Speckle Range & 0 & 100 & - \\
\hline
Disp12MaxDiff & 0 & no upper limit & - \\ \hline
\end{tabular} % for really simple tables, you can just use tabular
\end{table} % table makes a floating object with a title

The tuning effort resulted in best possible BM parameters for each use case. The process hall crane parameters are listed in table \ref{table:bm_parameters_overhead}.

\begin{table}
\centering
\ra{1.3}
\caption{Block matching parameters (OpenCV) used in the overhead process crane case. Parameters reported from early development phase and not currently associated with any dataset.}
\begin{tabular}{@{}ll@{}}\toprule
Block matching parameter & Value \\
\midrule
Pre-filter size & 9 \\
Pre-filter cap & 63 \\
SAD window size & 5 \\
Minimum disparity & 0 \\
Number of disparities & 96 \\
Texture threshold & 270 \\ 
Uniqueness ratio & 20 \\
Speckle window size & 42 \\
Speckle range & 10 \\
Disp12MaxDiff & 10 \\
\bottomrule
\end{tabular}
\label{table:bm_parameters_overhead}
\end{table}

\begin{table}
\centering
\ra{1.3}
\caption{Block matching parameters (OpenCV) used in the forestry crane case. Parameters reported from a forest case dataset 1377169118 on February 25th 2014.}
\begin{tabular}{@{}ll@{}}\toprule
Block matching parameter & Value\\
\midrule
Pre-filter size & 9 \\
Pre-filter cap & 63 \\
SAD window size & 5 \\
Minimum disparity & 0 \\
Number of disparities & 144 \\
Texture threshold & 30 \\ 
Uniqueness ratio & 1 \\
Speckle window size & 42 \\
Speckle range & 14 \\
Disp12MaxDiff & 1 \\
\bottomrule
\end{tabular}
\label{table:bm_parameters_forestry}
\end{table}

%\begin{table}
%\caption{Best BM parameter tuning set in forestry crane case.}
%\label{table:forestry_crane_bm_parameters} %label just after caption
%\begin{tabular}[c]{|p{4cm}|l|}
%\hline % The line on top of the table
%\textbf{Parameter Name} & \textbf{Value} \\
%\hline

%\end{tabular} % for really simple tables, you can just use tabular
%end{table} % table makes a floating object with a title

\section{Experiments}
\label{section:experiments}

\subsection{Overhead Crane Case}
\label{subsection:overhead_crane_case}

The following overhead crane image data sets were captured in August 2013 in a real overhead crane environment. A total of 13 data sets were captured that contain different kinds of tests aimed at revealing how the dimension measurement of the load object can function in different scenarios. The lighting conditions did not change during daytime in the overhead crane environment, so there was no need to test at different times of day, or different times of year because of possible changes in the environmental conditions. 

\subsubsection{Findings}

In the overhead crane data sets, a manual load selection approach was used to select an object as the target to measure. This was done when the end effector location was not available for more accurate selection process. Essentially, a load object was selected from near proximity of a pre-defined coordinate. Most of the time, the manual selection coordinate was set in the center of the image, where a load object would probably appear in e.g. the top-down camera configuration. Since no heuristics were used in the overhead crane load selection process, the test design introduced a lot of erroneous load object matches, such as people walking nearby the center of the image, or new objects selected as they came in the view as the platform moved in the process hall. These may be easily seen in the data set visualisations.\par
The decision to select the load object from a pre-defined 3d region of space was far from perfect, but it worked as a load selection condition without knowledge about the end effector location. In essence, it was guessed that a load object would be found near the center of the image, which was not always true. For example, for some data sets where the camera was located in the side of the overhead crane bridge, the load object was found in the bottom part of the image. A visual indicator for the manual load selection coordinate was added in the visualization tool to help set the manual selection center in a better location.\par
The data set coordinate system was not transformed to the world coordinate frame, but kept in the sensor frame at all times during data processing. The reason to do processing in the sensor frame was that the sensor frame implicitly encoded the moving platform signal in the data. An end effector location signal would have been needed if the processing was to be done in the world coordinate frame. Now, because such a signal was not available, the data was kept in the sensor frame, and as a result, a static coordinate depicted a point moving in space with the sensor platform. Also, this way the static camera implicitly encoded the platform movement, thus, no camera viewpoint signal was needed to construct. While the data in the sensor frame was not normalized, it did not introduce singularities or any mathematical problems that very large numbers tend to introduce in some complex algorithms. If the computation was done in the world frame, normalization would possibly be needed to prevent computational singularities if the origin of the framework was set far away from the actual overhead crane hall.\par

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
% 1372841000
\item Objects in the process hall are segmented in good detail. Geometry is well preserved in top-down camera views, and less well in the other used viewpoint (bird's eye).
\item Nearby objects, such as the upper balcony, can be detected by the machine vision system.
\item Some artefacts in the disparity map generate erroneous 3d constructs in the point cloud that are not present in the environment. In all the data sets a white bar can be seen on the right side of the disparity map, and its reprojection, which was a curved tall bar-like feature, can be seen in all 3d point cloud images on the right. A smaller image ROI could possibly remove such artefacts from the end result. 
\item A horizontal striped tape present in the data sets resulted in erroneous depth reprojection, and left a visible artefact under the floor level in the point cloud.
%1372842312
\item Nearby objects are grouped together as single objects by the segmentation algorithm.
%1372842573
\item Empty end effector gives such a small point response that it is discarded as not an object. It was not possible to track with the current system.
\item People near the load object merge into the load object and enlarge the reported bounding volume.
\item If the load object was lifted very high, it occluded most of the floor space in the image. If all the floor space is occluded by the load object, the floor removal will fail. Such an event was never encountered in testing.
%1372842898
\item The scene reprojection introduced large errors when the platform was moving. It was concluded that since the stereo camera head shutters are strictly not enforced to expose the scene at the same time, a moving platform will introduce a disparity map generation that does not match with the pre-defined camera baseline since the other camera is moving before it records the image. To counteract such inaccuracy, the platform should not move while measuring objects, or the shutter behaviour should be strictly enforced. For normal operation where the object is measured as it is lifted up, this is not problematic.
%1372843166
\item If people walk exactly under the top-down camera, their point response is so small that the response is discarded as an error in the point cloud data. This suggests that the lower limit of the point cloud object size should have been even smaller than the used value.
% 1372843682
\item If scene reprojection issues are present when the platform is moving, a lifted load object does not suffer from much inaccuracy because its image is not moving when the platform does. The errors introduced in the load object measurement while the platform is moving are negligible.
% 1372851661
\item The reprojection issues while the platform is moving depict the floor both too far away and too close in turns. This suggests that the HIMMELI platform shutters do not exposure images in the same left to right, or right to left order, but the order changes randomly due to network lag or other sync issues.
% 1372851907
\item In the bird's eye viewpoint the load being lifted in the background of the process hall results in a smaller point response than in the top-down view.
\item In the bird's eye viewpoint, the load object point cloud merges with the back wall point cloud when the load is near the wall. The load object could not be measured if it merged with the back wall. Merging is easily seen in the background because more noise is in features that are further away from the cameras. 
\item In the bird's eye viewpoint, the load object is self-occluded, and its bounding volume does not contain its backside. Thus, the bounding volume computation is not correct from this viewpoint for e.g. the large standard box.
\item In the bird's eye viewpoint, the hoisting wires are clearly visible in the point cloud output and the location is correctly reprojected in 3d.
%1372852148
\item In the bird's eye viewpoint, more points are generated to the 3d point cloud mainly because of the visible back wall. Increased number of points increased computation times.
\item In the bird's eye viewpoint, the distant objects introduced increasing noise and measurement error.
%1372852419
\item In the bird's eye viewpoint, the bounding volume will contain the hoisting wires that are attached to the actual load object. The hoisting wires will enlarge the bounding volume unnecessarily. The extra reported volume could be removed using knowledge about the end effector location and heuristics.
\item In the bird's eye viewpoint, the floor is not fully removed by the RANSAC search, because in the distance the noise spreads the floor's point response a lot. In data set 1372852419 the floor seems to curve up (objects may be seen above the floor level), which may be a result from calibration issues.
\item In the bird's eye viewpoint, the back wall is curved, which possibly is a result of a calibration issue near the image border region.
%1372852802
\item In the bird's eye view, the load object can be lifted out of the picture near the walls.
\item In the bird's eye view, the load object merges with the floor even though it is already lifted (near the opposite wall).
\item In the bird's eye view, the geometry of items near the camera are extracted accurately.
%1372853387
\item Partially see-through items are not registered (not seen) by the machine vision system because the block matching is not working in such an image region.
\item Shiny (e.g. metallic) items are difficult to block match.
\item Upright pole-shaped objects can be seen from the bird's eye viewpoint, but not very well from top-down cameras(for example, road construction markers).
%1372853605
\item See-through or shiny objects may be measured if non-reflective large markers are added on them so that they generate a visible point response in the point cloud. Still, the measurement on such an object is not possible with the currently implemented system used in this thesis. 
\end{itemize}

The effect of the fluorescent lighting in the environment was not analysed since the lighting conditions could not be controlled. The findings of this chapter were done in normal lighting conditions that may be found in a lot of warehouse environments. The fluorescent lights have some effect on the image quality, but it is unknown how the HDR cameras tune the image with the prevalent lighting conditions present in the data sets. The accurate effect of the lighting conditions to machine vision image quality are a vast research topic of their own.

\begin{figure}[ht]
  \begin{center}
    \includegraphics{dimension_measurements_box_ave.png}
    \caption{Dimension measurement data of a standard box from data set 1372841000 containing process hall data.}
    \label{fig:dimension_measurements_box_ave}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
    \includegraphics{dimension_measurements_box_ave.eps}
    \caption{Dimension measurement data of a standard box from data set 1372841000 containing process hall data in EPS file format.}
    \label{fig:dimension_measurements_box_ave_eps}
  \end{center}
\end{figure}

\subsubsection{Analysis}
\label{subsubsection:analysis_overheadcrane}

In order to verify the accuracy of the measurement, the output value of the geometry measurement software must be compared to a value produced by an actual measurement with e.g. measuring tape. In the overhead crane case, only the bounding volumes provided by the top-down camera viewpoint produced meaningful results, thus, the results of the top-down viewpoint will be primarily presented.
Figures x and y are screen captures from the data set 1372841000, which was used to analyse the bounding volume performance for the large standard box. As seen in figure x, the actual object lied on the ground so that the object was rotated about Y axis in a 34 degree angle compared to X-Y -plane. Now, the axis-aligned bounding box computes the maximum and minimum values along an axis, which results in a maximum projection along the axes in each dimension. The axis projection makes it difficult to verify that the measurement actually is correct, but we may estimate how much the measurement differs from reality for example in per centage. If a working online version of the software was available, it would be easy to rotate the standard box into correct orientation, and tune the system so that a correct measurement is given, provided that the calibration is good. Unfortunately this is not the case, and only offline data set may be used for verification in the thesis.
Now, a comparison criterion may be formulated that relates the output value of the geometry measurement software and the actual measures of the large standard box. The mean output values computed using MATLAB for the large box are shown in table \ref{table:large_standard_box_mean_values}.

% put here a table that shows delta X: 1671.6 mm delta Y: 1473.2 mm delta Z: 1058.5 mm
% put here a table that shows X = 152 cm Y = 94 cm Z = 92 cm
% why is the delta Z smallest when delta Z is in the sensor frame.... is it?

If the standard box is rotated 34 degrees, a geometric calculation using Pythagoras theorem results a correct bounding volume size of 1787 mm x 1875.42 mm x ???

In dataset 1372852802 an interesting phenomenon is visible in the visualisation of the data. In the end of the data set it seems like there is an object that continuosly changes size especially in the Z-axis direction. When the data was reviewed, it turned out that the load object was lifted near the ceiling, and as the load object was lowered, the hoisting wires become visible more and more as the load object is being lowered. Thus, the viewpoint generates this kind of continuously changing parameters that describe the bounding volume of the visible load object and the hoisting wires of the crane. 

% insert picture from data set 1372852802


\subsection{Forestry Crane Case}
\label{subsection:forestry_crane_case}

The forestry crane image data sets were captured in August 2013 and readily analysed in April 2014. A total of 30 data sets were captured in March 2013 and August 2013, but only 11 data from August 2013 sets were usable. The reason to this was that the winter data sets did not contain both left and right stereo head images, but only the left stereo image doubled as left and right pictures, resulting in loss of depth information. This setback was unfortunate, which is why the winter footage was not analyzed and the effects of reflective snow and winter conditions are not included in this thesis. On the contrary, the summer footage is excellent, and the computation using the captured imagery taken in August 2013 provided many useful insights into realising a measurement system that could provide the needed dimensions of a load object.

\subsubsection{Findings}

In the forestry crane data sets, the automatic TLD tracking was used in combination with a cylinder fitting scheme to provide more accurate results. It is good to understand that some of the findings in this section are exclusively based on the cylinder fitting results and its implemented heuristic checks.

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item Objects too close to each other will fuse as single object groups in the generated point cloud response. For example, if a person is in touch with the load object, the bounding volume will be enlarged to contain both the load object and the operator as a result. This phenomenon can be controlled using the kD-tree cluster tolerance if no occlusion is present in the image data.
\item Heuristics should be used to prevent cylinder fitting match wrong features from the environment. A check that the cylinder is not located above the end effector in the sensor frame was used, but it is not enough to prevent all erroneous matches.
\item If the end effector tool is empty, the cylinder fitting does not produce meaningful results. Currently, it will give fully or partially the volume of the end effector tool or parts of the crane boom depending whether the RANSAC search finds a cylinder fit in the data. This behaviour adds noise and erroneous measurements in the current AABB bounding volume data graphs as seen in the attachments section.
\item A rotation of the log perpendicularly to the optical axis will report different bounding volumes for different angles of rotation. The measurement is most accurate when the length of the log is fully shown in the image plane. Let this angle be the 0 degrees angle. A rotation of 0-65 degrees will give similar results, but a rotation of more than 65 degrees will cause a lot of self-occlusion of the log and generate a very poor object model. In the case of 90 degree rotation (maximum self-occlusion) only the other end of the log is visible, and in this case, it was not possible to measure the size of the log at all.
\item High smoothness constraint of the disparity map fills holes that are present in the environment. For example, the loops formed by the wiring seen above the end effector tool will generate a solid surface point cloud response. This phenomenon is present in the SGBM algorithm implementation, but not in the BM algorithm implementation. It was concluded that while the SGBM algorithm produces better 3d models of the environment with a high point response output, the BM algorithm actually can have finer detail in some features. In the end, the SGBM algorithm was more suitable for the development of the prototype software.
\item The cylinder fitting can handle picking of multiple logs at once. Multiple logs tend to be in a more hour-glass shape than a cylinder shape. Because of this, the cylinder fitting can fit most volume of the logs inside a single cylinder, but the furthest ends of the logs will stay outside the bounding volume. This is possibly problematic, but no verification was done in a real situation because the crane environment was not available after August 2013. It is possible that the enlarging effect of the disparity map generation increases the bounding volume so that the single cylinder fitting problem for multiple logs is not important.
\item The log can be measured even before it is grasped by the end effector. The measurement relies on close proximity constraint, thus, the log does not need to be actually gripped by the end effector before it can be measured. Further research is needed to verify usefulness of this phenomenon, but potentially the camera system can measure the log on the ground even before it is grasped, which can eliminate the need to pick the log up or rotate before a successful measurement.
\item If the load object is above a pile of logs, or touching or occluding nearby objects, the cylinder fitting will easily be disrupted by wrong features without heuristic checks. It is especially easy to fit a cylinder on a human or other logs.
\item People walking in the area are not easy to distinguish as their separate entity objects if they can jump over other objects, or be in their very near vicinity. For example, humans walking over logs are difficult to separate from the group of logs. If people are walking in the area where little or no other objects are in the near vicinity, then they are easy to separate as single objects.
\item As a result of some earlier findings, a person standing in close proximity to the end effector tool will easily be measured as the cylindrical load objects. Heuristics should be used to prevent mismatched load selections, for example: cylinders that are in upright position in the world frame will not be selected as load objects.
\item Static objects will generate more accurate data and provide means to statistically analyse the goodness of 3d model generation
\item The load object cloud only contains the surfaces visible to the camera, which will introduce errors in the cylinder fitting since only half a cylinder can be recovered as a point cloud. A RANSAC shape model data fitting will give an estimate on the log pose and volume, but it cannot recover any knowledge about the actual backside of the log. Another camera viewpoint and point cloud registration should be used to recover any knowledge about the backside of the log.  
\item If the crane boom is fully visible in both stereo images it can be measured and modeled. In normal operation with the current viewpoint, the crane partially goes out of the picture, which introduces challenges to continuos tracking. Another viewpoint that fully shows the crane boom must be used in case the boom modeling and tracking is needed.
\item It is possible to measure the state of the currently used end effector tool from the point cloud response. It would be possible to implement states that report whether the end effector is closed or open, and whether it carries a log or not. In some orientations of the end effector tool, it is a difficult problem, and in some orientations a fairly easy problem.
\item An occluded log cannot be measured properly. For example, if 50\% of the length of the log is visible in the image, then approximately 50\% of the actual length of the log can be measured by the machine vision system.
\item BM algorithm runs a lot faster than the SGBM algorith and produces similar results.
\end{itemize}

Some improvements to the implemented system may be instantly thought of. For example, the heuristic checks that select the correct load object can easily be improved. For example, if the distance between the candidate load object and the end effector is too large, then the candidate should be rejected. An improved heuristics model was not possible to implement during the thesis study due time constraints, thus, it is left as a suggested improvement in the works.

\chapter{Discussion And Conclusions}
\label{chapter:discussion}
% For Conclusions:
%Write down the most important findings from your work.
%Like the introduction, this chapter is not very long.
%Two to four pages might be a good limit.

\section{Load Object Modeling Quality}
\label{section:load_object_modeling_quality}
%fix passive
%We are not required to fully model the load object in order to find its position and a bounding volume. 
%Currently the produced point cloud only depicts the visible part of the load object since a normal stereo camera setup cannot generate a full envelop view of the object.
%If we wanted to fully model the load object then additional camera viewpoints would be needed that cover all the occluded and non-visible load object surfaces, such as the backside. With a full stereo camera envelop we are able to reconstruct the structure using any available reconstruction algorithm. 
%An interesting idea would be to use an adaptive reconstruction scheme that statistically models the object surface with a self-organizing map as described in de Medeiro's study\cite{Medeiros07}. For such a statistical modeling approach a normalized load object measurement for each discrete timestep would be needed as an input for the SOM model.
%It would be possible to do surface reconstruction if we can access additional viewpoints and envelop the load object fully 
%using adaptive iteration techniques in order to model the object in greater detail as we receive more information from other viewpoints, including its backside.
%statistically estimate and model the object in time using self-organizing maps and adaptive geometric meshing .
%The greatest challenge in computing the load object measurements and position is the low quality of the point cloud data received from the stereo camera rig. Due to limitations in e.g. block matching algorithm, filtering kernels, and changing environmental variables (such as lighting conditions) we may expect white noise to be present in the data for all discrete timesteps.
%Countering the varying geometric signal is difficult and a matter of discussion in itself: do we wish to add filtering that may possibly lower spatial resolution of the depicted scene in order to receive a filtered signal? 
%Since we are interested in the load object AABB measurements we are not required to normalize coordinates for 

A lot of time was used for working on the ROS implementation of the load detection software. The software has a simple backbone implementation thanks to the well-documented readily available OpenCV and PCL libraries. Some of the code runs advanced algorithms underneath, such as RANSAC model iteration, but these are not visible for the user. It would have been possible to implement better algorithms for higher level functions, such as load detection computation and load geometric measurement computation, than what we are currently running on. Due to limited resources we decided to keep the algorithms simple and instead realised a concept software that will run with a supported camera setup. This way we got a working platform that can be used as a testbench for future upgrades and as a framework for implementing more advanced algorithms.  

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{taakka_tunnistettu1.png}
    \caption{Cylindrical load detection in the FAMOUS software using very low quality scenery input data.}
    \label{fig:load_detected1}
  \end{center}
\end{figure}

We lifted a tree trunk with a forestry machine crane and processed the video output with the FAMOUS software. A screenshot of the resulting output from the visualizer can be seen in Figure \ref{fig:load_detected1} where the parameters for stereo matching and 3D segmentation were not tuned optimally. The segmentation of the scene into objects turned out to be quite robust against missing data, fluctuating point clouds, and bad parameter values. The robustness is partly due to a good automatic stereo parameter functionality of OpenCV that was shamelessly used to obtain the stereo parameters. Other reasons for good robustness are the smart handling of uninitialized data in PCL library, and the software architecture that does not differentiate X, Y, and Z directions - no information about orientation of the scene is utilized in 3D segmentation. 
    Then again, we can generate more accurate information with optimal parameter sets, and the measurements from the load object will change for each timestep unless we use adaptive filtering to fight out the white noise and orientation differences in time. We have considered a Kalman filter that tracks the estimate of the actual bounding volume signal, but is was not implemented for simplicity.
    
\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{taakka_tunnistettu2.png}
    \caption{Cylindrical load viewed from a virtual camera from across the crane working area.}
    \label{fig:load_detected2}
  \end{center}
\end{figure}

One problem present in Figure \ref{fig:load_detected2} was that the forestry crane boom belongs to the segmented load object after initial processing. We can see from the third viewport that the load object contains the crane boom, end effector tool, and the load itself. Normally we could assume that the load object is found fully under the end effector tool, but with use cases UCC2-1 and UCC2-2 this is not true. In Riihimäki tests, the tree trunks did easily reach above the end effector tool location when the operator picked them up so that they were tilted. Consequently, the bounding volume does reach above the end effector height, too. We decided to avoid accidental cutting of the load object after segmentation by not making any assumptions of the axis-wise load location other than it is located somewhere near the end effector tool for these 2 use cases. Instead, we opted to fit a cylinder object using RANSAC fitting, giving the approximate position and orientation for the cylindrical load as an output. 

Now, if the user wishes not to use the cylinder coefficients for pose measurements, and a crane-structure cutting AABB-based bounding volume is not a problem, then the AABB shown in Figure \ref{fig:load_detected2} can be used, too. The cylinder fitting implementation did pick up the cylindrical crane boom sometimes in testing, but we implemented a check for the condition that the load is located near the end effector tool location. With a little research, we found out that the cylinder fitting in a forestry crane machine vision system was not engineered previously in Aalto university. The advantage of the cylinder coefficients over the AABB bounding volume is that the oriented cylinder describes the actual volume and orientation of the tree trunk in much more detail compared to an AABB bounding volume, as discussed in chapter \ref{aabb_reference}. 

%end of the work
% Next up : references
% Load the bibliographic references
% ------------------------------------------------------------------
% You can use several .bib files:
% \bibliography{thesis_sources,ietf_sources}
\bibliography{ref}


% Appendices go here
% ------------------------------------------------------------------
% If you do not have appendices, comment out the following lines
\appendix
% \input{appendices.tex}

\chapter{First appendix}
\label{chapter:first-appendix}

This is the first appendix. You could put some test images or verbose data in an
appendix, if there is too much data to fit in the actual text nicely.

For now, the Aalto logo variants are shown in Figure~\ref{fig:aaltologo}.


\begin{figure}
\begin{center}
\subfigure[In English]{\includegraphics[width=.8\textwidth]{aalto-logo-en}}
\subfigure[Suomeksi]{\includegraphics[width=.8\textwidth]{aalto-logo-fi}}
\subfigure[Pä svenska]{\includegraphics[width=.8\textwidth]{aalto-logo-se}}
\caption{Aalto logo variants}
\label{fig:aaltologo}
\end{center}
\end{figure}


% End of document!
% ------------------------------------------------------------------
% The LastPage package automatically places a label on the last page.
% That works better than placing a label here manually, because the
% label might not go to the actual last page, if LaTeX needs to place
% floats (that is, figures, tables, and such) to the end of the
% document.
\end{document}
