% Lines starting with a percent sign (%) are comments. LaTeX will
% not process those lines. Similarly, everything after a percent
% sign in a line is considered a comment. To produce a percent sign
% in the output, write \% (backslash followed by the percent sign).
% ==================================================================
% Usage instructions:
% ------------------------------------------------------------------
% The file is heavily commented so that you know what the various
% commands do. Feel free to remove any comments you don't need from
% your own copy. When redistributing the example thesis file, please
% retain all the comments for the benefit of other thesis writers!
% ==================================================================
% Compilation instructions:
% ------------------------------------------------------------------
% Use pdflatex to compile! Input images are expected as PDF files.
% Example compilation:
% ------------------------------------------------------------------
% > pdflatex thesis-example.tex
% > bibtex thesis-example
% > pdflatex thesis-example.tex
% > pdflatex thesis-example.tex
% ------------------------------------------------------------------
% You need to run pdflatex multiple times so that all the cross-references
% are fixed. pdflatex will tell you if you need to re-run it (a warning
% will be issued)
% ------------------------------------------------------------------
% Compilation has been tested to work in ukk.cs.hut.fi and kosh.hut.fi
% - if you have problems of missing .sty -files, then the local LaTeX
% environment does not have all the required packages installed.
% For example, when compiling in vipunen.hut.fi, you get an error that
% tikz.sty is missing - in this case you must either compile somewhere
% else, or you cannot use TikZ graphics in your thesis and must therefore
% remove or comment out the tikz package and all the tikz definitions.
% ------------------------------------------------------------------

% General information
% ==================================================================
% Package documentation:
%
% The comments often refer to package documentation. (Almost) all LaTeX
% packages have documentation accompanying them, so you can read the
% package documentation for further information. When a package 'xxx' is
% installed to your local LaTeX environment (the document compiles
% when you have \usepackage{xxx} and LaTeX does not complain), you can
% find the documentation somewhere in the local LaTeX texmf directory
% hierarchy. In ukk.cs.hut.fi, this is /usr/texlive/2008/texmf-dist,
% and the documentation for the titlesec package (for example) can be
% found at /usr/texlive/2008/texmf-dist/doc/latex/titlesec/titlesec.pdf.
% Most often the documentation is located as a PDF file in
% /usr/texlive/2008/texmf-dist/doc/latex/xxx, where xxx is the package name;
% however, documentation for TikZ is in
% /usr/texlive/2008/texmf-dist/doc/latex/generic/pgf/pgfmanual.pdf
% (this is because TikZ is a front-end for PGF, which is meant to be a
% generic portable graphics format for LaTeX).
% You can try to look for the package manual using the ``find'' shell
% command in Linux machines; the find databases are up-to-date at least
% in ukk.cs.hut.fi. Just type ``find xxx'', where xxx is the package
% name, and you should find a documentation file.
% Note that in some packages, the documentation is in the DVI file
% format. In this case, you can copy the DVI file to your home directory,
% and convert it to PDF with the dvipdfm command (or you can read the
% DVI file directly with a DVI viewer).
%
% If you can't find the documentation for a package, just try Googling
% for ``latex packagename''; most often you can get a direct link to the
% package manual in PDF format.
% ------------------------------------------------------------------


% Document class for the thesis is report
% ------------------------------------------------------------------
% You can change this but do so at your own risk - it may break other things.
% Note that the option pdftext is used for pdflatex; there is no
% pdflatex option.
% ------------------------------------------------------------------
\documentclass[12pt,a4paper,oneside,pdftex]{report}

% The input files (tex files) are encoded with the latin-1 encoding
% (ISO-8859-1 works). Change the latin1-option if you use UTF8
% (at some point LaTeX did not work with UTF8, but I'm not sure
% what the current situation is)
\usepackage[utf8]{inputenc}
% OT1 font encoding seems to work better than T1. Check the rendered
% PDF file to see if the fonts are encoded properly as vectors (instead
% of rendered bitmaps). You can do this by zooming very close to any letter
% - if the letter is shown pixelated, you should change this setting
% (try commenting out the entire line, for example).
\usepackage[OT1]{fontenc} %fontenc first, then inputenc if need be
% The babel package provides hyphenating instructions for LaTeX. Give
% the languages you wish to use in your thesis as options to the babel
% package (as shown below). You can remove any language you are not
% going to use.
% Examples of valid language codes: english (or USenglish), british,
% finnish, swedish; and so on.
\usepackage[finnish,british]{babel}


% Font selection
% ------------------------------------------------------------------
% The default LaTeX font is a very good font for rendering your
% thesis. It is a very professional font, which will always be
% accepted.
% If you, however, wish to spicen up your thesis, you can try out
% these font variants by uncommenting one of the following lines
% (or by finding another font package). The fonts shown here are
% all fonts that you could use in your thesis (not too silly).
% Changing the font causes the layouts to shift a bit; you many
% need to manually adjust some layouts. Check the warning messages
% LaTeX gives you.
% ------------------------------------------------------------------
% To find another font, check out the font catalogue from
% http://www.tug.dk/FontCatalogue/mathfonts.html
% This link points to the list of fonts that support maths, but
% that's a fairly important point for master's theses.
% ------------------------------------------------------------------
% <rant>
% Remember, there is no excuse to use Comic Sans, ever, in any
% situation! (Well, maybe in speech bubbles in comics, but there
% are better options for those too)
% </rant>

% \usepackage{palatino}
% \usepackage{tgpagella}



% Optional packages
% ------------------------------------------------------------------
% Select those packages that you need for your thesis. You may delete
% or comment the rest.

% Natbib allows you to select the format of the bibliography references.
% The first example uses numbered citations:
\usepackage[square,sort&compress,numbers]{natbib}
% The second example uses author-year citations.
% If you use author-year citations, change the bibliography style (below);
% acm style does not work with author-year citations.
% Also, you should use \citet (cite in text) when you wish to refer
% to the author directly (\citet{blaablaa} said blaa blaa), and
% \citep when you wish to refer similarly than with numbered citations
% (It has been said that blaa blaa~\citep{blaablaa}).
% \usepackage[square]{natbib}

% The alltt package provides an all-teletype environment that acts
% like verbatim but you can use LaTeX commands in it. Uncomment if
% you want to use this environment.
% \usepackage{alltt}

% The eurosym package provides a euro symbol. Use with \euro{}
\usepackage{eurosym}

% Verbatim provides a standard teletype environment that renderes
% the text exactly as written in the tex file. Useful for code
% snippets (although you can also use the listings package to get
% automatic code formatting).
\usepackage{verbatim}

% The listing package provides automatic code formatting utilities
% so that you can copy-paste code examples and have them rendered
% nicely. See the package documentation for details.
% \usepackage{listings}

% The fancuvrb package provides fancier verbatim environments
% (you can, for example, put borders around the verbatim text area
% and so on). See package for details.
% \usepackage{fancyvrb}

% Supertabular provides a tabular environment that can span multiple
% pages.
%\usepackage{supertabular}
% Longtable provides a tabular environment that can span multiple
% pages. This is used in the example acronyms file.
\usepackage{longtable}

% The fancyhdr package allows you to set your the page headers
% manually, and allows you to add separator lines and so on.
% Check the package documentation.
% \usepackage{fancyhdr}

% Subfigure package allows you to use subfigures (i.e. many subfigures
% within one figure environment). These can have different labels and
% they are numbered automatically. Check the package documentation.
\usepackage{subfigure}

% The titlesec package can be used to alter the look of the titles
% of sections, chapters, and so on. This example uses the ``medium''
% package option which sets the titles to a medium size, making them
% a bit smaller than what is the default. You can fine-tune the
% title fonts and sizes by using the package options. See the package
% documentation.
\usepackage[medium]{titlesec}

% The TikZ package allows you to create professional technical figures.
% The learning curve is quite steep, but it is definitely worth it if
% you wish to have really good-looking technical figures.
\usepackage{tikz}
% You also need to specify which TikZ libraries you use
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.pathmorphing,decorations.markings}
\usetikzlibrary{shapes}
\usetikzlibrary{patterns}


% The aalto-thesis package provides typesetting instructions for the
% standard master's thesis parts (abstracts, front page, and so on)
% Load this package second-to-last, just before the hyperref package.
% Options that you can use:
%   mydraft - renders the thesis in draft mode.
%             Do not use for the final version.
%   doublenumbering - [optional] number the first pages of the thesis
%                     with roman numerals (i, ii, iii, ...); and start
%                     arabic numbering (1, 2, 3, ...) only on the
%                     first page of the first chapter
%   twoinstructors  - changes the title of instructors to plural form
%   twosupervisors  - changes the title of supervisors to plural form
\usepackage[mydraft,twosupervisors]{aalto-thesis}
\usepackage[mydraft,doublenumbering]{aalto-thesis}
\usepackage{aalto-thesis}


% Hyperref
% ------------------------------------------------------------------
% Hyperref creates links from URLs, for references, and creates a
% TOC in the PDF file.
% This package must be the last one you include, because it has
% compatibility issues with many other packages and it fixes
% those issues when it is loaded.
\RequirePackage[pdftex]{hyperref}
% Setup hyperref so that links are clickable but do not look
% different
\hypersetup{colorlinks=false,raiselinks=false,breaklinks=true}
\hypersetup{pdfborder={0 0 0}}
\hypersetup{bookmarksnumbered=true}
% The following line suggests the PDF reader that it should show the
% first level of bookmarks opened in the hierarchical bookmark view.
\hypersetup{bookmarksopen=true,bookmarksopenlevel=1}
% Hyperref can also set up the PDF metadata fields. These are
% set a bit later on, after the thesis setup.


% Thesis setup
% ==================================================================
% Change these to fit your own thesis.
% \COMMAND always refers to the English version;
% \FCOMMAND refers to the Finnish version; and
% \SCOMMAND refers to the Swedish version.
% You may comment/remove those language variants that you do not use
% (but then you must not include the abstracts for that language)
% ------------------------------------------------------------------
% If you do not find the command for a text that is shown in the cover page or
% in the abstract texts, check the aalto-thesis.sty file and locate the text
% from there.
% All the texts are configured in language-specific blocks (lots of commands
% that look like this: \renewcommand{\ATCITY}{Espoo}.
% You can just fix the texts there. Just remember to check all the language
% variants you use (they are all there in the same place).
% ------------------------------------------------------------------
\newcommand{\TITLE}{Software Processes for Dummies:}
\newcommand{\FTITLE}{Ohjelmistoprosessit mänteille:}
\newcommand{\STITLE}{Den stora stygga vargen:}
\newcommand{\SUBTITLE}{Re-inventing the Wheel}
\newcommand{\FSUBTITLE}{Uusi organisaatio, uudet pyörät}
\newcommand{\SSUBTITLE}{Lilla Vargens universum}
\newcommand{\DATE}{June 18, 2011}
\newcommand{\FDATE}{18. kesäkuuta 2011}
\newcommand{\SDATE}{Den 18 Juni 2011}

% Supervisors and instructors
% ------------------------------------------------------------------
% If you have two supervisors, write both names here, separate them with a
% double-backslash (see below for an example)
% Also remember to add the package option ``twosupervisors'' or
% ``twoinstructors'' to the aalto-thesis package so that the titles are in
% plural.
% Example of one supervisor:
%\newcommand{\SUPERVISOR}{Professor Antti Ylä-Jääski}
%\newcommand{\FSUPERVISOR}{Professori Antti Ylä-Jääski}
%\newcommand{\SSUPERVISOR}{Professor Antti Ylä-Jääski}
% Example of twosupervisors:
\newcommand{\SUPERVISOR}{Professor Antti Ylä-Jääski\\
  Professor Pekka Perustieteilijä}
\newcommand{\FSUPERVISOR}{Professori Antti Ylä-Jääski\\
  Professori Pekka Perustieteilijä}
\newcommand{\SSUPERVISOR}{Professor Antti Ylä-Jääski\\
  Professor Pekka Perustieteilijä}

% If you have only one instructor, just write one name here
\newcommand{\INSTRUCTOR}{Olli Ohjaaja M.Sc. (Tech.)}
\newcommand{\FINSTRUCTOR}{Diplomi-insinööri Olli Ohjaaja}
\newcommand{\SINSTRUCTOR}{Diplomingenjör Olli Ohjaaja}
% If you have two instructors, separate them with \\ to create linefeeds
% \newcommand{\INSTRUCTOR}{Olli Ohjaaja M.Sc. (Tech.)\\
%  Elli Opas M.Sc. (Tech)}
%\newcommand{\FINSTRUCTOR}{Diplomi-insinööri Olli Ohjaaja\\
%  Diplomi-insinööri Elli Opas}
%\newcommand{\SINSTRUCTOR}{Diplomingenjör Olli Ohjaaja\\
%  Diplomingenjör Elli Opas}

% If you have two supervisors, it is common to write the schools
% of the supervisors in the cover page. If the following command is defined,
% then the supervisor names shown here are printed in the cover page. Otherwise,
% the supervisor names defined above are used.
\newcommand{\COVERSUPERVISOR}{Professor Antti Ylä-Jääski, Aalto University\\
  Professor Pekka Perustieteilijä, University of Helsinki}

% The same option is for the instructors, if you have multiple instructors.
% \newcommand{\COVERINSTRUCTOR}{Olli Ohjaaja M.Sc. (Tech.), Aalto University\\
%  Elli Opas M.Sc. (Tech), Aalto SCI}


% Other stuff
% ------------------------------------------------------------------
\newcommand{\PROFESSORSHIP}{Data Communication Software}
\newcommand{\FPROFESSORSHIP}{Tietoliikenneohjelmistot}
\newcommand{\SPROFESSORSHIP}{Datakommunikationsprogram}
% Professorship code is the same in all languages
\newcommand{\PROFCODE}{T-110}
\newcommand{\KEYWORDS}{ocean, sea, marine, ocean mammal, marine mammal, whales,
cetaceans, dolphins, porpoises}
\newcommand{\FKEYWORDS}{AEL, aineistot, aitta, akustiikka, Alankomaat,
aluerakentaminen, Anttolanhovi, Arcada, ArchiCad, arkki}
\newcommand{\SKEYWORDS}{omsättning, kassaflöde, värdepappersmarknadslagen,
yrkesutövare, intresseföretag, verifieringskedja}
\newcommand{\LANGUAGE}{English}
\newcommand{\FLANGUAGE}{Englanti}
\newcommand{\SLANGUAGE}{Engelska}

% Author is the same for all languages
\newcommand{\AUTHOR}{Stella Student}


% Currently the English versions are used for the PDF file metadata
% Set the PDF title
\hypersetup{pdftitle={\TITLE\ \SUBTITLE}}
% Set the PDF author
\hypersetup{pdfauthor={\AUTHOR}}
% Set the PDF keywords
\hypersetup{pdfkeywords={\KEYWORDS}}
% Set the PDF subject
\hypersetup{pdfsubject={Master's Thesis}}


% Layout settings
% ------------------------------------------------------------------

% When you write in English, you should use the standard LaTeX
% paragraph formatting: paragraphs are indented, and there is no
% space between paragraphs.
% When writing in Finnish, we often use no indentation in the
% beginning of the paragraph, and there is some space between the
% paragraphs.

% If you write your thesis Finnish, uncomment these lines; if
% you write in English, leave these lines commented!
% \setlength{\parindent}{0pt}
% \setlength{\parskip}{1ex}

% Use this to control how much space there is between each line of text.
% 1 is normal (no extra space), 1.3 is about one-half more space, and
% 1.6 is about double line spacing.
\linespread{1} % This is the default
% \linespread{1.3}

% Bibliography style
% acm style gives you a basic reference style. It works only with numbered
% references.
\bibliographystyle{acm}
% Plainnat is a plain style that works with both numbered and name citations.
% \bibliographystyle{plainnat}


% Extra hyphenation settings
% ------------------------------------------------------------------
% You can list here all the files that are not hyphenated correctly.
% You can provide many \hyphenation commands and/or separate each word
% with a space inside a single command. Put hyphens in the places where
% a word can be hyphenated.
% Note that (by default) LaTeX will not hyphenate words that already
% have a hyphen in them (for example, if you write ``structure-modification
% operation'', the word structure-modification will never be hyphenated).
% You need a special package to hyphenate those words.
\hyphenation{di-gi-taa-li-sta yksi-suun-tai-sta}



% The preamble ends here, and the document begins.
% Place all formatting commands and such before this line.
% ------------------------------------------------------------------
\begin{document}
% This command adds a PDF bookmark to the cover page. You may leave
% it out if you don't like it...
\pdfbookmark[0]{Cover page}{bookmark.0.cover}
% This command is defined in aalto-thesis.sty. It controls the page
% numbering based on whether the doublenumbering option is specified
\startcoverpage

% Cover page
% ------------------------------------------------------------------
% Options: finnish, english, and swedish
% These control in which language the cover-page information is shown
\coverpage{english}


% Abstracts
% ------------------------------------------------------------------
% Include an abstract in the language that the thesis is written in,
% and if your native language is Finnish or Swedish, one in that language.

% Abstract in English
% ------------------------------------------------------------------
\thesisabstract{english}{

The purpose of this diploma thesis is to realise a crane load position measurement setup.
The measurement should accurately describe the position and orientation of an arbitrary load
lifted with a process crane or a boom (todo: check type) crane.

This thesis will primarily study a machine vision approach that uses stereo vision for the work environment
depth mapping. Other optional sensors, such as laser scanner, are considered.

Existing machine vision approaches will be applied to the crane load problem. 
The location measurement of the crane load could enable additional improvements in obstable avoidance while
operating the crane.}

%\fixme{Abstract text goes here (and this is an example how to use fixme).}
%Fixme is a command that helps you identify parts of your thesis that still
%require some work. When compiled in the custom \texttt{mydraft} mode, text
%parts tagged with fixmes are shown in bold and with fixme tags around them. When
%compiled in normal mode, the fixme-tagged text is shown normally (without
%special formatting). The draft mode also causes the ``Draft'' text to appear on
%the front page, alongside with the document compilation date. The custom
%\texttt{mydraft} mode is selected by the \texttt{mydraft} option given for the
%package \texttt{aalto-thesis}, near the top of the \texttt{thesis-example.tex}
%file.

%The thesis example file (\texttt{thesis-example.tex}), all the chapter content
%files (\texttt{1introduction.tex} and so on), and the Aalto style file
%(\texttt{aalto-thesis.sty}) are commented with explanations on how the Aalto
%thesis works. The files also contain some examples on how to customize various
%details of the thesis layout, and of course the example text works as an
%example in itself. Please read the comments and the example text; that should
%get you well on your way!}

% Abstract in Finnish
% ------------------------------------------------------------------
\thesisabstract{finnish}{
\fixme{
Kivi on materiaali, joka muodostuu mineraaleista ja luokitellaan
mineraalisisältönsä mukaan. Kivet luokitellaan yleensä ne muodostaneiden
prosessien mukaan magmakiviin, sedimenttikiviin ja metamorfisiin kiviin.
Magmakivet ovat muodostuneet kiteytyneestä magmasta, sedimenttikivet vanhempien
kivilajien rapautuessa ja muodostaessa iskostuneita yhdisteitä, metamorfiset
kivet taas kun magma- ja sedimenttikivet joutuvat syvällä maan kuoressa
lämpötilan ja kovan paineen alaiseksi.

Kivi on epäorgaaninen eli elottoman luonnon aine, mikä tarkoittaa ettei se
sisällä hiiltä tai muita elollisen orgaanisen luonnon aineita. Niinpä kivestä
tehdyt esineet säilyvät maaperässä tuhansien vuosien ajan mätänemättä. Kun
orgaaninen materiaali jättää jälkensä kiveen, tulos tunnetaan nimellä fossiili.

Suomen peruskallio on suurimmaksi osaksi graniittia, gneissiä ja
Kaakkois-Suomessa rapakiveä.

Kiveä käytetään teollisuudessa moniin eri tarkoituksiin, kuten keittiötasoihin.
Kivi on materiaalina kalliimpaa mutta kestävämpää kuin esimerkiksi puu.}}
% Abstract in Swedish
% ------------------------------------------------------------------
%\thesisabstract{swedish}{
%illa Vargens universum är det tredje fiktiva universumet inom huvudfäran av de
%ecknade disneyserierna - de övriga två är Kalle Ankas och Musse Piggs
%niversum. Figurerna runt Lilla Vargen kommer huvudsakligen frän tre källor ---
%els persongalleriet i kortfilmen Tre smä grisar frän 1933 och dess uppföljare,
%dels längfilmen Sängen om Södern frän 1946, och dels frän episoden ``Bongo'' i
%ängfilmen Pank och fägelfri frän 1947. Framför allt de två första har
%sedermera även kommit att leva vidare, utvidgas och införlivas i varandra genom
%tecknade serier, främst sädana producerade av Western Publishing för
%amerikanska Disneytidningar under ären 1945--1984.

%Världen runt Lilla Vargen är, i jämförelse med den runt Kalle Anka eller Musse
%Pigg, inte helt enhetlig, vilket bland annat märks i Bror Björns skiftande
%personlighet. Den har även varit betydligt mer öppen för influenser frän andra
%Disneyvärldar, inte minst de tecknade längfilmerna. Ytterligare en skillnad är
%tt varelserna i vargserierna förefaller stä närmare sina förebilder inom den
%verkliga djurvärlden. Att vargen Zeke vill äta upp grisen Bror Duktig är till
%exempel ett ständigt äterkommande tema, men om katten Svarte Petter skulle fä
%för sig att äta upp musen Musse Pigg skulle detta antagligen höja ett och annat
%ögonbryn bland läsarna.}


% Acknowledgements
% ------------------------------------------------------------------
% Select the language you use in your acknowledgements
\selectlanguage{english}

% Uncomment this line if you wish acknoledgements to appear in the
% table of contents
%\addcontentsline{toc}{chapter}{Acknowledgements}

% The star means that the chapter isn't numbered and does not
% show up in the TOC
\chapter*{Acknowledgements}

I wish to thank all students who use \LaTeX\ for formatting their theses,
because theses formatted with \LaTeX\ are just so nice.

Thank you, and keep up the good work!
\vskip 10mm

\noindent Espoo, \DATE
\vskip 5mm
\noindent\AUTHOR

% Acronyms
% ------------------------------------------------------------------
% Use \cleardoublepage so that IF two-sided printing is used
% (which is not often for masters theses), then the pages will still
% start correctly on the right-hand side.
\cleardoublepage
% Example acronyms are placed in a separate file, acronyms.tex
% \input{acronyms}

\addcontentsline{toc}{chapter}{Abbreviations and Acronyms}
\chapter*{Abbreviations and Acronyms}

% The longtable environment should break the table properly to multiple pages,
% if needed

\noindent
\begin{longtable}{@{}p{0.25\textwidth}p{0.7\textwidth}@{}}
2k/4k/8k mode & COFDM operation modes \\
3GPP & 3rd Generation Partnership Project \\
ESP & Encapsulating Security Payload; An IPsec security protocol \\
FLUTE  & The File Delivery over Unidirectional Transport protocol \\
e.g.& for example (do not list here this kind of common acronymbs or abbreviations, but only those that are essential for understanding the content of your thesis. \\
note & Note also, that this list is not compulsory, and should be omitted if you have only few abbreviations

\end{longtable}


% Table of contents
% ------------------------------------------------------------------
\cleardoublepage
% This command adds a PDF bookmark that links to the contents.
% You can use \addcontentsline{} as well, but that also adds contents
% entry to the table of contents, which is kind of redundant.
% The text ``Contents'' is shown in the PDF bookmark.
\pdfbookmark[0]{Contents}{bookmark.0.contents}
\tableofcontents

% List of tables
% ------------------------------------------------------------------
% You only need a list of tables for your thesis if you have very
% many tables. If you do, uncomment the following two lines.
% \cleardoublepage
% \listoftables

% Table of figures
% ------------------------------------------------------------------
% You only need a list of figures for your thesis if you have very
% many figures. If you do, uncomment the following two lines.
% \cleardoublepage
% \listoffigures

% The following label is used for counting the prelude pages
\label{pages-prelude}
\cleardoublepage

%%%%%%%%%%%%%%%%% The main content starts here %%%%%%%%%%%%%%%%%%%%%
% ------------------------------------------------------------------
% This command is defined in aalto-thesis.sty. It controls the page
% numbering based on whether the doublenumbering option is specified
\startfirstchapter

% Add headings to pages (the chapter title is shown)
\pagestyle{headings}

% The contents of the thesis are separated to their own files.
% Edit the content in these files, rename them as necessary.
% ------------------------------------------------------------------

% \input{1introduction.tex}


%===========================================================================================================
\section{Notation and Symbols}

%\footnote{http://owl.english.purdue.edu/owl/resource/574/02/} of
%Purdue University or Strunk's Elements of
%Style\footnote{http://www.bartleby.com/141/}. Remember that footnotes

%\emph{cite}.

% An example of a traditional LaTeX table
% ------------------------------------------------------------------
% A note on underfull/overfull table cells and tables:
% ------------------------------------------------------------------
% In professional typography, the width of the text in a page is always a lot
% less than the width of the page. If you are accustomed to the (too wide) text
% areas used in Microsoft Word's standard documents, the width of the text in
% this thesis layout may suprise you. However, text in a book needs wide
% margins. Narrow text is easier to read and looks nicer. Longer lines are
% hard to read, because the start of the next line is harder to locate when
% moving from line to the next.
% However, tables that are in the middle of the text often would require a wider
% area. By default, LaTeX will complain if you create too wide tables with
% ``overfull'' error messages, and the table will not be positioned properly
% (not centered). If at all possible, try to make the table narrow enough so
% that it fits to the same space as the text (total width = \textwidth).
% If you do need more space, you can either
% 1) ignore the LaTeX warnings
% 2) use the textpos-package to manually position the table (read the package
%    documentation)
% 3) if you have the table as a PDF document (of correct size, A4), you can use
%    the pdfpages package to include the page. This overrides the margin
%    settings for this page and LaTeX will not complain.
% ------------------------------------------------------------------
% Another note:
% ------------------------------------------------------------------
% If your table fits to \textwidth, but the cells are so narrow that the text
% in p{..}-formatted cells does not flow nicely (you get underfull warnings
% because LaTeX tries to justify the text in the cells) you can manually set
% the text to unjustified by using the \raggedright command for each cell
% that you do not want to be justified (see the example below). \raggedleft
% is also possible, of course...
% ------------------------------------------------------------------
% If you need to have linefeeds (\\) inside a cell, you must create a new
% paragraph-formatting environment inside the cell. Most common ones are
% the minipage-environment and the \parbox command (see LaTeX documentation
% for details; or just google for ``LaTeX minipage'' and ``LaTeX parbox'').
% Alignment of sells: l=left, c=center, r=right.
% If you want wrapping lines, use p{width} exact cell widths.
% If you want vertical lines between columns, write | above between the letters
% Horizontal lines are generated with the \hline command:

% Place a & between the columns
% In the end of the line, use two backslashes \\ to break the line,
% then place a \hline to make a horizontal line below the row

%\begin{table}
%\begin{tabular}{|p{2cm}|p{3.8cm}|p{4.5cm}|p{1.1cm}|}
%\hline % The line on top of the table
%\textbf{Code} & \textbf{Name} & \textbf{Methods} & \textbf{Area} \\
%\hline
%T-110.6130 & Systems Engineering for Data Communications
%    Software & \raggedright Computer simulations, mathematical modeling,
%  experimental research, data analysis, and network service business
%  research methods, (agile method) & T-110 \\
%\hline
%\multicolumn{2}{|p{6.25cm}|}{Mat-2.3170 Simulation (here is an example of
% multicolumn for tables)}& Details of how to build simulations & T-110 \\
%\hline
%S-38.3184 & Network Traffic Measurements and Analysis
%& \raggedright How to measure and analyse network
%  traffic & T-110 \\ \hline
% \end{tabular} % for really simple tables, you can just use tabular
% You can place the caption either below (like here) or above the table
%\caption{Research methodology courses}
% Place the label just after the caption to make the link work
%\label{table:courses}
%\end{table} % table makes a floating object with a title


%The multicolumn command takes the following 3 arguments:
% the number of cells to merge, the cell formatting for the new cell, and the
% contents of the cell

\chapter{Introduction}
\label{chapter:introduction}

Cranes have been used for moving goods as long as people have been moving from one place to another.

\section{How To Detect Goods?}

\section{Towards Automatic Operation}

The results achieved by image processing technologies have claimed a lot of interest in two major application areas.

First area is the enhancing of graphical content for human interpretation. In scientific image treatment it simply means that a noisy or corrupted original image can be treated in order to remove or diminish the corruption, resulting in useful content for human users to survey.

The second area is enabling autonomous machine perception. A machine can tirelessly inspect items on a product assembly line that would end up being dangerous, monotonous, or otherwise problematic for a human worker. Additionally, we may utilize the full electromagnetic spectral range in addition to visible light spectrum for quality control purposes.
 
Research effort in image processing technologies continues to result in better and faster algorithms for image enhancing.

\section{Instrumentation Problem}

The very first problem we had was that the boom in the forestry crane cannot be sensorised. Or so the John Deere representatives say. The sensor life-time expectancy is low in the vibrating environment, and the space for installing any sensors is very limited.

\section{Improved Safety}


\section{}

% BACKGROUND => METHODS => IMPLEMENTATION

%====================================================================================================
\chapter{Applications For Load Positioning and Tracking}
\label{chapter:applications}

\section{Safety Applications}

Traditional and new safety applications can utilize load positioning information for added functionality.

A traditional safety application stops the crane process if a human is detected in a dangerous work zone \cite{Raula11}. Alternatively, an adaptive safety scheme can be utilised to restrict the operation range of the crane, but not halt it. If we are able to reliably detect a location for the moving load

The work area of the crane can be modelled with state-of-the-art sensing of the environment enabling for example pre-collision avoidance.

Adaptive safety techniques can be realised with the improved

%====================================================================================================
\chapter{Sensing Of Time-Varying Real World Events}

\section{Object Recognition}

Object recognition works on a classification principle: a classifier detects attributes in an object and works out into which object class it belongs to.

+ classification principle
    * statistical classifier
    
+ knowledge representations
    + syntactic pattern recognition
        * relations and description language
+ machine learning
    * neural nets
    * genetic algorithms
    
+ graph matching

+ fuzzy systems


\subsection{}



\section{Object modeling of a real-world item}

The digital model of a real-world item

In the virtual reality (VR) research the modeling of real-world items has been a big problem for many years. 
It is not easy to recreate a real-world item in a digital system, but many approaches have been suggested.

One option is to record video from multiple cameras that look at the object from different points of view. In order to obtain a digital model we may utilise a multi-camera stereo process and compute range images for further processing. Obviously, this approach is very computationally difficult and results vary depending on the object convexity, lighting environment and the used algorithms.  

A study by the Carnegie Mellon university robotics institute call the model acquired from the intensity image and the corresponding range imagery a visible surface model (VSM) \cite{Rander97}. Basically a VSM contains a subset of real-world surface features and texture in the digital format, and the VSM can be further placed into a more comprehensive representation of the world, namely a complete surface model (CSM). 
m

\subsection{Structure From Motion}



\subsection{Multiple Perspective Interactive Video}

Multiple perspective interactive video (MPI) method is used to enhance live video with interactivity. The user can adjust camera views, or add digital content in the video. 

Basically, a detection of motion is achieved when each video frame is compared to a background image and the differences are logged. 

The method starts with knowing a priori metric map of the scene, and it requires multiple cameras to work with. The number of cameras connected to the system is in range from 2 to 1000 cameras.

Visible surface model
+ 2.5D Marr paradigm
+ Structure recovery method: multibaseline stereo
+ First make a triangle mesh model 

The real-world coordinates of each backprojected pixel can be computed using the depth and image coordinates given a camera calibration \cite{Rander97}. The resulting set of [x,y,z] coordinates is mainly referred to as a point cloud, depicting a point structure of the computed object.

In MPI method a triangle mesh presentation of the depicted scene is computed with the resulting point cloud. Many software exist that can visualise a point cloud, such as rviz in RoS system, but the suggested textured mesh presentation needs further processing and texture mapping. 

After digital processing we get a digital model of the scene from viewpoint A. When the camera is located exactly in viewpoint A, the presentation is flawless in comparison to the original scene. The report from Rander et al. \cite{Rander97} states that if a virtual camera is moved not far away, the scene can be transformed to another viewpoint B. In viewpoint B the image contains holes where occlusion was present in the original scene.

If the point cloud contains N x N points in the depth map, the computational complexity for triangles is O(N^2) \cite{Rander97}. Furthermore, the modeling of planar surfaces introduce shortcomings in surface tessellation. The surface may be tessellated with 1000 triangles in favor of 2 triangles, which means the computation overhead increases and causes extra calculations in texturing and modeling phases. The number of triangles in an area is described by a measure called mesh resolution that can be controlled in favor of smarter modeling \cite{Johnson96}.

\subsection{Stereo Techniques}

Synthetic random dot images can be used for algorithm testing \cite{Zitnick00}.

\subsubsection{Stereo Assumptions}

Some basic assumptions about the scene to be measured must be made.

+ A single pixel corresponds to a single surface point (restriction: no opaque materials | fish + fish bowl)
+ Disparity values are generally continuos (smooth within a local neighbourhood | discontinuities occur only at object boundaries)
+ Ordering constraint (if an object A is to the left of the object B in the left image, then the object A will be left of the object B also in the right image | sometimes violated by pole-like objects)
+ Lambertian surfaces => surfaces do not change appearance when viewed from another angle

Stereo vision produces a dense disparity map . (If we compare to laser scanner sensor, the denseness of the produced point cloud is much higher)
Some requirements for the disparity map are elaborated in Zitnick et al. report, namely the disparity map should be smooth and detailed . The desirable result would provide smooth continuos mapping that detects small surface elements as separable regions. It turns out that these requirements are opposing to each other: a smooth continuos disparit map tends to filter out small details, and a detail preserving mapping is affected by noise \cite{Zitnick00}.

\subsubsection{Stereo Errors}

In standard stereo processing the object measurement error increases with items that are located far away. If a single baseline and resolution is used the error grows quadratically with increasing depth \cite{Gallup08}.

A simple equation for the depth error is introduced in \cite{Gallup08}:

\begin{equation}
\epsilon_z = \frac{z^2}{bf}\dot \epsilon_d
\end{equation}

where \emph{z} is the depth value, \emph{b} is the baseline value, \emph{f} is the focal length, and \epsilon_d is the disparity matching error (in pixels). The resulting depth error $\epsilon_z$ 



\subsubsection{Stereo Matching}

It is important to choose a proper window size for stereo matching. In low contrast regions of the image too small a window cannot guarantee a unique match because of too little intensity variation. 
In general, a small window is desirable to avoid unnecessary smoothing, but optimal window size depends on the region intensity variation, texture, and disparity \cite{Zitnick00}.
The problems of window size can be solved with iterative window size methods, but increase in computation overhead or problems at occlusion boundary are still left unsolved \cite{Zitnick00}..

\image{disparity_space_from_zitnick_report}

+ Uniqueness and continuity assumptions

\section{Camera Calibration}

+ Why do we have to calibrate the cameras?
    * correct distortions
    * rectify images
        + Why do have to we rectify images?
    * improve performance of stereo corresponsdence algorithms
    
+ How do we calibrate a camera?
    * Air-glass interfaces in the lens system
    * 

Calibration is the means of linking the three-dimensional object and its two-dimensional projected representation in a scientific way. By modeling the image capture and projection with a camera, a camera model is presented. A camera model consist of multiple unknown parameters that need to be solved for a complete representation of the imaging sequence.


\subsection{Planar Calibration Grids}
Planar calibration grids are the most common calibration method for a stereo camera setup. Mainly checkerboard patterns and rectangular grid patterns are used for calibration purposes.

\subsection{Calibration Error Sources}

+ Tangential distortion

+ Radial distortion

+ Chromatic aberration

+ Affine distortion (aspect ratio)

+ Camera electronics, light intensity changes affect the phase locked loop (PPL) in the electronics => line jitter with sync signal systematic or random noise change

+ Calibration target location
\subsection{The Camera Model}
The camera model maps the transformation between the real-world object and the projection of the camera image plane. The camera model is a set of parameter that define this transformation as realistically as possible. 

The real-world object projection on the camera image plane is a lossy transformation where a lot of information is lost due transformation. For example, when a cube is projected on the image plane, we cannot compute the texture of all 6 cube faces anymore because the data of all non-visible cube faces is lost in the image plane.

Typical camera model is usually based on ortographic projection or perspective projection models. In 3D imaging a suitable camera model would be the perspective projection model with lens distortion modeling included. The selection really depends on the applications and its accuracy requirements for model accuracy, but in simple computer vision applications the linear ortographic model would be appropriate. 

\subsubsection{Intrinsic and Extrinsic Camera Parameters}

A camera model has a multitude of parameters whose amount depend on the selected camera and lens distortion models, and the number of calibration images used. 

The camera model has internal parameters that describe the major features of the camera model. Those are focal length, principal point, skew coefficient, and distortion coefficients \cite{CaltechWeb10}.



A calibration technique that is unbiased and optimal is ideal. 

Originally, camera calibration algorithms were used in aerial imaging systems.


Many fast but inaccurate algortihms for parameter estimation have been introduced earlier. The increase in processing power of computing platforms helps to develop new non-linear camera calibration methods that are accurate. Heikkilä \cite{Heikkila00} claims that a subpixel accuracy of 1/50 of a pixel can be realistically achieved with a modern CCD imaging cell if such non-linear calibration is used.

+ Intrinsic camera parameters

+ Extrinsic camera parameters
\subsection{Effects of Weather Conditions}

Quality of measurements degrade in bad weather, such as fog, mist, rain, or snow \cite{Kawai12}. 
Especially camera systems are affected with possibly inhomogenous degradation in image quality.

Different types of degradation:
    + lens flare
    + condensation in humidity changes
    + changes in illumination
    + shadowing
    + occlusion
    + noise from moving droplets of water e.g.

If a stereo camera is used, inhomogenous degradation in image quality


Colored markers are sensitive to natural light in outdoor environments \cite{Kawai12}.

\chapter{3D Vision}

3D vision or stereo vision is a machine vision technique that is used in many engineering applications.
We are mainly interested in machine vision as means of extracting point cloud data from a scene that represents the crane load in a crane working area.

3D vision task is generally seen as a problem of 3D scene reconstruction, object property understanding, or a visual control task of minimization, depending on the scientific application at hand \cite{Sonka07}.

\section{Marr's Theory}

Marr's theory especially points out the great challenge in machine vision of how to achieve a better understanding of the visual system in general, and not just create another vision application suited for a single specific target.

\subsection{From 2D to 3D}

According to \cite{Sonka07}, an object-centered approach is required in favor of perspective approach in order to fully realise the power of descriptor classes used for reconstruction of the scene. The requirement then, is to move step-by-step from a pixel-based intensity image representation to a full three-dimensional representation.

1. From pixels to surface representation
2. From surface representation to surface orientation model
3. From surface orientation model to a full three-dimensional description

2D => primal sketch => 2.5D sketch => full 3D

The primal sketch is an edge image where physical edges are not incurred. The idea is to extract information about physical features of different scales using filtering and second-order zero crossings. First, the original image is filtered with different radii Gaussian filters and after that a Laplacian operator is used to locate second-order zero crossings \cite{Sonka07}. 

The next step constructs the 2.5D image, or a depth map, with bottom-up machine vision techniques. Since bottom-up techniques are used, this step of depth map computing is applicable for all applications and no additional domain-speficic information is needed. 

The final transformation from the depth map to a full three-dimensional object representation is not presented in the thesis. It is also the most difficult transformation as opposed to well-known mechanisms of primal sketch generation and depth map generation. It is 


No additional domain-specific information is needed since the 

Interesting problems
+ Feature observability in an image
+ Marr's theory
+ Perspective projection challenges

Bottom-up reconstruction
    * from multiple image construct range images

Top-down recognition
    * CAD model recognition especially

Space Carving
    * vcolumetric representationtgoo


+ object surface parameters
    * reflectivity e.g.
    
+ 

\section{Perspective projection geometry}

In general, a perspective projection, or central projection, is the de facto type of projection for human beings. We all have a pair of eyes that can be modeled with a pinhole camera model, and the pinhole camera depicts any scene with a perspective projection. In perspective projection objects that are far away appear smaller than objects that are near. This phenomenon is called a perspective. 

The stereo cameras that sense the world with CCD arrays give the user a perspective projection of the world on a 2D image plane. The basics of this projective geometry will be discussed in the case of non-parallel setup of the stereo cameras. That means the optical axes of the cameras are not in parallel, but have a specified angle with which the optical axes coincide. The cameras themselves will be modeled as pinhole cameras with certain lens system errors corrected for.

In computer vision, 

+ homogenous transformation => transforms lines into lines



The camera geometry model that we use is a single perspective camera model or a pinhole camera model.


As depicted in the \ref{}

\section{The Pinhole Camera}

A pinhole camera will be the basis of the theoretical camera modeling since it is the simplest approximation that is suitable for a computer vision applications \cite{Sonka07}.

Some basic geometry items. There is an image plane. An optical axis lies perpendicularly to the image plane pointing out from the image plane towards the scene. 
Optical axis.
Focal point (optical point)













The pinhole camera includes a thin lens system that can be approximated with ideal formulation. 




The image sensor has width \emph{w} and height \emph{h} which relate to each other with aspect ratio \emph{a} so that

\begin{equation}
a = \frac{w}{h}
\end{equation}

We can relate the width of the image sensor with the field of view angle by equation

\begin{equation}
w = 2f\tan{\frac{\theta_{fov}}{2}}
\label{equation:width}
\end{equation}

If we consider a fixed-baseline stereo setup where baseline \emph{b} is kept constant a report by Gallup et al. easily shows how the depth error is related to the depth measurement in quadratic sense by equation

If we consider the effect of resolution on the disparity computing, or depth value computing, we can start with the number of pixels in the image sensor by equation

\begin{equation}
number of pixels = wh = \frac{w^2}{a}
\end{equation}

Furthermore, we can substitute width \emph{w} from \ref{equation:width} getting the equation for the number of pixels as in \cite{Gallup08}:

\begin{equation}
nop = \frac{{z_{far}}^4}{{\epsilon_z}^2} \frac{4\tan^2{\frac{\theta_{fov}}{2}}}{b^2 a}
\label{equation:zdepth}
\end{equation}

As we can see from equation \ref{equation:zepth} the increase in pixel resolution to match a specified accuracy also must match the depth resolution proportionally to ${z_{far}}^4$. Consequently, the required increase in resolution may be impossible due to engineering limitations of new hardware or computations limits of increased overheard due to increase in number of pixels.

The increase in stereo processing computation overhead is tightly coupled with the selected image resolution and depth error. According to \cite{Gallup08} the number of pixel comparisons needed is in the growth range of $ \Omega({z_{far}}^6 {\epsilon_z}^-3)$. For example, if we wish to extend the depth range by a factor of 3 the required number of comparisons would be $3^6 = 729$ times more computationally expensive.

When faced with this kind of image computation problem that should ideally be run in real-time the design of the imaging system must be carefully planned. It should be noted that increase in image resolution seems not to solve a stereo correspondence problem, but it may severely increase the computation time. 

The basis for camera modeling begins with the pinhole camera model where image coordinates \emph{u} and \emph{v} are transformed as

\begin{equation}
\Biggl[ \begin{array}{c}
\emph{u} \\
\emph{v} \\
1 \end{array} \Biggl]
= \Biggl[ \begin{array}{c}
\lambda \emph{u} \\
\lambda \emph{v} \\
\lambda \end{array} \Biggl]
= \textbf{F}\Biggl[ \begin{array}{c}
\emph{X} \\
\emph{Y} \\
\emph{Z} \\
1 \end{array} \Biggl]
= \textbf{PM} \Biggl[ \begin{array}{c} 
\emph{X} \\
\emph{Y} \\
\emph{Z} \\
1 \end{array} \Biggl]
\end{equation}

where a perspective transformation matrix \emph{P} is introduced as

%\begin{equation}
%\textbf{P} = \Biggl[ \begin{array}{cccc}
%emph{sf} & 0 & \emph{u_0} & 0 \\
% & \emph{f} & \emph{v_0} & 0 \\
% & 0 & 1 & 0 \end{array} \Biggl]
%\end{equation}

%+ The camera matrix

\begin{equation} K =
\Biggl[ \begin{array}{ccc}
Focal length 1 & Aspect ratio * Focal length 1 & Principal point 1 \\
0 & Focal length 2 & Principal point 2 \\
0 & 0 & 1 \Biggl] \end{array}
\end{equation}

+ Aspect ratio

\image{lossy_transformation}

Camera calibration can be weak or strong. If strong camera parameters are known it is possible to calculate image scene Euclidian metrics. If weak camera parameters are known, only a pixel to pixel transformation from one image to another is possible, for example, an epipolar projection transformation can be done with weak parameters \cite{Rander97}.


\section{Object Occlusion}

Occlusion means partial or full loss of line of sight of the object in an image. In stereo matching, an object may be partially occluded by another object thus resulting in the original object surface in a stereo image A but not visible in the stereo image B. The object may be occluded in only left or right stereo image, or partially occluded in both. In both cases, the other image has visible object surface that cannot be matched in the other image.

Occlusion is a big problem in stereo matching process. Most solutions do not explicitly detect these occluded regions 
Some methods:
+ multiple cameras and camera masking
+ bidirectional matching
+ 

%================================================================================================================================
\chapter{Hardware}

\section{HIMMELI Sensor Rig}

HIMMELI sensor rig is a modular sensor platform that was created in Aalto university department of automation and systems technology as a research project during 2012. HIMMELI sensor rig was manufactured for usage in future automated machinery research and its purpose is to provide flexible sensing of environment with multitude of sensors.

The rig consists of three modules that can be installed separately depending on the space constraints of a selected application. The base rig on the backside includes two computing stations with a control cabinet. There are two modules in front where the lower module of the rig includes IR light source, and the upper module includes most camera sensors.

The sensor rig can be powered with a 230 V mains power or alternatively with a 24 V DC source. We do not consider the details of powering up the HIMMELI rig since the power-up sequence and details are irrelevant for the end user.

\subsection{HIMMELI Control Cabinet}

The control cabinet includes 11 different components that are listed briefly in table \ref{table:controlcabinet}.

\begin{table}
\caption{Control cabinet components in the HIMMELI sensor rig.}
\label{table:controlcabinet}
\begin{tabular}{|p{6cm}|p{5cm}|}
\hline
\textbf{Component} & \textbf{Details} \\
\hline
ATX power supply x 2 & One for each embedded PC. \\
\hline
Embedded PC 1 & Operating system Windows 7 \\
\hline
Embedded PC 2 & Operating system GNU/Linux
\hline
Gigabit Ethernet switch & \\
\hline
Power supply switch & 230VAC/24VDC \\
\hline
Fuse box & \\
\hline
Capacitor & For power change-over usage. \\
\hline
Wireless module & \\
\hline
Power ground box & \\
\hline
Selector relay & For power change-over usage. \\ \hline
\end{tabular}
\end{table}

in order to provide a flexible sensory platform for research 

is currently readily available for recording stero image data, and is located in Tampere university of technology 



The HIMMELI sensor rig is used to collect data from the environment. It can sense environment from a distance with the installed cameras, and sense the orientation of the platform with its inertial measurement unit, IMU.

HIMMELI includes

\begin{itemize}
% item separation length in a list
\setlength{\itemsep}{0pt}
\item HDR stereo camera pair
\item High resolution camera
\item 3D LIDAR laser scanner \url{http://velodynelidar.com/lidar/hdlproducts/hdl32e.aspx}
\item Infrared (IR) camera
\item Automotive radar
\end{itemize}

Additionally, an IR light source and LED flash are included.

\begin{table}
\caption{Device and its connector type in HIMMELI sensor rig.}
\label{table:connectortypes} %lable just after caption
\begin{tabular}{|p{6cm}|p{5cm}|}
\hline % The line on top of the table
\textbf{Device} & \textbf{Connector Type} \\
\hline
High dynamic range camera & Gigabit Ethernet \\
\hline
High resolution camera & IEEE 1394b (FireWire) \\
\hline
Automotive radar & CAN bus \\
\hline
Thermal camera & CVBS (PAL) or serial bus \\
\hline
LIDAR laser scanner & LIDAR \\
\hline
GNSS receiver & Serial bus 1 \\ \hline
\end{tabular} % for really simple tables, you can just use tabular
\end{table} % table makes a floating object with a title

The rig has computing capability with an embedded Linux PC that is mainly used for data routing and sensor fusion. The rig can be remotely operated via a WLAN network.

The 3D LIDAR sensor is a Velodyne HDL-32 model that scans the environment with 32 micromirror controlled lasers.

The stereo camera setup installed on the rig has two HDR cameras.

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=\textwidth]{himmeli.pdf}
%    \includegraphics[width=9cm]{example_ihaa.jpg}
    \caption{HIMMELI Sensor Platform Schematic.}
    \label{fig:himmeli}
  \end{center}
\end{figure}

%=================================================================================================================================
\chapter{Industry Applications}

\section{Forestry Machines}

\subsection{Harvester}

\subsection{Log Picker Truck}

\section{Process Crane}

\section{Skylifter}
%============================================================================================================================

\chapter{Image Coordinate Systems}

A widely accepted right-handed coordinate system will be used for describing a three-dimensional coordinate system. The details will be further discussed in the following sections. In general, the positive Z axis points out towards the scene in a two-dimensional image plane.

\section{Matrix Coordinate Transformations}

The coordinate transformation matrices depicted in this thesis are three-dimensional Cartesian coordinate transformations who use a base coordinate denoted (x,y,z).

A point is a metric location in space that is uniquely defined with a three-dimensional vector in a chosen coordinate system.
A spatial coordinate has three parameters, namely x, y, and z, and the corresponding vector is referred to with a subscript after the coordinate space name.

\begin{equation*}
\Biggl[ \begin{array}{ccc}
a & b & c \\
d & e & f \\
g & h & i \end{array} \Biggl]
\end{equation*}

The subscript depicts the coordinate space where a point is located. 
Also the transformation matrix has a subscript: it depicts on which coordinate space point (or vector) the transformation acts upon.
The transformation matrix has a superscript: it depicts to which coordinate space the point will end up transformed in.


\subsection{Prototype Transforms and Ordering}

Most of the matrix transformations are computed using three simple prototype transformations: translation, scaling, and rotation.

\textbf{Translation} of a point is given by a translation transformation matrix

\begin{equation}
\Biggl[ \begin{array}{c}
X^* \\
Y^* \\
Z^* \end{array} \Biggl]
= \Biggl[ \begin{array}{cccc}
1 & 0 & 0 & X_0 \\
0 & 1 & 0 & Y_0 \\
0 & 0 & 1 & Z_0 \\
0 & 0 & 0 & 1 \end{array} \Biggl]

\end{equation}

Using of square matrix is enforced here to enable multiple sequential transformations computation into a single transformation matrix. Additionally, a square matrix representation simplifies the notation a lot.

\textbf{Scaling} of a point is given by a scaling transformation matrix

+ scaling matrix definition here (2.5-8).

When is scaling used?

\textbf{Rotation} of a point is given by a rotation transformation matrix. A simple rotation matrix operates on a single axis at a time, and rotation about multiple axes is achieved with sequential single rotations about different axes of the three-dimensional image space. The outcome of multiple rotations depends on the order of selected axis rotations.

\begin{equation}
\textbf{R}_\alpha = \Biggl[ \begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & \cos \alpha & \sin \alpha & 0 \\
0 & -\sin \alpha & \cos \alpha & 0 \\
0 & 0 & 0 & 1 \end{array} \Biggl]
\end{equation}

Concatenating multiple transformations into a single $4 x 4$ matrix can achieve a simultaneous translation, rotation, and scaling. The ordering of the transformations depend on the matrix commutability, which in general is not a common property in transformation matrices, thus, the correct order of the transformations is crucial. 

It is obviously desirable to transform multiple coordinate points at a one time. Multiple coordinate point sets are called point clouds, and the transformation for a point cloud can be computed with the same square matrix used for single point transform.

If the point cloud $P$ contains $n$ points, we may structure a matrix P that contains the set of points as column vectors where

\begin{equation}
P = \Biggl[ \begin{array}{ccccc}
X_0 & X_1 & X_2 & \dots & X_n \\
Y_0 & Y_1 & X_2 & \dots & Y_n \\
Z_0 & Z_1 & X_2 & \dots & Z_n \end{array} \Biggl]
\end{equation}

\subsection{Camera Image Plane}

The camera image plane is the projection plane where the three-dimensional point of the imaged scene is transformed with a non-linear transformation. The camera image plane is a two-dimensional plane that is orthogonally coincident with the optical axis of the lens. 

The camera image plane has its own coordinate system, namely $(x_i,y_i,z_i)$ coordinate system. We shall call the coordinate system a 

\subsection{World Coordinate System}

The world scene that the camera sensors are detecting is called \emph{the world} (or scene interchangeably), which maps to arbitrary world coordinate system with the coordinate convention $(X_w, Y_w, Z_w)$. Thus, the origin of the world coordinate system is the point $O_w$. The subscript $w$ is used with any point that belongs to the world.

The world is a three-dimensional system where a point lies in an Euclidian space $R^n$ and its span is $R^3$.

The extrinsic parameters of the camera can be computed from the interaction between the camera coordinate system and the world coordinate system. Since the camera lies in some arbitrary world coordinates, and the detected scene changes according to the camera orientation, we can easily see that the extrinsic camera parameters change when the camera pose changes in the world coordinate system. The relation between the world coordinate system and the camera coordinate system can be computed with an Euclidean transformation consisting of a translation and rotation. 

+ Additionally, the world lighting conditions and weather affect the extrinsic parameters to some extent.?

The world coordinate system axes may be referred to as world axes or scene axes. Interchangeably, they may be referred to as extrinsic axes of the camera.

\subsection{Camera Coordinate System}

The camera sensor array dimensions define the first two orthogonal dimensions of the camera coordinate system. The camera coordinate x-y-plane lies on the camera image array, and the z-axis points out from the image plane towards the camera lens system along the optical axis. The camera coordinate system z-axis coincides with the optical axis of the camera in an ideal situation.

The focal point of the camera defines the origin of the camera coordinate system, namely the focal point $O_c$. The subscript $c$ is used with any point that belongs to the camera coordinate system, mainly the physical points of the imaging array.

The camera coordinate system axes may be referred to as camera axes. Interchangeably, they may be referred to as intrinsic axes of the camera.

\subsection{Image Affine Coordinate System}

The affine coordinate system represents the shear (skew) and rescale aspects of the image Euclidean coordinate system \cite{Sonka07}. Shear and rescale options are usually called the aspect ratio of the image.  

The reason to introduce the image affine coordinate system is that pixels are not necessarily perfectly perpendicular. The correction for pixel dimension distortion is needed for applications that need high accuracy in measurements.

The image affine coordinate system introduces axes $u_i$, $v_i$, and $w_i$ that are not orthogonal. Axes v and w coincide with the camera coordinate system axes Y and Z, but axis u does not coincide with axis X, but is tilted. 

\section{Geometric projection}

A world point $X_w$ is translated and rotated into the camera coordinate system so that

\begin{equation}
\textbf{X}_c = \Biggl[ \begin{array}{c}
x_c \\
y_c \\
z_c \end{array} \Biggl] = R(\textbf{X}_w - \textbf{t})
\end{equation}

The point in the world according to the camera coordinate system now transformed to point
$X_c$. Next, the point will be projected onto the image plane $\pi$ according to the pinhole camera model. 
Point $X_c$ projects onto the image plane as Euclidean point $U_c$ where the projected point can be computed from the similar triangles of image \ref{image:projectedpoint}. 

\begin{equation}
\label{equation:projectedpointUc}
\textbf{U}_c = \Biggl[ \begin{array}{c}
\frac{-fx_c}{z_c} \\
\frac{-fy_c}{z_c} \\
-f \end{array} \Biggl]
\end{equation}

The image point that the camera outputs is the affine transformed $\textbf{U}_c$. Next, we must formulate a homogenous coordinate representation that allows to compute the affine transformation with a $3x3$ matrix multiplication for \ref{equation:projectedpointUc}.

We can represent a homogenous coordinate point in the image plane $\pi$ as 

\begin{equation}
\label{equation:affinedefinition}
\tilde{u} = \Biggl[ \begin{array}{c}
U \\
V \\
W \end{array} \Biggl]
\end{equation}

so that a Euclidean distance representation of point $\tilde{u}$ is 

\begin{equation}
\textbf{u} = \Biggl[ \begin{array}{c}
\emph{u} \\
\emph{v} \end{array} \Biggl] = \Biggl[ \begin{array}{c}
\frac{U}{W} \\
\frac{V}{W} \end{array} \Biggl]
\end{equation}

Now we have calculated the projected point \ref{equation:projectedpointUc} and we have determined that the resulting affine transformation results in $\tilde{u}$ in \ref{equation:affinedefinition}. We still need to define a principal point of the image plane $\pi$, which is the point where the optical axis and the image plane $\pi$ intersect. This point defines the homogenous transformation according to the intrinsic camera parameters. The principal point is defined in the affine image coordinate system as

\begin{equation}
\textbf{U}_0a = \Biggl[ \begin{array}{c}
\emph{u_0} \\
\emph{v_0} \\
0 \end{array} \Biggl]
\end{equation}

We have followed the definition of Sonka et al. \cite{Sonka07} and the resulting camera calibration matrix is an upper triangle matrix containing three unknown parameters of shearing and rescaling, and the principal point so that

\begin{equation}
\tilde{u} = \Biggl[ \begin{array}{c}
U \\
V \\
W \end{array} \Biggl] = \begin{array}{ccc}
a & b & \emph{-u_0} \\
0 & c & \emph{-v_0} \\
0 & 0 & 1 \end{array} \Biggl] \Biggl[ \begin{array}{c}
\frac{-fx_c}{z_c} \\
\frac{-fy_c}{z_c} \\
1 \end{array} \Biggl] 
\end{equation}

and with reordering the multiplication of $-f$ we obtain

\begin{equation}
\tilde{u} = \Biggl[ \begin{array}{ccc}
\emph{-fa} & \emph{-fb} & \emph{-u_0} \\
0 & \emph{-fc} & \emph{-v_0} \\
0 & 0 & 1 \end{array} \Biggl] \Biggl[ \begin{array}{c}
\frac{x_c}{z_c} \\
\frac{y_c}{z_c} \\
1 \end{array} \Biggl]
\end{equation}



\begin{equation}
\label{equation:projectedpoint}
U_c = \Biggl[ \begin{array}{c}
a \\
b \\
0 \end{array} \Biggl]
\end{equation}

Now, the point $U_c$ in equation \ref{equation:projectedpoint} can be represented in homogenous coordinate system as 

\begin{equation}
\tilde{U_c} = \Biggl[\begin{array}{c}
U \\
V \\
W \end{array} \Biggl]
\end{equation}


\begin{equation*}


\end{equation*}

\begin{equation}

%====================================================================================================================================
\chapter{Image Manipulations}
\section{Histogram Operations}

A histogram is a two-dimensional graphical representation of an image grey-level distribution. The grey-levels correspond to the image intensity. As depicted in figure X, the X axis represents the number of independent grey levels that the image contains. The grey-levels range from 0 to N levels. For example, in an 8-bit single channel image there can be up to 255 independent different tones of gray. The Y axis represents per centage information about a single grey-level ranging from 0\% to 100\%. Thus, the histogram essentially displays a distribution of all grey-levels.

\subsection{Histogram Equalization}

Histogram equalization maps the current image intensity response over the full available intensity range. In figure N, the first image depicts image intensities distributed over a limited band of intensities. The second image displays the mapping result of the equalization process. The resulting histogram is calculated with the cumulative distribution function H'(i).

todo: corresponding histograms of two test images, and their cumulative distribution functions

\subsection{Linear Filters}

This section introduces linear filters that can be used for image enhancing.

\subsubsection{Gaussian Blur Filter}

\subsubsection{Gradient Filters}

\subsubsection{Laplacian Filters}


\subsubsection{Image Convolution Filters}

Some image convolution kernels are 

\subsection{Non-linear Filters}

\subsubsection{Median Filter}

\subsubsection{Wiener Filter}

\subsubsection{Bilateral Filter}

\subsection{Frequency Filters}

\chapter{Machine Learning In Object Recognition}

+ probability density estimation
+ direct loss minimization


\chapter{Cascade Classifiers}
\section{Haar Training}

Haar training is an object detection system that can detect features in a digital image. The first suggestion of using Haar training for feature detection was by Viola et al in 

\subsection{Machine Learning Classifiers}

A classifier is a concept of machine learning that is utilized in Haar training. A single classifier itself is a weak member of a more powerful committee of cascaded classifiers \cite{Freund96}. 

A number of simple, computationally light classifiers seem to outperform strong classifiers such as neural networks in principle \cite{Lienhart03}. 

In 2002, Lienhart and Maydt \cite{Lienhart02} proposed improvements upon the original work of the authors of Haar training classifiers. The Intel labs research team added a rotated feature classifier, which improves upon the performance of the original simple feature classifiers. One year later, they analysed different boosting algorithms in Haar training and concluded that Gentle Adaboost method performed the best in the training. 

The feature search space in Haar training is large in an image. The computational complexity depends on the image resolution, and on the chosen prototype of a feature.  For example, the number of features for a small window size of 24x24 is reported to total up to 117,941 features\cite{Lienhart03}. 

The computation of a single feature can be fast. The journal by... states that it can be done in constant time.

complexity for a single feature is constant. 

Currently, the 

+ 14 Haar-like features
+ 4 edge features, 8 line features, 2 center-surround features, 1 line diagonal feature
+ 

\subsection{Boosting Algorithms}

There are a number of different boosting algorithms available, 
+ Discrete Adaboost
+ Gentle Adaboost 
+ Real Adaboost

All the mentioned boosting algorithms are identical in classification sense, but they differ in their parameter learning algorithms \cite{Lienhart03}.

\subsection{Cascade of Classifiers}

A cascade of classifiers is a decision tree structure which is constructed out of simple classifiers called stage classifiers. The cascade of classifiers results in a positive detection of a feature if the test image can pass all the stages of the cascade.

\image{cascade_of_n_classifiers}

Accordingly, we may estimate the probability for a false alarm rate given the probability of a single weak classifier detection and the number of stages.

\formula{false_alarm_rate}

The cascade of classifiers can be constructed to enable feature scaling and scale-invariant detection. If the test image is re-scaled, the computation still works in constant time since a look-up table can be scaled accordingly and used in the computation. The downside of feature scaling is the loss of accuracy due integer rounding. The algorithm is sensitive to rounding errors and the result can vary severely.


+ 20 stages used in \cite{Lienhart03}


\chapter{Available Sensor Technologies}

\section{Automotive Radar}

Automotive radar is a millimeter wavelength radar that can measure ground properties or obstacles in front of a robot or a vehicle. It can measure environmental properties in the range of 1 to 120 meters \cite{Ahtiainen12}.

Radar is often usen

The advantages that the automotive radar has are weather-independent operation, and direct acquisition of range and velocity measurements \cite{Wenger07}.

The challenges that automotive radar introduces are measurement accuracy in resolution, and electromagnetic emissions on other radio frequencies \cite{Wenger07}. 

The automotive radar operates on a wide band of 5GHz to achieve a resolution of 5 cm \cite{Wenger07}.

In Europe the bands that are reserved for automotive radar operation in traffic applications are approximately 21 GHz to 27 GHz and 77 GHz to 81 GHz \cite{Wenger07}.

A world-wide harmonized channel allocation system for radar widebands has been proposed, but also considered very difficult to achieve due to national regulations \cite{Wenger07}.

Radar is not very widely used for machine perception due to low resolution and high cost. Instead, a stereo camera is often used. Still, automotive radar is an interesting add-on because of its capability of operation in adverse weathe, for example fog. 



% \input{6evaluation.tex}

\chapter{Evaluation}
\label{chapter:evaluation}

You have done your work, but that's\footnote{By the way, do \emph{not} use
shorthands like this in your text! It is not professional! Always write out all
the words: ``that is''.} not enough.

You also need to evaluate how well your implementation works.  The
nature of the evaluation depends on your problem, your method, and
your implementation that are all described in the thesis before this
chapter.  If you have created a program for exact-text matching, then
you measure how long it takes for your implementation to search for
different patterns, and compare it against the implementation that was
used before.  If you have designed a process for managing software
projects, you perhaps interview people working with a waterfall-style
management process, have them adapt your management process, and
interview them again after they have worked with your process for some
time. See what's changed.

The important thing is that you can evaluate your success somehow.
Remember that you do not have to succeed in making something spectacular; a
total implementation failure may still give grounds for a very good master's
thesis---if you can analyze what went wrong and what should have been done.




% \input{7discussion.tex}

\chapter{Discussion}
\label{chapter:discussion}

At this point, you will have some insightful thoughts on your implementation
and you may have ideas on what could be done in the future.
This chapter is a good place to discuss your thesis as a whole and to show your
professor that you have really understood some non-trivial aspects of the
methods you used\ldots



% \input{8conclusions.tex}

\chapter{Conclusions}
\label{chapter:conclusions}

Time to wrap it up!
Write down the most important findings from your work.
Like the introduction, this chapter is not very long.
Two to four pages might be a good limit.



% Load the bibliographic references
% ------------------------------------------------------------------
% You can use several .bib files:
% \bibliography{thesis_sources,ietf_sources}
\bibliography{ref}


% Appendices go here
% ------------------------------------------------------------------
% If you do not have appendices, comment out the following lines
\appendix
% \input{appendices.tex}

\chapter{First appendix}
\label{chapter:first-appendix}

This is the first appendix. You could put some test images or verbose data in an
appendix, if there is too much data to fit in the actual text nicely.

For now, the Aalto logo variants are shown in Figure~\ref{fig:aaltologo}.

\begin{figure}
\begin{center}
\subfigure[In English]{\includegraphics[width=.8\textwidth]{aalto-logo-en}}
\subfigure[Suomeksi]{\includegraphics[width=.8\textwidth]{aalto-logo-fi}}
\subfigure[Pä svenska]{\includegraphics[width=.8\textwidth]{aalto-logo-se}}
\caption{Aalto logo variants}
\label{fig:aaltologo}
\end{center}
\end{figure}


% End of document!
% ------------------------------------------------------------------
% The LastPage package automatically places a label on the last page.
% That works better than placing a label here manually, because the
% label might not go to the actual last page, if LaTeX needs to place
% floats (that is, figures, tables, and such) to the end of the
% document.
\end{document}
